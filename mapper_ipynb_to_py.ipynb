{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da527b7e-2d30-4660-b6ee-1a63bc9df48f",
   "metadata": {},
   "source": [
    "### THIS SCRIPT USES MetaMap to try and map the bulk of terms, and Name Resolver to pick up what's left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
<<<<<<< Updated upstream
   "id": "730291b8-7f12-4e72-bfa6-4c7ae33ae738",
   "metadata": {
    "tags": []
   },
=======
   "id": "5e5d2411",
   "metadata": {},
>>>>>>> Stashed changes
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>.output_result { max-width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# display cells to maximum width \n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "display(HTML(\"<style>.output_result { max-width:100% !important; }</style>\"))\n",
    "\n",
    "# lets you preint multiple outputs per cell, not just last\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70b0f433-c7bb-4b98-aa7b-1d1e6ad1e3b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import collections\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pickle\n",
    "from functools import reduce\n",
    "import time\n",
    "from time import sleep\n",
    "import concurrent.futures\n",
    "import multiprocessing\n",
    "import datetime as dt\n",
    "from datetime import date\n",
    "import pathlib\n",
    "import configparser\n",
    "import urllib\n",
    "import zipfile\n",
    "import csv\n",
    "import gc\n",
    "# import sys\n",
    "# sys.path.insert(0, '/Volumes/TOSHIBA_EXT/ISB/clinical_trials/pymetamap-master')\n",
    "# from pymetamap import MetaMap  # https://github.com/AnthonyMRios/pymetamap/blob/master/pymetamap/SubprocessBackend.py\n",
    "from pandas import ExcelWriter\n",
    "import ast\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import subprocess\n",
    "import shlex\n",
    "from collections import Counter\n",
    "from ratelimit import limits, sleep_and_retry\n",
    "\n",
    "# %pip install thefuzz\n",
    "# %pip install levenshtein\n",
    "# %pip install xlsxwriter\n",
    "# %pip install ratelimit\n",
    "# %pip install timeout_decorator\n",
    "\n",
    "from thefuzz import fuzz # fuzzy matching explained: https://www.datacamp.com/tutorial/fuzzy-string-python\n",
    "\n",
    "# 40 calls per minute\n",
    "CALLS = 40\n",
    "RATE_LIMIT = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23d4d831-06e5-4439-920e-b6495d81adc3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_token_sort_ratio(str1, str2):\n",
    "    \"\"\" fuzzy matching explained: https://www.datacamp.com/tutorial/fuzzy-string-python \"\"\"\n",
    "    try:\n",
    "        return fuzz.token_sort_ratio(str1, str2)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def get_similarity_score(str1, str2):\n",
    "    \"\"\" fuzzy matching explained: https://www.datacamp.com/tutorial/fuzzy-string-python \"\"\"\n",
    "    try:\n",
    "        return fuzz.ratio(str1, str2)\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "def convert_seconds_to_hms(seconds):\n",
    "    \"\"\" converts the elapsed time or runtime to hours, min, sec \"\"\"\n",
    "    hours = seconds // 3600\n",
    "    seconds %= 3600\n",
    "    minutes = seconds // 60\n",
    "    seconds %= 60\n",
    "    return hours, minutes, seconds\n",
    "\n",
    "def de_ascii_er(text):\n",
    "    non_ascii = \"[^\\x00-\\x7F]\"\n",
    "    pattern = re.compile(r\"[^\\x00-\\x7F]\")\n",
    "    non_ascii_text = re.sub(pattern, ' ', text)\n",
    "    return non_ascii_text\n",
    "\n",
    "def start_metamap_servers(metamap_dirs):\n",
    "    global metamap_pos_server_dir\n",
    "    global metamap_wsd_server_dir\n",
    "    metamap_pos_server_dir = 'bin/skrmedpostctl' # Part of speech tagger\n",
    "    metamap_wsd_server_dir = 'bin/wsdserverctl' # Word sense disambiguation \n",
    "    \n",
    "    metamap_executable_path_pos = os.path.join(metamap_dirs['metamap_base_dir'], metamap_pos_server_dir)\n",
    "    metamap_executable_path_wsd = os.path.join(metamap_dirs['metamap_base_dir'], metamap_wsd_server_dir)\n",
    "    command_pos = [metamap_executable_path_pos, 'start']\n",
    "    command_wsd = [metamap_executable_path_wsd, 'start']\n",
    "\n",
    "    # Start servers, with open portion redirects output of metamap server printing output to NULL\n",
    "    with open(os.devnull, \"w\") as fnull:\n",
    "        result_post = subprocess.call(command_pos, stdout = fnull, stderr = fnull)\n",
    "        result_wsd = subprocess.call(command_wsd, stdout = fnull, stderr = fnull)\n",
    "    sleep(5)\n",
    "\n",
    "def stop_metamap_servers(metamap_dirs):\n",
    "    metamap_executable_path_pos = os.path.join(metamap_dirs['metamap_base_dir'], metamap_pos_server_dir)\n",
    "    metamap_executable_path_wsd = os.path.join(metamap_dirs['metamap_base_dir'], metamap_wsd_server_dir)\n",
    "    command_pos = [metamap_executable_path_pos, 'stop']\n",
    "    command_wsd = [metamap_executable_path_wsd, 'stop']\n",
    "    \n",
    "    # Stop servers, with open portion redirects output of metamap server printing output to NULL\n",
    "    with open(os.devnull, \"w\") as fnull:\n",
    "        result_post = subprocess.call(command_pos, stdout = fnull, stderr = fnull)\n",
    "        result_wsd = subprocess.call(command_wsd, stdout = fnull, stderr = fnull)\n",
    "    sleep(2)  \n",
    "    \n",
    "def check_os():\n",
    "    if \"linux\" in sys.platform:\n",
    "        print(\"Linux platform detected\")\n",
    "        metamap_base_dir = \"{}/metamap/\".format(pathlib.Path.cwd().parents[0])\n",
    "        metamap_bin_dir = 'bin/metamap20'\n",
    "    else:\n",
    "        metamap_base_dir = '/Volumes/TOSHIBA_EXT/ISB/clinical_trials/public_mm/' # for running on local\n",
    "        metamap_bin_dir = 'bin/metamap18'\n",
    "        \n",
    "    return {\"metamap_base_dir\":metamap_base_dir, \"metamap_bin_dir\":metamap_bin_dir} \n",
    "\n",
    "@sleep_and_retry\n",
    "@limits(calls=CALLS, period=RATE_LIMIT)\n",
    "def check_limit():\n",
    "    ''' Empty function just to check for calls to Name Resolver API '''\n",
    "    return\n",
    "\n",
    "def wrap(x): # use this to convert string objects to dicts \n",
    "    try:\n",
    "        a = ast.literal_eval(x)\n",
    "        return(a)\n",
    "    except:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6746b051-2524-4d93-ad30-69e7fcd225e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_raw_ct_data():\n",
    "    term_program_flag = True\n",
    "    global data_dir\n",
    "    global data_extracted\n",
    "    \n",
    "    try:\n",
    "        # get all the links and associated dates of upload into a dict called date_link\n",
    "        url_all = \"https://aact.ctti-clinicaltrials.org/download\"\n",
    "        response = requests.get(url_all)\n",
    "        soup = BeautifulSoup(response.text, features=\"lxml\")\n",
    "        body = soup.find_all('option') #Find all\n",
    "        date_link = {}\n",
    "        for el in body:\n",
    "            tags = el.find('a')\n",
    "            try:\n",
    "                zip_name = tags.contents[0].split()[0]\n",
    "                date = zip_name.split(\"_\")[0]\n",
    "                date = dt.datetime.strptime(date, '%Y%m%d').date()\n",
    "                date_link[date] = tags.get('href')\n",
    "            except:\n",
    "                pass\n",
    "        latest_file_date = max(date_link.keys())   # get the date of the latest upload\n",
    "        url = date_link[latest_file_date]   # get the corresponding download link of the latest upload so we can download the raw data\n",
    "        date_string = latest_file_date.strftime(\"%m_%d_%Y\")\n",
    "        data_dir = \"{}/data\".format(pathlib.Path.cwd())\n",
    "        data_extracted = data_dir + \"/{}_extracted\".format(date_string)\n",
    "        data_path = \"{}/{}_pipe-delimited-export.zip\".format(data_dir, date_string)\n",
    "    except:\n",
    "        print(\"continue\")\n",
    "\n",
    "    if not os.path.exists(data_path):   # if folder containing most recent data doesn't exist, download and extract it into data folder\n",
    "\n",
    "        term_program_flag = False   # flag below for terminating program if latest download exists (KG is assumed up to date)\n",
    "        print(\"Attempting download of Clinical Trial data as of {}\\n\".format(date_string))\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            if response.status_code == 200:\n",
    "                with open(data_path, 'wb') as file:\n",
    "                    file.write(response.content)\n",
    "                print(\"Finished download of zip\")\n",
    "                with zipfile.ZipFile(data_path, 'r') as download:\n",
    "                    print(\"Unzipping data\")\n",
    "                    download.extractall(data_extracted)\n",
    "        except:\n",
    "            print(\"Failed to scrape AACT for download. Please navigate to https://aact.ctti-clinicaltrials.org/download and manually download zip file.\")\n",
    "            print(\"Please store the downloaded zip in the /data directory. This should be the only item besides the cache file, condition manual review file, and intervention manual review file, in the directory at this time.\")\n",
    "            done = input(\"Type Done when done: \")\n",
    "            if done == \"Done\":\n",
    "                data_dir = \"{}/data\".format(pathlib.Path.cwd())\n",
    "                # list_of_files = glob.glob(data_dir + \"/*\") # get all files in directory\n",
    "                try:\n",
    "                    # latest_file = max(list_of_files, key=os.path.getctime) # get the most recent file in the directory\n",
    "                    pattern = os.path.join(data_dir, \"*.zip\")\n",
    "                    zip_file = glob.glob(pattern) # look for file in directory that ends in \".zip\"\n",
    "                    zip_file = zip_file[0]\n",
    "                    print(\"File found at: \")\n",
    "                    print(zip_file)\n",
    "                    # print(latest_file)\n",
    "                    print(\"Please make sure this the correct zip file from AACT\")\n",
    "                    if not os.path.exists(data_extracted):   # if folder of unzipped data does not exist, unzip\n",
    "                        try:\n",
    "                            with zipfile.ZipFile(zip_file, 'r') as download:\n",
    "                                print(\"Unzipping data into\")\n",
    "                                cttime = os.path.getctime(zip_file)\n",
    "                                date_string = dt.datetime.fromtimestamp(cttime).strftime('%m_%d_%Y')\n",
    "                                data_extracted = data_dir + \"/{}_extracted\".format(date_string)\n",
    "                                print(data_extracted)\n",
    "                                download.extractall(data_extracted)\n",
    "                        except:\n",
    "                            pattern = os.path.join(data_dir, \"*_extracted\")\n",
    "                            extracted_file = glob.glob(pattern) # look for file in directory that ends in \"_extracted\"\n",
    "                            data_extracted = extracted_file[0]\n",
    "                            extracted_name = os.path.basename(os.path.normpath(extracted_file[0]))\n",
    "                            date_string = extracted_name.replace('_extracted', '')\n",
    "                            print(\"Assuming data is already unzipped\")\n",
    "                        \n",
    "                except:\n",
    "                    print(\"Unable to download and extract Clincal Trial data.\")\n",
    "                    print(\"Cannot find pipe-delimited zip in /data folder.\")\n",
    "    else:\n",
    "        print(\"KG is already up to date.\")\n",
    "\n",
    "    return {\"term_program_flag\": term_program_flag, \"data_extracted_path\": data_extracted, \"date_string\": date_string}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4850c49-b00e-422a-805b-137fccb7cd94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_raw_ct_data(flag_and_path, subset_size):\n",
    "    if flag_and_path[\"term_program_flag\"]:\n",
    "        print(\"Exiting program. Assuming KG has already been constructed from most recent data dump from AACT.\")\n",
    "        exit()\n",
    "    else:\n",
    "        data_extracted = flag_and_path[\"data_extracted_path\"]\n",
    "        # read in pipe-delimited files \n",
    "        conditions_df = pd.read_csv(data_extracted + '/conditions.txt', sep='|', index_col=False, header=0, on_bad_lines = 'warn')\n",
    "        interventions_df = pd.read_csv(data_extracted + '/interventions.txt', sep='|', index_col=False, header=0, on_bad_lines = 'warn')\n",
    "        interventions_alts_df = pd.read_csv(data_extracted + '/intervention_other_names.txt', sep='|', index_col=False, header=0, on_bad_lines = 'warn')\n",
    "\n",
    "        if subset_size:   # if a subset size is given, we are running this script on a small subset of the dataset\n",
    "            conditions_df = conditions_df.sample(n=subset_size)\n",
    "            interventions_df = interventions_df.sample(n=subset_size)\n",
    "            interventions_alts_df = interventions_alts_df.sample(n=subset_size)\n",
    "    \n",
    "    df_dict = {\"conditions\": conditions_df, \"interventions\": interventions_df, \"interventions_alts\": interventions_alts_df}\n",
    "    return df_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd612efd-e483-4754-8edd-3dc45fe95e88",
   "metadata": {},
   "source": [
    "# Check against cache, retrieve terms not already mapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eddc0c83-10b4-4065-b2bb-1bad9cb0e63a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def check_against_cache(df_dict):\n",
    "    \n",
    "    conditions_list = df_dict['conditions'].name.unique().tolist()\n",
    "    conditions_list = [str(i) for i in conditions_list]\n",
    "    conditions_list = list(set([i.lower() for i in conditions_list]))\n",
    "    \n",
    "    interventions_list = df_dict['interventions'].name.unique().tolist()\n",
    "    interventions_list = [str(i) for i in interventions_list]\n",
    "    interventions_list = list(set([i.lower() for i in interventions_list]))\n",
    "    \n",
    "    interventions_alts_list = df_dict['interventions_alts'].name.unique().tolist()\n",
    "    interventions_alts_list = [str(i) for i in interventions_alts_list]\n",
    "    interventions_alts_list = list(set([i.lower() for i in interventions_alts_list]))\n",
    "    \n",
    "    try:\n",
    "        cache_manually_selected_terms()\n",
    "    except:\n",
    "        print(\"No manually selected terms file found\")\n",
    "    \n",
    "    try:        \n",
    "        cache_df = pd.read_csv(\"mapping_cache.tsv\", sep =\"\\t\", index_col=False, header=0, on_bad_lines = 'warn')\n",
    "        \n",
    "        conditions_cache = cache_df[cache_df[\"term_type\"] == \"condition\"]\n",
    "        conditions_cache = conditions_cache['clintrial_term'].unique().tolist()\n",
    "        conditions_cache = list(set([i.lower() for i in conditions_cache]))\n",
    "        \n",
    "        conditions_new = [x for x in conditions_list if x not in conditions_cache] # find conditions not in the cache (i.g. new conditions to map)\n",
    "        conditions_new = list(filter(None, conditions_new))\n",
    "        conditions_new = [str(i) for i in conditions_new]\n",
    "        \n",
    "        interventions_cache = cache_df[cache_df[\"term_type\"] == \"intervention\"]\n",
    "        interventions_cache = interventions_cache['clintrial_term'].unique().tolist()\n",
    "        interventions_cache = list(set([i.lower() for i in interventions_cache]))\n",
    "        \n",
    "        interventions_new = [x for x in interventions_list if x not in interventions_cache] # find interventions not in the cache (i.g. new interventions to map)\n",
    "        interventions_new = list(filter(None, interventions_new))\n",
    "        interventions_new = [str(i) for i in interventions_new]\n",
    "        \n",
    "        interventions_alts_cache = cache_df[cache_df[\"term_type\"] == \"intervention_alternate\"]\n",
    "        interventions_alts_cache = interventions_alts_cache['clintrial_term'].unique().tolist()\n",
    "        interventions_alts_cache = list(set([i.lower() for i in interventions_alts_cache]))\n",
    "        \n",
    "        interventions_alts_new = [x for x in interventions_alts_list if x not in interventions_alts_cache] # find interventions_alts not in the cache (i.g. new interventions_alts to map)\n",
    "        interventions_alts_new = list(filter(None, interventions_alts_new))\n",
    "        interventions_alts_new = [str(i) for i in interventions_alts_new]\n",
    "        \n",
    "    except:\n",
    "        print(\"No cache of terms found. Proceeding to map entire KG from scratch\")\n",
    "        conditions_new = conditions_list\n",
    "        interventions_new = interventions_list\n",
    "        interventions_alts_new = interventions_alts_list\n",
    "        \n",
    "    dict_new_terms = {\"conditions\": conditions_new, \"interventions\": interventions_new, \"interventions_alts\": interventions_alts_new}\n",
    "\n",
    "    return dict_new_terms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c2745f5-583f-41be-9073-1b29760604b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cache_manually_selected_terms():    \n",
    "\n",
    "    def return_curie_dict(curie_info_delimited):\n",
    "        keys = [\"mapped_name\", \"mapped_curie\", \"mapped_score\", \"mapped_semtypes\"]\n",
    "        curie_list = curie_info_delimited.split(\" | \")\n",
    "        curie_dict = dict(zip(keys, curie_list))\n",
    "        return curie_dict\n",
    "\n",
    "    files = glob.glob(\"*.xlsx\")\n",
    "    manually_selected_file = [i for i in files if \"manual_review\" in i if not i.startswith(\"~\")][0] # find the file of manual selections\n",
    "    manually_selected = pd.read_excel(manually_selected_file)\n",
    "    cols_to_fill = [\"mapping_tool\", \"term_type\", \"clintrial_term\", \"input_term\"]\n",
    "    manually_selected.loc[:,cols_to_fill] = manually_selected.loc[:,cols_to_fill].ffill()\n",
    "\n",
    "    manually_selected = manually_selected[~manually_selected['manually_selected_CURIE'].isnull()] # get rows where terms were manually chosen\n",
    "    manually_selected.drop([\"mapping_tool_response\"], axis = 1, inplace = True)\n",
    "\n",
    "    manually_selected[\"manually_selected_CURIE\"] = manually_selected[\"manually_selected_CURIE\"].apply(lambda x: return_curie_dict(x)) # convert | delimited strings to CURIE dict\n",
    "    manually_selected[\"score\"] = 1000  # human curated score = 1000\n",
    "    manually_selected.rename(columns = {'manually_selected_CURIE':'mapping_tool_response'}, inplace = True)\n",
    "    manually_selected = manually_selected[[\"mapping_tool\", \"term_type\", \"clintrial_term\", \"input_term\", \"mapping_tool_response\", \"score\"]] # reorder columns to be same as the cache files we're appending to \n",
    "    manually_selected.to_csv(\"mapping_cache.tsv\", mode='a', header=False, sep =\"\\t\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9a77e9-7ead-4ee6-a4df-d2aa636cc893",
   "metadata": {},
   "source": [
    "# Map new terms using Mapper function (MetaMap + Name Resolver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee1c310c-1087-4d72-bef2-53f535f92e6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_nr_response(orig_term):\n",
    "    def create_session():\n",
    "        s = requests.Session()\n",
    "        return s\n",
    " \n",
    "    sess = create_session()\n",
    " \n",
    "    \"\"\"   Runs Name Resolver   \"\"\"\n",
    "    nr_url = 'https://name-resolution-sri.renci.org/lookup'\n",
    "    max_retries = 3 \n",
    "    \n",
    "    input_term = orig_term # in MetaMap, we have to potentially deascii the term and lower case it...for Name Resolver, we don't need to do that. To keep columns consist with MetaMap output, we just keep it and say the original term and the input term are the same. For MetaMap, they might be different\n",
    "    retries = 0\n",
    "    params = {'string':orig_term, 'limit':1} # limit -1 makes this return all available equivalent CURIEs name resolver can give (deprecated)\n",
    "    while retries <= max_retries:\n",
    "        try:\n",
    "            r = sess.post(nr_url, params=params)\n",
    "            check_limit() # counts how many requests have been sent to NR. If limit of 40 have been sent, sleeps for 1 min\n",
    "            if r.status_code == 200:\n",
    "                mapping_tool_response = r.json()  # process Name Resolver response\n",
    "                return mapping_tool_response\n",
    "            else:\n",
    "                return None\n",
    "        except (requests.RequestException, ConnectionResetError, OSError) as ex:\n",
    "            print(f\"\\nName Resolver request failed for term: {term}. Error: {ex}\")\n",
    "            retries += 1\n",
    "            if retries < max_retries:\n",
    "                print(f\"Retrying ({retries}/{max_retries}) after a delay.\")\n",
    "                time.sleep(2 ** retries)  # Increase the delay between retries exponentially\n",
    "            else:\n",
    "                print(f\"Max retries (Name Resolver) reached for term: {term}.\")\n",
    "                return None"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": null,
   "id": "1b60d026-6fdb-44bc-8cf2-6ae28d2edcf6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# I'm only getting 1 concept from Name Resolver. \n",
    "# Both MetaMap and Name Resolver return several, \n",
    "# but I only take 1 from Name Resolver bc they have a preferred concept.\n",
    "# MetaMap's 2nd or 3rd result is often the best one, so I collect all of them and try to score\"\n",
    "\n",
    "def process_metamap_concept(concept):\n",
    "    concept = concept._asdict()\n",
    "    concept_dict  = {\"mapped_name\": concept.get(\"preferred_name\"),\n",
    "                     \"mapped_curie\": concept.get(\"cui\"),\n",
    "                     \"mapped_score\": concept.get(\"score\"),\n",
    "                     \"mapped_semtypes\": concept.get(\"semtypes\")}\n",
    "    if not concept.get(\"preferred_name\"): # if condition triggered if the concept dict looks like following, where AA is for Abbreviation.... {'index': 'USER', 'aa': 'AA', 'short_form': 'copd', 'long_form': 'chronic obstructive pulmonary disease', 'num_tokens_short_form': '1', 'num_chars_short_form': '4', 'num_tokens_long_form': '7', 'num_chars_long_form': '37', 'pos_info': '43:4'}\n",
    "        concept_dict = None\n",
    "    return concept_dict\n",
    "\n",
    "def process_nameresolver_response(nr_response):              \n",
    "    nr_curie = nr_response[0][\"curie\"]\n",
    "    nr_name = nr_response[0][\"label\"]\n",
    "    nr_type = nr_response[0][\"types\"][0]\n",
    "    nr_score = nr_response[0][\"score\"]\n",
    "    concept_dict = {\"mapped_name\": nr_name,\n",
    "                    \"mapped_curie\": nr_curie,\n",
    "                    \"mapped_score\": nr_score,\n",
    "                    \"mapped_semtypes\": nr_type}\n",
    "    return concept_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1292c371-6c17-4102-8f17-0f45846c23b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_mappers(term_pair, params, term_type, csv_writer):\n",
    "    \n",
    "    orig_term = term_pair[0]\n",
    "    input_term = term_pair[1]\n",
    "    from_mapper = []\n",
    "    mm = MetaMap.get_instance(metamap_dirs[\"metamap_base_dir\"] + metamap_dirs[\"metamap_bin_dir\"])\n",
    "\n",
    "    # Format of output TSV: header = ['mapping_tool', 'term_type', 'clintrial_term', 'input_term', 'mapping_tool_response', 'score']\n",
    "\n",
    "    if params.get(\"exclude_sts\") is None: # exclude_sts is used for Interventions. restrict_to_sts is used for Conditions. So, the logic is, if we're mapping Conditions, execute \"if\" part of code. If we're mapping Interventions, execute \"else\" part of code\n",
    "        try:\n",
    "            concepts,error = mm.extract_concepts([input_term],\n",
    "                                                 restrict_to_sts = params[\"restrict_to_sts\"],\n",
    "                                                 term_processing = params[\"term_processing\"],\n",
    "                                                 ignore_word_order = params[\"ignore_word_order\"],\n",
    "                                                 strict_model = params[\"strict_model\"],)\n",
    "                                                    \n",
    "            if concepts:   # if MetaMap gives response, process response\n",
    "                mapping_tool = \"metamap\"\n",
    "                for concept in concepts:\n",
    "                    concept_info = []\n",
    "                    new_concept_dict = process_metamap_concept(concept)\n",
    "                    concept_info.extend([mapping_tool, term_type, orig_term, input_term, new_concept_dict]) # score column is empty, Format of output TSV: header = ['mapping_tool', 'term_type', 'clintrial_term', 'input_term', 'mapping_tool_response', 'score']\n",
    "                    from_mapper.append(concept_info)\n",
    "            else:   # if MetaMap fails, try using Name Resolver and process response\n",
    "                nr_response = get_nr_response(orig_term)\n",
    "                if nr_response: # if Name Resolver gives response, process repsonse\n",
    "                    input_term = orig_term # no preprocessing (lowercasing or deascii-ing) necessary to submit terms to Name Resolver (unlike MetaMap)\n",
    "                    mapping_tool = \"nameresolver\"\n",
    "                    concept_info = []\n",
    "                    new_concept_dict = process_nameresolver_response(nr_response)\n",
    "                    concept_info.extend([mapping_tool, term_type, orig_term, input_term, new_concept_dict]) # Add None for score column, empty bc not scored yet\n",
    "                    from_mapper.append(concept_info)\n",
    "                else:\n",
    "                    concept_info = []\n",
    "                    # print(\"Nothing returned from NR or Metamap\")\n",
    "                    concept_info.extend([\"mapping_tools_failed\", term_type, orig_term, input_term, \"mapping_tools_failed\"])\n",
    "                    from_mapper.append(concept_info)\n",
    "        except:\n",
    "            concept_info = []\n",
    "            # print(\"Nothing returned from NR or Metamap\")\n",
    "            concept_info.extend([\"mapping_tools_failed\", term_type, orig_term, input_term, \"mapping_tools_failed\"])\n",
    "            from_mapper.append(concept_info)\n",
    "            \n",
    "    else:   # Else block triggered if mapping Interventions\n",
    "        try:\n",
    "            concepts,error = mm.extract_concepts([input_term],\n",
    "                                                 exclude_sts = params[\"exclude_sts\"],\n",
    "                                                 term_processing = params[\"term_processing\"],\n",
    "                                                 ignore_word_order = params[\"ignore_word_order\"],\n",
    "                                                 strict_model = params[\"strict_model\"],) \n",
    "                                                   \n",
    "            if concepts:   # if MetaMap gives response, process response\n",
    "                mapping_tool = \"metamap\"\n",
    "                for concept in concepts:\n",
    "                    concept_info = []\n",
    "                    new_concept_dict = process_metamap_concept(concept)\n",
    "                    concept_info.extend([mapping_tool, term_type, orig_term, input_term, new_concept_dict]) # score column is empty, Format of output TSV: header = ['mapping_tool', 'term_type', 'clintrial_term', 'input_term', 'mapping_tool_response', 'score']\n",
    "                    from_mapper.append(concept_info)\n",
    "            else:   # if MetaMap fails, try using Name Resolver and process response\n",
    "                nr_response = get_nr_response(orig_term) \n",
    "                if nr_response: # if Name Resolver gives response, process repsonse\n",
    "                    input_term = orig_term # no preprocessing (lowercasing or deascii-ing) necessary to submit terms to Name Resolver (unlike MetaMap)\n",
    "                    mapping_tool = \"nameresolver\"\n",
    "                    concept_info = []\n",
    "                    new_concept_dict = process_nameresolver_response(nr_response)\n",
    "                    concept_info.extend([mapping_tool, term_type, orig_term, input_term, new_concept_dict])\n",
    "                    from_mapper.append(concept_info)\n",
    "                else:\n",
    "                    concept_info = []\n",
    "                    # print(\"Nothing returned from NR or Metamap\")\n",
    "                    concept_info.extend([\"mapping_tools_failed\", term_type, orig_term, input_term, \"mapping_tools_failed\"])\n",
    "                    from_mapper.append(concept_info)\n",
    "        except:\n",
    "            concept_info = []\n",
    "            # print(\"Nothing returned from NR or Metamap\")\n",
    "            concept_info.extend([\"mapping_tools_failed\", term_type, orig_term, input_term, \"mapping_tools_failed\"])\n",
    "            from_mapper.append(concept_info)\n",
    "      \n",
    "    for result in from_mapper:\n",
    "        # print(result)\n",
    "        if result[0] == \"mapping_tools_failed\":\n",
    "            result.append(-1)\n",
    "        else:\n",
    "            result.append(\"unscored\")\n",
    "        # print(result)\n",
    "        csv_writer.writerow(result)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485fd6bb-2e4a-40f5-bbc9-e92af2c16a87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def parallelize_mappers(term_pair_list, params, term_type, csv_writer):\n",
    "    \n",
    "#     start_metamap_servers(metamap_dirs) # start the MetaMap servers\n",
    "#     terms_left = len(term_pair_list)\n",
    "#     future_to_pair = {}\n",
    "#     with concurrent.futures.ThreadPoolExecutor(max_workers=6) as executor:\n",
    "#         future_to_pair = {executor.submit(run_mappers, term_pair, params, term_type, csv_writer): term_pair for term_pair in term_pair_list}\n",
    "#         for future in concurrent.futures.as_completed(future_to_pair):\n",
    "#             term_pair = future_to_pair[future]\n",
    "#             try:\n",
    "#                 result = future.result()\n",
    "#                 # Process result if needed\n",
    "#             except Exception as exc:\n",
    "#                 print(f\"Job {term_pair} generated an exception: {exc}\")\n",
    "#             finablly:\n",
    "#                 terms_left -= 1\n",
    "#                 # pbar.update(n=1)\n",
    "#                 if terms_left % 10 == 0:\n",
    "#                     gc.collect()\n",
    "#                     time.sleep(3)\n",
    "#     stop_metamap_servers(metamap_dirs) # stop the MetaMap servers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2775f948-3fec-4cdc-99c8-fa744c0e0876",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import multiprocessing\n",
    "# from mapper_wrapper import run_mappers_wrapper\n",
    "\n",
    "\n",
    "# # def run_mappers_wrapper(args):\n",
    "# #     term_pair, params, mm, term_type, output = args\n",
    "# #     with open(output, 'w', newline='') as csvfile:\n",
    "# #         csv_writer = csv.writer(output, delimiter='\\t')\n",
    "# #         try:\n",
    "# #             return run_mappers(term_pair, params, mm, term_type, csv_writer)\n",
    "# #         except Exception as exc:\n",
    "# #             return f\"Error processing {term_pair}: {exc}\"\n",
    "    \n",
    "\n",
    "# def parallelize_mappers(term_pair_list, params, term_type):\n",
    "    \n",
    "#     # open mapping cache to add mapped terms\n",
    "#     mapping_filename = \"mapping_cache.tsv\"\n",
    "#     if os.path.exists(mapping_filename):\n",
    "#         output = open(mapping_filename, 'a', newline='', encoding=\"utf-8\") \n",
    "#         csv_writer = csv.writer(output, delimiter='\\t')\n",
    "#         output.close()\n",
    "#     else:\n",
    "#         output = open(mapping_filename, 'w+', newline='', encoding='utf-8')\n",
    "#         col_names = ['mapping_tool', 'term_type', 'clintrial_term', 'input_term', 'mapping_tool_response', 'score']\n",
    "#         csv_writer = csv.writer(output, delimiter='\\t')\n",
    "#         csv_writer.writerow(col_names)\n",
    "#         output.close()\n",
    "    \n",
    "#     terms_left = len(term_pair_list)\n",
    "#     # Prepare arguments for run_mappers\n",
    "#     args_list = [(term_pair, params, term_type, output) for term_pair in term_pair_list]\n",
    "\n",
    "#     with multiprocessing.Pool(processes=6) as pool:\n",
    "#         results = pool.map(run_mappers_wrapper, args_list)\n",
    "\n",
    "#     # Process results\n",
    "#     for result in results:\n",
    "#         if isinstance(result, str):\n",
    "#             # Handle error\n",
    "#             print(result)\n",
    "#         else:\n",
    "#             # Process result if needed\n",
    "#             pass\n",
    "#         terms_left -= 1\n",
    "#         if terms_left % 20 == 0:\n",
    "#             gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "091f5141-f625-4259-8fb0-1a6a1232467f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import csv\n",
    "from mapper_wrapper import run_mappers_wrapper\n",
    "\n",
    "\n",
    "# def run_mappers_wrapper(args):\n",
    "#     term_pair, params, mm, term_type, output = args\n",
    "#     with open(output, 'w', newline='') as csvfile:\n",
    "#         csv_writer = csv.writer(csvfile, delimiter='\\t')  # Use csvfile instead of output\n",
    "#         try:\n",
    "#             return run_mappers(term_pair, params, mm, term_type, csv_writer)\n",
    "#         except Exception as exc:\n",
    "#             return f\"Error processing {term_pair}: {exc}\"\n",
    "\n",
    "def parallelize_mappers(term_pair_list, params, term_type):\n",
    "    \n",
    "    start_metamap_servers(metamap_dirs) # start the MetaMap servers\n",
    "    terms_left = len(term_pair_list)\n",
    "    # Prepare arguments for run_mappers\n",
    "    args_list = [(term_pair, params, term_type, \"mapping_cache.tsv\") for term_pair in term_pair_list]\n",
    "\n",
    "    with multiprocessing.Pool(processes=6) as pool:\n",
    "        results = pool.map(run_mappers_wrapper, args_list)\n",
    "\n",
    "    # Process results\n",
    "    for result in results:\n",
    "        if isinstance(result, str):\n",
    "            # Handle error\n",
    "            print(result)\n",
    "        else:\n",
    "            # Process result if needed\n",
    "            pass\n",
    "        terms_left -= 1\n",
    "        if terms_left % 20 == 0:\n",
    "            gc.collect()\n",
    "            \n",
    "    stop_metamap_servers(metamap_dirs) # stop the MetaMap servers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f79ca09-cac5-4f30-a262-ff86690265e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def term_list_to_mappers(dict_new_terms):   \n",
    "    metamap_version = [int(s) for s in re.findall(r'\\d+', metamap_dirs.get('metamap_bin_dir'))] # get MetaMap version being run \n",
    "    deasciier = np.vectorize(de_ascii_er) # vectorize function\n",
    "\n",
    "    #  - Conditions\n",
    "    condition_semantic_type_restriction = ['acab,anab,cgab,comd,dsyn,inpo,mobd,neop,patf,clna,fndg']  # see https://lhncbc.nlm.nih.gov/ii/tools/MetaMap/Docs/SemanticTypes_2018AB.txt for semantic types (\"acab,anab,etc.\")\n",
    "    conditions = dict_new_terms.get(\"conditions\")\n",
    "    condition_params = {\"restrict_to_sts\":condition_semantic_type_restriction, \"term_processing\":True, \"ignore_word_order\":True, \"strict_model\":False} # strict_model and relaxed_model are presumably opposites? relaxed_model = True is what I want, but that option appears to be broken in Pymetamap (returns no results when used). Using strict_model = False instead...\n",
    "    # conditon_term_type = \"condition\"\n",
    "\n",
    "    #  - Interventions\n",
    "    condition_semantic_type_restriction = ['acab,anab,cgab,comd,dsyn,inpo,mobd,neop,patf,clna,fndg']  # see https://lhncbc.nlm.nih.gov/ii/tools/MetaMap/Docs/SemanticTypes_2018AB.txt for semantic types (\"acab,anab,etc.\")\n",
    "    interventions = dict_new_terms.get(\"interventions\")\n",
    "    intervention_params = {\"exclude_sts\":condition_semantic_type_restriction, \"term_processing\":True, \"ignore_word_order\":True, \"strict_model\":False} # strict_model and relaxed_model are presumably opposites? relaxed_model = True is what I want, but that option appears to be broken in Pymetamap (returns no results when used). Using strict_model = False instead...\n",
    "    # intervention_term_type = \"intervention\"\n",
    "\n",
    "    #  - Alternate Interventions\n",
    "    condition_semantic_type_restriction = ['acab,anab,cgab,comd,dsyn,inpo,mobd,neop,patf,clna,fndg']  # see https://lhncbc.nlm.nih.gov/ii/tools/MetaMap/Docs/SemanticTypes_2018AB.txt for semantic types (\"acab,anab,etc.\")\n",
    "    interventions_alts = dict_new_terms.get(\"interventions_alts\")\n",
    "    intervention_alts_params = intervention_params # same params as interventions\n",
    "    # intervention_alternate_term_type = \"intervention_alternate\"\n",
    "    \n",
    "    chunksize = 20\n",
    "    \n",
    "    if metamap_version[0] >= 20:\n",
    "        \n",
    "        cons_processed = list(zip(conditions, conditions))  # these are lists of the same term repeated twice, bc MetaMap 2020 does not require deasciing, so the 2nd term remains unchanged and is a repeat of the first term\n",
    "        ints_processed = list(zip(interventions, interventions))\n",
    "        ints_alts_processed = list(zip(interventions_alts, interventions_alts))\n",
    "    \n",
    "        conditions_chunked = [cons_processed[i:i + chunksize] for i in range(0, len(cons_processed), chunksize)]  \n",
    "        interventions_chunked = [ints_processed[i:i + chunksize] for i in range(0, len(ints_processed), chunksize)]  \n",
    "        interventions_alts_chunked = [ints_alts_processed[i:i + chunksize] for i in range(0, len(ints_alts_processed), chunksize)] \n",
    "        \n",
    "        print(\"MetaMap version >= 2020, conduct mapping on original terms\")\n",
    "        \n",
    "        LENGTH = len(cons_processed)  # Number of iterations required to fill progress bar (pbar)\n",
    "        pbar = tqdm(total=LENGTH, desc=\"% conditions mapped\", position=0, leave=True, mininterval = LENGTH/20, bar_format='{l_bar}{bar:20}{r_bar}{bar:-10b}')  # Init progress bar\n",
    "        for chunk in conditions_chunked:\n",
    "            # parallelize_mappers(chunk, condition_params, \"condition\", csv_writer)\n",
    "            parallelize_mappers(chunk, condition_params, \"condition\")\n",
    "            pbar.update(n=len(chunk))\n",
    "        \n",
    "        LENGTH = len(ints_processed)  # Number of iterations required to fill progress bar (pbar)\n",
    "        pbar = tqdm(total=LENGTH, desc=\"% interventions mapped\", position=0, leave=True, mininterval = LENGTH/20, bar_format='{l_bar}{bar:20}{r_bar}{bar:-10b}')  # Init progress bar\n",
    "        for chunk in interventions_chunked:\n",
    "            # parallelize_mappers(chunk, intervention_params, \"intervention\", csv_writer)\n",
    "            parallelize_mappers(chunk, intervention_params, \"intervention\")\n",
    "            pbar.update(n=len(chunk))\n",
    "        \n",
    "        LENGTH = len(ints_alts_processed)  # Number of iterations required to fill progress bar (pbar)\n",
    "        pbar = tqdm(total=LENGTH, desc=\"% alternate interventions mapped\", position=0, leave=True, mininterval = LENGTH/20, bar_format='{l_bar}{bar:20}{r_bar}{bar:-10b}')  # Init progress bar\n",
    "        for chunk in interventions_alts_chunked:\n",
    "            # parallelize_mappers(chunk, intervention_alts_params, \"alternate_intervention\", csv_writer)\n",
    "            parallelize_mappers(chunk, intervention_alts_params, \"alternate_intervention\")\n",
    "            pbar.update(n=len(chunk))\n",
    "        \n",
    "    else:\n",
    "        print(\"MetaMap version < 2020, conduct mapping on terms after removing ascii characters\")\n",
    "        \n",
    "        deascii_cons = deasciier(conditions)\n",
    "        deascii_ints = deasciier(interventions)\n",
    "        deascii_int_alts = deasciier(interventions_alts)\n",
    "                \n",
    "        cons_processed = list(zip(conditions, deascii_cons)) # these are lists of the original term, and the deasciied term, bc MetaMap 2018 does not process ascii characters\n",
    "        ints_processed = list(zip(interventions, deascii_ints))\n",
    "        ints_alts_processed = list(zip(interventions_alts, deascii_int_alts))\n",
    "        \n",
    "        conditions_chunked = [cons_processed[i:i + chunksize] for i in range(0, len(cons_processed), chunksize)]  \n",
    "        interventions_chunked = [ints_processed[i:i + chunksize] for i in range(0, len(ints_processed), chunksize)]  \n",
    "        interventions_alts_chunked = [ints_alts_processed[i:i + chunksize] for i in range(0, len(ints_alts_processed), chunksize)] \n",
    "        \n",
    "        LENGTH = len(cons_processed)  # Number of iterations required to fill progress bar (pbar)\n",
    "        pbar = tqdm(total=LENGTH, desc=\"% conditions mapped\", position=0, leave=True, mininterval = LENGTH/20, bar_format='{l_bar}{bar:20}{r_bar}{bar:-10b}')  # Init progress bar\n",
    "        for chunk in conditions_chunked:\n",
    "            # parallelize_mappers(chunk, condition_params, \"condition\", csv_writer)\n",
    "            parallelize_mappers(chunk, condition_params, \"condition\")\n",
    "            pbar.update(n=len(chunk))\n",
    "        \n",
    "        LENGTH = len(ints_processed)  # Number of iterations required to fill progress bar (pbar)\n",
    "        pbar = tqdm(total=LENGTH, desc=\"% interventions mapped\", position=0, leave=True, mininterval = LENGTH/20, bar_format='{l_bar}{bar:20}{r_bar}{bar:-10b}')  # Init progress bar\n",
    "        for chunk in interventions_chunked:\n",
    "            # parallelize_mappers(chunk, intervention_params, \"intervention\", csv_writer)\n",
    "            parallelize_mappers(chunk, intervention_params, \"intervention\")\n",
    "            pbar.update(n=len(chunk))\n",
    "        \n",
    "        LENGTH = len(ints_alts_processed)  # Number of iterations required to fill progress bar (pbar)\n",
    "        pbar = tqdm(total=LENGTH, desc=\"% alternate interventions mapped\", position=0, leave=True, mininterval = LENGTH/20, bar_format='{l_bar}{bar:20}{r_bar}{bar:-10b}')  # Init progress bar\n",
    "        for chunk in interventions_alts_chunked:\n",
    "            # parallelize_mappers(chunk, intervention_alts_params, \"alternate_intervention\", csv_writer)\n",
    "            parallelize_mappers(chunk, intervention_alts_params, \"alternate_intervention\")\n",
    "            pbar.update(n=len(chunk))\n",
    "\n",
    "    \n",
    "    # \"\"\" Remove duplicate rows \"\"\"\n",
    "    mapping_filename = \"mapping_cache.tsv\"\n",
    "    cache = pd.read_csv(mapping_filename, sep='\\t', index_col=False, header=0, encoding_errors='ignore')\n",
    "    cache = cache.drop_duplicates()\n",
    "    cache.to_csv(mapping_filename, sep=\"\\t\", index=False, header=True) # output deduplicated cache terms to TSV\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cee2c13b-775b-44a6-92be-7b75f70254af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_mappings():   \n",
    "    print(\"Scoring cache\")\n",
    "\n",
    "    def get_max_score(str1, str2, old_score):\n",
    "        \n",
    "        try:\n",
    "            if old_score == \"unscored\":\n",
    "                sortratio_score = get_token_sort_ratio(str1, str2)\n",
    "                similarity_score = get_similarity_score(str1, str2)\n",
    "                max_score = max(sortratio_score, similarity_score)\n",
    "                score = max_score\n",
    "            else:\n",
    "                score = old_score   \n",
    "        except:\n",
    "            score = old_score\n",
    "        return score\n",
    "\n",
    "    def wrap(x): # use this to convert string objects to dicts \n",
    "        try:\n",
    "            a = ast.literal_eval(x)\n",
    "            return(a)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    with pd.read_csv(\"mapping_cache.tsv\", sep='\\t', index_col=False, header=0, on_bad_lines = 'warn', usecols=lambda c: not c.startswith('Unnamed:'), chunksize=1000) as reader:\n",
    "        write_header = True\n",
    "        for chunk in reader:\n",
    "            chunk[\"mapping_tool_response\"] = chunk[\"mapping_tool_response\"].apply(lambda x: wrap(x))\n",
    "            mapping_info = chunk[\"mapping_tool_response\"].apply(pd.Series, dtype='object')\n",
    "            chunk[\"mapped_name\"] = mapping_info[\"mapped_name\"]\n",
    "            chunk[\"score\"] = chunk.apply(lambda x: get_max_score(x['input_term'], x['mapped_name'], x['score']), axis=1) # get score for score rows that are empty/not scored yet\n",
    "            chunk.drop([\"mapped_name\"], axis = 1, inplace = True)\n",
    "            chunk.to_csv(f'mapping_cache_scored_temp.tsv', sep=\"\\t\", index=False, header=write_header, mode = 'a', encoding=\"utf-8\") # output to TSV\n",
    "            write_header = False\n",
    "\n",
    "    os.rename('mapping_cache.tsv','mapping_cache_backup.tsv')       \n",
    "    os.rename('mapping_cache_scored_temp.tsv','mapping_cache.tsv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10f1c3e5-5f4c-4a01-9cf7-0f403503f108",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_terms_files():\n",
    "    print(\"Generating output files\")\n",
    "\n",
    "    \"\"\"   Get high scorers   \"\"\"\n",
    "    cache = pd.read_csv(\"mapping_cache.tsv\", sep='\\t', index_col=False, header=0)\n",
    "    cache['score'] = pd.to_numeric(cache['score'], errors='coerce')\n",
    "    highscorers = cache[cache['score'] >= 80] \n",
    "    idx = highscorers.groupby('clintrial_term')['score'].idxmax()  # group by the clinical trial term and get the highest scoring\n",
    "    auto_selected = highscorers.loc[idx]\n",
    "    auto_selected.to_csv(f'autoselected_terms.tsv', sep=\"\\t\", index=False, header=True) # output to TSV\n",
    "\n",
    "    \"\"\"   Get low scorers, aggregate for manual selections  \"\"\"\n",
    "    low_scorers = cache[cache['score'] < 80]\n",
    "    manual_review = low_scorers[~low_scorers.clintrial_term.isin(highscorers['clintrial_term'].unique().tolist())] # there are terms autoselected that have mappings that didn't pass threshold too, but we want to consider that term mapped. So get rid of these rows too\n",
    "    mapping_tool_response = manual_review['mapping_tool_response'].apply(lambda x: wrap(x))\n",
    "    manual_review = manual_review.copy()\n",
    "    mapping_tool_response = mapping_tool_response.apply(pd.Series)\n",
    "    manual_review.loc[:, 'mapping_tool_response_lists'] = mapping_tool_response.values.tolist()\n",
    "    manual_review.drop('mapping_tool_response', axis=1, inplace=True)\n",
    "    manual_review = manual_review[[\"mapping_tool\", \"term_type\", \"clintrial_term\", \"mapping_tool_response_lists\", \"input_term\", \"score\"]]\n",
    "    manual_review['mapping_tool_response'] = [' | '.join(map(str, l)) for l in manual_review['mapping_tool_response_lists']]\n",
    "    manual_review.drop('mapping_tool_response_lists', axis=1, inplace=True)\n",
    "    manual_review = manual_review.sort_values(by=[\"mapping_tool\", \"term_type\", \"clintrial_term\", \"input_term\"], ascending=False)\n",
    "    manual_review.set_index([\"mapping_tool\", \"term_type\", \"clintrial_term\", \"input_term\"], inplace=True)   # create index\n",
    "    manual_review['manually_selected_CURIE'] = None # make a column \n",
    "    manual_review.to_excel('manual_review.xlsx', engine='xlsxwriter', index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a8fe1d-e5b2-4752-81df-8dcac1502e44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315e7514-ec69-462f-9fb7-77cf0afc47be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e73d53-d952-4f1c-924a-7df5bb494afc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046274d4-f7fe-4daf-8dd6-84e1ea90fd4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd659f7-df88-47fd-9965-567b640b61ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96458f45-d889-4c08-b9d0-b2213d2c343a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d719ee3-148e-4948-8dfe-72a6bd0d730c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087ed414-58ba-4746-ad22-bd8872c8886f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfddebe-14c3-4289-9df7-f68d2657b2af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d00fe8-c542-4d2a-b379-27e3719b00fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "98ceddf5-0ebe-41d9-9b89-85fc647f4136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No cache of terms found. Proceeding to map entire KG from scratch\n",
      "MetaMap version < 2020, conduct mapping on terms after removing ascii characters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "% conditions mapped:   0%|                    | 0/40 [00:00<?, ?it/s] Process SpawnPoolWorker-27:\n",
      "Process SpawnPoolWorker-25:\n",
      "Process SpawnPoolWorker-30:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/Kamileh/anaconda3/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "Process SpawnPoolWorker-28:\n",
      "Process SpawnPoolWorker-26:\n",
      "    self.run()\n",
      "  File \"/Users/Kamileh/anaconda3/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/Kamileh/anaconda3/lib/python3.11/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/Users/Kamileh/anaconda3/lib/python3.11/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/Kamileh/anaconda3/lib/python3.11/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Users/Kamileh/anaconda3/lib/python3.11/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/Kamileh/anaconda3/lib/python3.11/multiprocessing/spawn.py\", line 135, in _main\n",
      "  File \"/Users/Kamileh/anaconda3/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/Kamileh/anaconda3/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/Kamileh/anaconda3/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "    return self._bootstrap(parent_sentinel)\n",
      "  File \"/Users/Kamileh/anaconda3/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/Kamileh/anaconda3/lib/python3.11/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/Users/Kamileh/anaconda3/lib/python3.11/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/Kamileh/anaconda3/lib/python3.11/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/Kamileh/anaconda3/lib/python3.11/multiprocessing/process.py\", line 330, in _bootstrap\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "    traceback.print_exc()\n",
      "  File \"/Users/Kamileh/anaconda3/lib/python3.11/multiprocessing/process.py\", line 317, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/Users/Kamileh/anaconda3/lib/python3.11/multiprocessing/util.py\", line 320, in _exit_function\n",
      "    def _exit_function(info=info, debug=debug, _run_finalizers=_run_finalizers,\n",
      "    \n",
      "KeyboardInterrupt\n",
      "  File \"/Users/Kamileh/anaconda3/lib/python3.11/traceback.py\", line 183, in print_exc\n",
      "  File \"/Users/Kamileh/anaconda3/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/Kamileh/anaconda3/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/Kamileh/anaconda3/lib/python3.11/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/Users/Kamileh/anaconda3/lib/python3.11/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/Kamileh/anaconda3/lib/python3.11/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "    print_exception(*sys.exc_info(), limit=limit, file=file, chain=chain)\n",
      "  File \"/Users/Kamileh/anaconda3/lib/python3.11/traceback.py\", line 124, in print_exception\n",
      "    te = TracebackException(type(value), value, tb, limit=limit, compact=True)\n",
      "  File \"/Users/Kamileh/anaconda3/lib/python3.11/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/Users/Kamileh/anaconda3/lib/python3.11/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/Kamileh/anaconda3/lib/python3.11/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/Kamileh/anaconda3/lib/python3.11/traceback.py\", line 677, in __init__\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/Kamileh/anaconda3/lib/python3.11/multiprocessing/process.py\", line 317, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/Users/Kamileh/anaconda3/lib/python3.11/multiprocessing/util.py\", line 320, in _exit_function\n",
      "    def _exit_function(info=info, debug=debug, _run_finalizers=_run_finalizers,\n",
      "    \n",
      "KeyboardInterrupt\n",
      "  File \"/Users/Kamileh/anaconda3/lib/python3.11/multiprocessing/queues.py\", line 365, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/Kamileh/anaconda3/lib/python3.11/multiprocessing/connection.py\", line 215, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/Kamileh/anaconda3/lib/python3.11/multiprocessing/connection.py\", line 413, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "          ^^^^^^^^^^^^^\n",
      "  File \"/Users/Kamileh/anaconda3/lib/python3.11/multiprocessing/connection.py\", line 378, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "    def __init__(self, exc_type, exc_value, exc_traceback, *, limit=None,\n",
      "\n",
      "KeyboardInterrupt\n",
      "  File \"/Users/Kamileh/anaconda3/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/Kamileh/anaconda3/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/Kamileh/anaconda3/lib/python3.11/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/Users/Kamileh/anaconda3/lib/python3.11/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/Kamileh/anaconda3/lib/python3.11/multiprocessing/synchronize.py\", line 97, in __exit__\n",
      "    def __exit__(self, *args):\n",
      "    \n",
      "KeyboardInterrupt\n",
      "  File \"/Users/Kamileh/anaconda3/lib/python3.11/multiprocessing/process.py\", line 317, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/Users/Kamileh/anaconda3/lib/python3.11/multiprocessing/util.py\", line 350, in _exit_function\n",
      "    for p in active_children():\n",
      "             ^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/Kamileh/anaconda3/lib/python3.11/multiprocessing/process.py\", line 47, in active_children\n",
      "    _cleanup()\n",
      "  File \"/Users/Kamileh/anaconda3/lib/python3.11/multiprocessing/process.py\", line 63, in _cleanup\n",
      "    for p in list(_children):\n",
      "             ^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "Process SpawnPoolWorker-29:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/Kamileh/anaconda3/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/Kamileh/anaconda3/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/Kamileh/anaconda3/lib/python3.11/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/Users/Kamileh/anaconda3/lib/python3.11/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/Kamileh/anaconda3/lib/python3.11/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "% interventions mapped:   0%|                    | 0/39 [08:58<?, ?it/\n",
      "% conditions mapped:   0%|                    | 0/39 [04:33<?, ?it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 23\u001b[0m, in \u001b[0;36mparallelize_mappers\u001b[0;34m(term_pair_list, params, term_type)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m multiprocessing\u001b[38;5;241m.\u001b[39mPool(processes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m pool:\n\u001b[0;32m---> 23\u001b[0m     results \u001b[38;5;241m=\u001b[39m pool\u001b[38;5;241m.\u001b[39mmap(run_mappers_wrapper, args_list)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Process results\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/multiprocessing/pool.py:367\u001b[0m, in \u001b[0;36mPool.map\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;124;03mApply `func` to each element in `iterable`, collecting the results\u001b[39;00m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;124;03min a list that is returned.\u001b[39;00m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m--> 367\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_map_async(func, iterable, mapstar, chunksize)\u001b[38;5;241m.\u001b[39mget()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/multiprocessing/pool.py:768\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    769\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mready():\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/multiprocessing/pool.py:765\u001b[0m, in \u001b[0;36mApplyResult.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwait\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 765\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event\u001b[38;5;241m.\u001b[39mwait(timeout)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/threading.py:622\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    621\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 622\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cond\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    623\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m     waiter\u001b[38;5;241m.\u001b[39macquire()\n\u001b[1;32m    321\u001b[0m     gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m df_dict \u001b[38;5;241m=\u001b[39m read_raw_ct_data(flag_and_path, subset_size) \u001b[38;5;66;03m# read the clinical trial data\u001b[39;00m\n\u001b[1;32m      9\u001b[0m dict_new_terms \u001b[38;5;241m=\u001b[39m check_against_cache(df_dict) \u001b[38;5;66;03m# use the existing cache of MetaMapped terms so that only new terms are mapped\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m term_list_to_mappers(dict_new_terms)\n\u001b[1;32m     11\u001b[0m score_mappings()\n\u001b[1;32m     12\u001b[0m output_terms_files()\n",
      "Cell \u001b[0;32mIn[14], line 77\u001b[0m, in \u001b[0;36mterm_list_to_mappers\u001b[0;34m(dict_new_terms)\u001b[0m\n\u001b[1;32m     74\u001b[0m pbar \u001b[38;5;241m=\u001b[39m tqdm(total\u001b[38;5;241m=\u001b[39mLENGTH, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m% c\u001b[39;00m\u001b[38;5;124monditions mapped\u001b[39m\u001b[38;5;124m\"\u001b[39m, position\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, mininterval \u001b[38;5;241m=\u001b[39m LENGTH\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m20\u001b[39m, bar_format\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{l_bar}\u001b[39;00m\u001b[38;5;132;01m{bar:20}\u001b[39;00m\u001b[38;5;132;01m{r_bar}\u001b[39;00m\u001b[38;5;132;01m{bar:-10b}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Init progress bar\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m conditions_chunked:\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;66;03m# parallelize_mappers(chunk, condition_params, \"condition\", csv_writer)\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m     parallelize_mappers(chunk, condition_params, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcondition\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     78\u001b[0m     pbar\u001b[38;5;241m.\u001b[39mupdate(n\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(chunk))\n\u001b[1;32m     80\u001b[0m LENGTH \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(ints_processed)  \u001b[38;5;66;03m# Number of iterations required to fill progress bar (pbar)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[20], line 22\u001b[0m, in \u001b[0;36mparallelize_mappers\u001b[0;34m(term_pair_list, params, term_type)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Prepare arguments for run_mappers\u001b[39;00m\n\u001b[1;32m     20\u001b[0m args_list \u001b[38;5;241m=\u001b[39m [(term_pair, params, term_type, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmapping_cache.tsv\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m term_pair \u001b[38;5;129;01min\u001b[39;00m term_pair_list]\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m multiprocessing\u001b[38;5;241m.\u001b[39mPool(processes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m pool:\n\u001b[1;32m     23\u001b[0m     results \u001b[38;5;241m=\u001b[39m pool\u001b[38;5;241m.\u001b[39mmap(run_mappers_wrapper, args_list)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Process results\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/multiprocessing/pool.py:739\u001b[0m, in \u001b[0;36mPool.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    738\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, exc_type, exc_val, exc_tb):\n\u001b[0;32m--> 739\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mterminate()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/multiprocessing/pool.py:657\u001b[0m, in \u001b[0;36mPool.terminate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    655\u001b[0m util\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mterminating pool\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m=\u001b[39m TERMINATE\n\u001b[0;32m--> 657\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_terminate()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/multiprocessing/util.py:224\u001b[0m, in \u001b[0;36mFinalize.__call__\u001b[0;34m(self, wr, _finalizer_registry, sub_debug, getpid)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    222\u001b[0m     sub_debug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinalizer calling \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m with args \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m and kwargs \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    223\u001b[0m               \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_kwargs)\n\u001b[0;32m--> 224\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_kwargs)\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_weakref \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_args \u001b[38;5;241m=\u001b[39m \\\n\u001b[1;32m    226\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/multiprocessing/pool.py:695\u001b[0m, in \u001b[0;36mPool._terminate_pool\u001b[0;34m(cls, taskqueue, inqueue, outqueue, pool, change_notifier, worker_handler, task_handler, result_handler, cache)\u001b[0m\n\u001b[1;32m    692\u001b[0m task_handler\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m=\u001b[39m TERMINATE\n\u001b[1;32m    694\u001b[0m util\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhelping task handler/workers to finish\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 695\u001b[0m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_help_stuff_finish(inqueue, task_handler, \u001b[38;5;28mlen\u001b[39m(pool))\n\u001b[1;32m    697\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m result_handler\u001b[38;5;241m.\u001b[39mis_alive()) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mlen\u001b[39m(cache) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m    698\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    699\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot have cache with result_hander not alive\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/multiprocessing/pool.py:675\u001b[0m, in \u001b[0;36mPool._help_stuff_finish\u001b[0;34m(inqueue, task_handler, size)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_help_stuff_finish\u001b[39m(inqueue, task_handler, size):\n\u001b[1;32m    673\u001b[0m     \u001b[38;5;66;03m# task_handler may be blocked trying to put items on inqueue\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     util\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mremoving tasks from inqueue until task handler finished\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 675\u001b[0m     inqueue\u001b[38;5;241m.\u001b[39m_rlock\u001b[38;5;241m.\u001b[39macquire()\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m task_handler\u001b[38;5;241m.\u001b[39mis_alive() \u001b[38;5;129;01mand\u001b[39;00m inqueue\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39mpoll():\n\u001b[1;32m    677\u001b[0m         inqueue\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39mrecv()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# flag_and_path = get_raw_ct_data() # download raw data\n",
    "flag_and_path = {\"term_program_flag\": False,\n",
    "                 \"data_extracted_path\": \"/Users/Kamileh/Work/ISB/NCATS_BiomedicalTranslator/Projects/ClinicalTrials/ETL_Python/data/02_27_2024_extracted\",\n",
    "                 \"date_string\": \"02_27_2024\"}\n",
    "global metamap_dirs\n",
    "metamap_dirs = check_os()\n",
    "subset_size = 40\n",
    "df_dict = read_raw_ct_data(flag_and_path, subset_size) # read the clinical trial data\n",
    "dict_new_terms = check_against_cache(df_dict) # use the existing cache of MetaMapped terms so that only new terms are mapped\n",
    "term_list_to_mappers(dict_new_terms)\n",
    "score_mappings()\n",
    "output_terms_files()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9775bd6-feaf-4536-a7d9-0b981f00df6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011a2122-0904-4b7c-938c-90ccf6027170",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a7955a-24a6-4bc1-82b2-5d4c8d81b59e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8ddb07-a201-4573-8841-cb55a6ca0399",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dc7cfe-5e65-47a3-b531-96a121472fcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388a3697-bd81-4227-a180-66edd5d84a4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fee34d5-3444-405c-ba7f-337834dcbb31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa85aed8-8fbb-4bfa-ac9c-15eba9954dde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a92caa7-a790-401b-8f1e-6ec31fb673d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ea68da-03bb-4b3c-99a2-6741f3bf829f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5027af0-eddf-4e0f-8538-a54d5bbd3d7f",
=======
   "execution_count": 3,
   "id": "3a1fbb75",
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import collections\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pickle\n",
    "from functools import reduce\n",
    "import time\n",
    "from time import sleep\n",
    "import concurrent.futures\n",
    "import multiprocessing\n",
    "import datetime as dt\n",
    "from datetime import date\n",
    "import pathlib\n",
    "import configparser\n",
    "import sys\n",
    "import urllib\n",
<<<<<<< Updated upstream
    "import zipfile\n",
    "import csv\n",
    "import gc\n",
    "# sys.path.insert(0, '/Volumes/TOSHIBA_EXT/ISB/clinical_trials/pymetamap-master') # for local\n",
    "sys.path.insert(0, '/users/knarsinh/projects/clinical_trials/metamap/pymetamap') # for hypatia\n",
    "from pymetamap import MetaMap  # https://github.com/AnthonyMRios/pymetamap/blob/master/pymetamap/SubprocessBackend.py\n",
    "from pandas import ExcelWriter\n",
    "import ast\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import subprocess\n",
    "import shlex\n",
    "from collections import Counter\n",
    "from ratelimit import limits, sleep_and_retry\n",
    "\n",
=======
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe92846f",
   "metadata": {},
   "outputs": [],
   "source": [
    "global sublist_length \n",
    "sublist_length = 990 # Name Resolver takes batches of 1000\n",
    "\n",
    "global CAS_SERVERURL \n",
    "global II_SKR_SERVERURL \n",
    "global METAMAP_INTERACTIVE_URL \n",
    "global stserverurl \n",
    "global tgtserverurl\n",
    "global apikey \n",
    "global serviceurl \n",
    "global ksource "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2477cf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "CAS_SERVERURL = \"https://utslogin.nlm.nih.gov/cas/v1\"\n",
    "II_SKR_SERVERURL = 'https://ii.nlm.nih.gov/cgi-bin/II/UTS_Required'\n",
    "METAMAP_INTERACTIVE_URL = II_SKR_SERVERURL + \"/API_MM_interactive.pl\"\n",
    "stserverurl = \"https://utslogin.nlm.nih.gov/cas/v1/tickets\"\n",
    "tgtserverurl = \"https://utslogin.nlm.nih.gov/cas/v1/api-key\"\n",
    "serviceurl = METAMAP_INTERACTIVE_URL\n",
    "ksource = '2020AB'\n",
    "cfg = configparser.ConfigParser()\n",
    "cfg.read('config.ini')\n",
    "apikey = cfg['METAMAP']['apikey']\n",
    "apikey = apikey.strip(\"\\''\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a26d5a2d",
   "metadata": {},
   "outputs": [],
   "source": [
>>>>>>> Stashed changes
    "# %pip install thefuzz\n",
    "# %pip install levenshtein\n",
    "# %pip install xlsxwriter\n",
    "# %pip install ratelimit\n",
    "# %pip install timeout_decorator\n",
    "\n",
    "from thefuzz import fuzz # fuzzy matching explained: https://www.datacamp.com/tutorial/fuzzy-string-python\n",
    "\n",
    "# 40 calls per minute\n",
    "CALLS = 40\n",
    "RATE_LIMIT = 60\n",
    "\n",
<<<<<<< Updated upstream
=======
    "from thefuzz import fuzz # fuzzy matching explained: https://www.datacamp.com/tutorial/fuzzy-string-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4495291",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nr_response(chunk):\n",
    "    \n",
    "    \"\"\"   Runs Name Resolver   \"\"\"\n",
    "    nr_url = 'https://name-resolution-sri.renci.org/lookup'\n",
    "    nr_terms = []   # list to be populated with dictionaries of Name Resolver names and CURIES for each term searched\n",
    "    max_retries = 5 \n",
    "\n",
    "    for term in chunk:\n",
    "        retries = 0\n",
    "        nr_term = {}\n",
    "        params = {'string':term, 'limit':1} # limit -1 makes this return all available equivalent CURIEs name resolver can give (deprecated)\n",
    "        while retries <= max_retries:\n",
    "            try:\n",
    "                r = requests.post(nr_url, params=params)\n",
    "                if r.status_code == 200:\n",
    "                    res = r.json()  # process Name Resolver response\n",
    "                    for key, val in res.items():\n",
    "                        nr_term[term] = [key, val[0]]\n",
    "                        nr_terms.append(nr_term) # add Name Resolver name and CURIE to list of dictionaries\n",
    "                    break # Break out of the retry loop since the request was successful\n",
    "            except (requests.RequestException, ConnectionResetError, OSError) as ex:\n",
    "                print(f\"\\nName Resolver request failed for term: {term}. Error: {ex}\")\n",
    "                retries += 1\n",
    "                if retries < max_retries:\n",
    "                    print(f\"Retrying ({retries}/{max_retries}) after a delay.\")\n",
    "                    time.sleep(2 ** retries)  # Increase the delay between retries exponentially\n",
    "                else:\n",
    "                    print(f\"Max retries (Name Resolver) reached for term: {term}. Moving to the next term.\")\n",
    "    return nr_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "16f9ad3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dry_run_get_nr_response(chunk):\n",
    "    \n",
    "    \"\"\"   Runs Name Resolver   \"\"\"\n",
    "    nr_url = 'https://name-resolution-sri.renci.org/lookup'\n",
    "    nr_terms = []   # list to be populated with dictionaries of Name Resolver names and CURIES for each term searched\n",
    "    max_retries = 5 \n",
    "\n",
    "    for term in chunk:\n",
    "        retries = 0\n",
    "        nr_term = {}\n",
    "        params = {'string':term, 'limit':1} # limit -1 makes this return all available equivalent CURIEs name resolver can give (deprecated)\n",
    "        while retries <= max_retries:\n",
    "            try:\n",
    "#                 r = requests.post(nr_url, params=params)\n",
    "#                 if r.status_code == 200:\n",
    "#                     res = r.json()  # process Name Resolver response\n",
    "                res = {\"dry_run\": \"dry_run\"}\n",
    "                for key, val in res.items():\n",
    "                    nr_term[term] = [key, val[0]]\n",
    "                    nr_terms.append(nr_term) # add Name Resolver name and CURIE to list of dictionaries\n",
    "                break # Break out of the retry loop since the request was successful\n",
    "            except (requests.RequestException, ConnectionResetError, OSError) as ex:\n",
    "                print(f\"\\nName Resolver request failed for term: {term}. Error: {ex}\")\n",
    "                retries += 1\n",
    "                if retries < max_retries:\n",
    "                    print(f\"Retrying ({retries}/{max_retries}) after a delay.\")\n",
    "                    time.sleep(2 ** retries)  # Increase the delay between retries exponentially\n",
    "                else:\n",
    "                    print(f\"Max retries (Name Resolver) reached for term: {term}. Moving to the next term.\")\n",
    "    return nr_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba8719b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunk = df_dict[\"conditions\"].name.unique()[:2000]\n",
    "\n",
    "# nr_url = 'https://name-resolution-sri.renci.org/lookup'\n",
    "# nr_terms = []\n",
    "# max_retries = 5 \n",
    "\n",
    "# for term in chunk:\n",
    "#     retries = 0\n",
    "#     nr_term = {}\n",
    "#     params = {'string':term, 'limit':1} # limit -1 makes this return all available equivalent CURIEs name resolver can give \n",
    "#     while retries <= max_retries:\n",
    "#         try:\n",
    "#             r = requests.post(nr_url, params=params)\n",
    "#             if r.status_code == 200:\n",
    "#                 res = r.json()\n",
    "#                 for key, val in res.items():\n",
    "#                     nr_term[term] = [key, val[0]]\n",
    "#                     nr_terms.append(nr_term)\n",
    "#                 break\n",
    "#         except (requests.RequestException, ConnectionResetError, OSError) as ex:\n",
    "#             print(f\"Name Resolver request failed for element: {term}. Error: {ex}\")\n",
    "#             retries += 1\n",
    "#             if retries < max_retries:\n",
    "#                 print(f\"Retrying ({retries}/{max_retries}) after a delay.\")\n",
    "#                 time.sleep(2 ** retries)  # Increase the delay between retries exponentially\n",
    "#             else:\n",
    "#                 print(f\"Max retries (Name Resolver) reached for term: {term}. Moving to next term.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "13fe3e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_parallel_threads_nr(unmapped_chunked):\n",
    "    # multithread implementation for retrieving Name Resolver responses\n",
    "    # Create a ThreadPoolExecutor with the desired number of threads\n",
    "    with concurrent.futures.ThreadPoolExecutor(multiprocessing.cpu_count() - 1) as executor:\n",
    "        # Submit the get_response() function for each item in the list\n",
    "#         futures = [executor.submit(get_nr_response, chunk) for chunk in unmapped_chunked]\n",
    "        futures = [executor.submit(dry_run_get_nr_response, chunk) for chunk in unmapped_chunked]\n",
    "\n",
    "        # Retrieve the results as they become available\n",
    "        output = [future.result() for future in concurrent.futures.as_completed(futures)]\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "809714e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_list(lst, sublist_length):\n",
    "    return [lst[i:i+sublist_length] for i in range(0, len(lst), sublist_length)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2f966a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fuzzy matching explained: https://www.datacamp.com/tutorial/fuzzy-string-python\n",
>>>>>>> Stashed changes
    "\n",
    "def get_token_sort_ratio(str1, str2):\n",
    "    \"\"\" fuzzy matching explained: https://www.datacamp.com/tutorial/fuzzy-string-python \"\"\"\n",
    "    try:\n",
    "        return fuzz.token_sort_ratio(str1, str2)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def get_similarity_score(str1, str2):\n",
    "    \"\"\" fuzzy matching explained: https://www.datacamp.com/tutorial/fuzzy-string-python \"\"\"\n",
    "    try:\n",
    "        return fuzz.ratio(str1, str2)\n",
    "    except:\n",
    "        return None\n",
<<<<<<< Updated upstream
    "    \n",
    "def convert_seconds_to_hms(seconds):\n",
    "    \"\"\" converts the elapsed time or runtime to hours, min, sec \"\"\"\n",
    "    hours = seconds // 3600\n",
    "    seconds %= 3600\n",
    "    minutes = seconds // 60\n",
    "    seconds %= 60\n",
    "    return hours, minutes, seconds\n",
    "\n",
    "def de_ascii_er(text):\n",
    "    non_ascii = \"[^\\x00-\\x7F]\"\n",
    "    pattern = re.compile(r\"[^\\x00-\\x7F]\")\n",
    "    non_ascii_text = re.sub(pattern, ' ', text)\n",
    "    return non_ascii_text\n",
    "\n",
    "def start_metamap_servers(metamap_dirs):\n",
    "    global metamap_pos_server_dir\n",
    "    global metamap_wsd_server_dir\n",
    "    metamap_pos_server_dir = 'bin/skrmedpostctl' # Part of speech tagger\n",
    "    metamap_wsd_server_dir = 'bin/wsdserverctl' # Word sense disambiguation \n",
    "    \n",
    "    metamap_executable_path_pos = os.path.join(metamap_dirs['metamap_base_dir'], metamap_pos_server_dir)\n",
    "    metamap_executable_path_wsd = os.path.join(metamap_dirs['metamap_base_dir'], metamap_wsd_server_dir)\n",
    "    command_pos = [metamap_executable_path_pos, 'start']\n",
    "    command_wsd = [metamap_executable_path_wsd, 'start']\n",
    "\n",
    "    # Start servers, with open portion redirects output of metamap server printing output to NULL\n",
    "    with open(os.devnull, \"w\") as fnull:\n",
    "        result_post = subprocess.call(command_pos, stdout = fnull, stderr = fnull)\n",
    "        result_wsd = subprocess.call(command_wsd, stdout = fnull, stderr = fnull)\n",
    "    sleep(5)\n",
    "\n",
    "def stop_metamap_servers(metamap_dirs):\n",
    "    metamap_executable_path_pos = os.path.join(metamap_dirs['metamap_base_dir'], metamap_pos_server_dir)\n",
    "    metamap_executable_path_wsd = os.path.join(metamap_dirs['metamap_base_dir'], metamap_wsd_server_dir)\n",
    "    command_pos = [metamap_executable_path_pos, 'stop']\n",
    "    command_wsd = [metamap_executable_path_wsd, 'stop']\n",
    "    \n",
    "    # Stop servers, with open portion redirects output of metamap server printing output to NULL\n",
    "    with open(os.devnull, \"w\") as fnull:\n",
    "        result_post = subprocess.call(command_pos, stdout = fnull, stderr = fnull)\n",
    "        result_wsd = subprocess.call(command_wsd, stdout = fnull, stderr = fnull)\n",
    "    sleep(2)  \n",
    "    \n",
    "def check_os():\n",
    "    if \"linux\" in sys.platform:\n",
    "        print(\"Linux platform detected\")\n",
    "        # metamap_base_dir = \"{}/metamap/\".format(pathlib.Path.cwd().parents[0])\n",
    "        metamap_base_dir = \"/users/knarsinh/projects/clinical_trials/metamap/public_mm/\"\n",
    "        metamap_bin_dir = 'bin/metamap20'\n",
    "    else:\n",
    "        metamap_base_dir = '/Volumes/TOSHIBA_EXT/ISB/clinical_trials/public_mm/' # for running on local\n",
    "        metamap_bin_dir = 'bin/metamap18'\n",
    "        \n",
    "    return {\"metamap_base_dir\":metamap_base_dir, \"metamap_bin_dir\":metamap_bin_dir} \n",
    "\n",
    "@sleep_and_retry\n",
    "@limits(calls=CALLS, period=RATE_LIMIT)\n",
    "def check_limit():\n",
    "    ''' Empty function just to check for calls to Name Resolver API '''\n",
    "    return\n",
    "\n",
    "def wrap(x): # use this to convert string objects to dicts \n",
    "    try:\n",
    "        a = ast.literal_eval(x)\n",
    "        return(a)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
=======
    "sim_score = np.vectorize(get_similarity_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa74123b",
   "metadata": {},
   "outputs": [],
   "source": [
>>>>>>> Stashed changes
    "def get_raw_ct_data():\n",
    "    term_program_flag = True\n",
    "    global data_dir\n",
    "    global data_extracted\n",
    "    \n",
    "    try:\n",
    "        # get all the links and associated dates of upload into a dict called date_link\n",
    "        url_all = \"https://aact.ctti-clinicaltrials.org/download\"\n",
    "        response = requests.get(url_all)\n",
    "        soup = BeautifulSoup(response.text, features=\"lxml\")\n",
    "        body = soup.find_all('option') #Find all\n",
    "        date_link = {}\n",
    "        for el in body:\n",
    "            tags = el.find('a')\n",
    "            try:\n",
    "                zip_name = tags.contents[0].split()[0]\n",
    "                date = zip_name.split(\"_\")[0]\n",
    "                date = dt.datetime.strptime(date, '%Y%m%d').date()\n",
    "                date_link[date] = tags.get('href')\n",
    "            except:\n",
    "                pass\n",
    "        latest_file_date = max(date_link.keys())   # get the date of the latest upload\n",
    "        url = date_link[latest_file_date]   # get the corresponding download link of the latest upload so we can download the raw data\n",
    "        date_string = latest_file_date.strftime(\"%m_%d_%Y\")\n",
    "        data_dir = \"{}/data\".format(pathlib.Path.cwd())\n",
    "        data_extracted = data_dir + \"/{}_extracted\".format(date_string)\n",
    "        data_path = \"{}/{}_pipe-delimited-export.zip\".format(data_dir, date_string)\n",
    "    except:\n",
    "        print(\"continue\")\n",
    "\n",
    "    if not os.path.exists(data_path):   # if folder containing most recent data doesn't exist, download and extract it into data folder\n",
    "\n",
    "        term_program_flag = False   # flag below for terminating program if latest download exists (KG is assumed up to date)\n",
<<<<<<< Updated upstream
    "        print(\"Attempting download of Clinical Trial data as of {}\\n\".format(date_string))\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            if response.status_code == 200:\n",
    "                with open(data_path, 'wb') as file:\n",
    "                    file.write(response.content)\n",
    "                print(\"Finished download of zip\")\n",
    "                with zipfile.ZipFile(data_path, 'r') as download:\n",
    "                    print(\"Unzipping data\")\n",
    "                    download.extractall(data_extracted)\n",
    "        except:\n",
    "            print(\"Failed to scrape AACT for download. Please navigate to https://aact.ctti-clinicaltrials.org/download and manually download zip file.\")\n",
    "            print(\"Please store the downloaded zip in the /data directory. This should be the only item besides the cache file, condition manual review file, and intervention manual review file, in the directory at this time.\")\n",
    "            done = input(\"Type Done when done: \")\n",
    "            if done == \"Done\":\n",
    "                data_dir = \"{}/data\".format(pathlib.Path.cwd())\n",
    "                # list_of_files = glob.glob(data_dir + \"/*\") # get all files in directory\n",
    "                try:\n",
    "                    # latest_file = max(list_of_files, key=os.path.getctime) # get the most recent file in the directory\n",
    "                    pattern = os.path.join(data_dir, \"*.zip\")\n",
    "                    zip_file = glob.glob(pattern) # look for file in directory that ends in \".zip\"\n",
    "                    zip_file = zip_file[0]\n",
    "                    print(\"File found at: \")\n",
    "                    print(zip_file)\n",
    "                    # print(latest_file)\n",
    "                    print(\"Please make sure this the correct zip file from AACT\")\n",
    "                    if not os.path.exists(data_extracted):   # if folder of unzipped data does not exist, unzip\n",
    "                        try:\n",
    "                            with zipfile.ZipFile(zip_file, 'r') as download:\n",
    "                                print(\"Unzipping data into\")\n",
    "                                cttime = os.path.getctime(zip_file)\n",
    "                                date_string = dt.datetime.fromtimestamp(cttime).strftime('%m_%d_%Y')\n",
    "                                data_extracted = data_dir + \"/{}_extracted\".format(date_string)\n",
    "                                print(data_extracted)\n",
    "                                download.extractall(data_extracted)\n",
    "                        except:\n",
    "                            pattern = os.path.join(data_dir, \"*_extracted\")\n",
    "                            extracted_file = glob.glob(pattern) # look for file in directory that ends in \"_extracted\"\n",
    "                            data_extracted = extracted_file[0]\n",
    "                            extracted_name = os.path.basename(os.path.normpath(extracted_file[0]))\n",
    "                            date_string = extracted_name.replace('_extracted', '')\n",
    "                            print(\"Assuming data is already unzipped\")\n",
    "                        \n",
    "                except:\n",
    "                    print(\"Unable to download and extract Clincal Trial data.\")\n",
    "                    print(\"Cannot find pipe-delimited zip in /data folder.\")\n",
    "    else:\n",
    "        print(\"KG is already up to date.\")\n",
    "\n",
    "    return {\"term_program_flag\": term_program_flag, \"data_extracted_path\": data_extracted, \"date_string\": date_string}\n",
    "\n",
    "def read_raw_ct_data(flag_and_path, subset_size):\n",
=======
    "        print(\"Downloading Clinical Trial data as of {}\".format(date_string))\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            with open(data_path, 'wb') as file:\n",
    "                file.write(response.content)\n",
    "            print(\"Finished download of zip\")\n",
    "            with zipfile.ZipFile(data_path, 'r') as download:\n",
    "                print(\"Unzipping data\")\n",
    "                download.extractall(data_extracted)\n",
    "        else:\n",
    "            print(\"KG is already up to date.\")\n",
    "    return {\"term_program_flag\": term_program_flag, \"data_extracted_path\": data_extracted}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c7b2c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_raw_ct_data(flag_and_path):\n",
>>>>>>> Stashed changes
    "    if flag_and_path[\"term_program_flag\"]:\n",
    "        print(\"Exiting program. Assuming KG has already been constructed from most recent data dump from AACT.\")\n",
    "        exit()\n",
    "    else:\n",
    "        data_extracted = flag_and_path[\"data_extracted_path\"]\n",
    "        # read in pipe-delimited files \n",
    "        conditions_df = pd.read_csv(data_extracted + '/conditions.txt.gz', sep='|', index_col=False, header=0, on_bad_lines = 'warn')\n",
    "        interventions_df = pd.read_csv(data_extracted + '/interventions.txt.gz', sep='|', index_col=False, header=0, on_bad_lines = 'warn')\n",
    "        interventions_alts_df = pd.read_csv(data_extracted + '/intervention_other_names.txt.gz', sep='|', index_col=False, header=0, on_bad_lines = 'warn')\n",
    "\n",
    "        if subset_size:   # if a subset size is given, we are running this script on a small subset of the dataset\n",
    "            conditions_df = conditions_df.sample(n=subset_size)\n",
    "            interventions_df = interventions_df.sample(n=subset_size)\n",
    "            interventions_alts_df = interventions_alts_df.sample(n=subset_size)\n",
    "    \n",
    "    df_dict = {\"conditions\": conditions_df, \"interventions\": interventions_df, \"interventions_alts\": interventions_alts_df}\n",
    "    return df_dict\n",
    "\n",
    "\n",
    "\n",
    "def cache_manually_selected_terms():    \n",
    "\n",
    "    def return_curie_dict(curie_info_delimited):\n",
    "        keys = [\"mapped_name\", \"mapped_curie\", \"mapped_score\", \"mapped_semtypes\"]\n",
    "        curie_list = curie_info_delimited.split(\" | \")\n",
    "        curie_dict = dict(zip(keys, curie_list))\n",
    "        return curie_dict\n",
    "\n",
    "    files = glob.glob(\"*.xlsx\")\n",
    "    manually_selected_file = [i for i in files if \"manual_review\" in i if not i.startswith(\"~\")][0] # find the file of manual selections\n",
    "    manually_selected = pd.read_excel(manually_selected_file)\n",
    "    cols_to_fill = [\"mapping_tool\", \"term_type\", \"clintrial_term\", \"input_term\"]\n",
    "    manually_selected.loc[:,cols_to_fill] = manually_selected.loc[:,cols_to_fill].ffill()\n",
    "\n",
    "    manually_selected = manually_selected[~manually_selected['manually_selected_CURIE'].isnull()] # get rows where terms were manually chosen\n",
    "    manually_selected.drop([\"mapping_tool_response\"], axis = 1, inplace = True)\n",
    "\n",
    "    manually_selected[\"manually_selected_CURIE\"] = manually_selected[\"manually_selected_CURIE\"].apply(lambda x: return_curie_dict(x)) # convert | delimited strings to CURIE dict\n",
    "    manually_selected[\"score\"] = 1000  # human curated score = 1000\n",
    "    manually_selected.rename(columns = {'manually_selected_CURIE':'mapping_tool_response'}, inplace = True)\n",
    "    manually_selected = manually_selected[[\"mapping_tool\", \"term_type\", \"clintrial_term\", \"input_term\", \"mapping_tool_response\", \"score\"]] # reorder columns to be same as the cache files we're appending to \n",
    "    manually_selected.to_csv(\"mapping_cache.tsv\", mode='a', header=False, sep =\"\\t\", index=False)\n",
    "\n",
    "\n",
    "def check_against_cache(df_dict):\n",
    "    \n",
    "    conditions_list = df_dict['conditions'].name.unique().tolist()\n",
    "    conditions_list = [str(i) for i in conditions_list]\n",
    "    conditions_list = list(set([i.lower() for i in conditions_list]))\n",
    "    \n",
    "    interventions_list = df_dict['interventions'].name.unique().tolist()\n",
    "    interventions_list = [str(i) for i in interventions_list]\n",
    "    interventions_list = list(set([i.lower() for i in interventions_list]))\n",
    "    \n",
    "    interventions_alts_list = df_dict['interventions_alts'].name.unique().tolist()\n",
    "    interventions_alts_list = [str(i) for i in interventions_alts_list]\n",
    "    interventions_alts_list = list(set([i.lower() for i in interventions_alts_list]))\n",
    "    \n",
    "    try:\n",
    "        cache_manually_selected_terms()\n",
    "    except:\n",
    "        print(\"No manually selected terms file found\")\n",
    "    \n",
    "    try:        \n",
    "        cache_df = pd.read_csv(\"mapping_cache.tsv\", sep =\"\\t\", index_col=False, header=0, on_bad_lines = 'warn')\n",
    "        \n",
    "        conditions_cache = cache_df[cache_df[\"term_type\"] == \"condition\"]\n",
    "        conditions_cache = conditions_cache['clintrial_term'].unique().tolist()\n",
    "        conditions_cache = list(set([i.lower() for i in conditions_cache]))\n",
    "        \n",
    "        conditions_new = [x for x in conditions_list if x not in conditions_cache] # find conditions not in the cache (i.g. new conditions to map)\n",
    "        conditions_new = list(filter(None, conditions_new))\n",
    "        conditions_new = [str(i) for i in conditions_new]\n",
    "        \n",
    "        interventions_cache = cache_df[cache_df[\"term_type\"] == \"intervention\"]\n",
    "        interventions_cache = interventions_cache['clintrial_term'].unique().tolist()\n",
    "        interventions_cache = list(set([i.lower() for i in interventions_cache]))\n",
    "        \n",
    "        interventions_new = [x for x in interventions_list if x not in interventions_cache] # find interventions not in the cache (i.g. new interventions to map)\n",
    "        interventions_new = list(filter(None, interventions_new))\n",
    "        interventions_new = [str(i) for i in interventions_new]\n",
    "        \n",
    "        interventions_alts_cache = cache_df[cache_df[\"term_type\"] == \"intervention_alternate\"]\n",
    "        interventions_alts_cache = interventions_alts_cache['clintrial_term'].unique().tolist()\n",
    "        interventions_alts_cache = list(set([i.lower() for i in interventions_alts_cache]))\n",
    "        \n",
    "        interventions_alts_new = [x for x in interventions_alts_list if x not in interventions_alts_cache] # find interventions_alts not in the cache (i.g. new interventions_alts to map)\n",
    "        interventions_alts_new = list(filter(None, interventions_alts_new))\n",
    "        interventions_alts_new = [str(i) for i in interventions_alts_new]\n",
    "        \n",
    "    except:\n",
    "        print(\"No cache of terms found. Proceeding to map entire KG from scratch\")\n",
    "        conditions_new = conditions_list\n",
    "        interventions_new = interventions_list\n",
    "        interventions_alts_new = interventions_alts_list\n",
    "        \n",
    "    dict_new_terms = {\"conditions\": conditions_new, \"interventions\": interventions_new, \"interventions_alts\": interventions_alts_new}\n",
    "\n",
<<<<<<< Updated upstream
    "    return dict_new_terms\n",
    "\n",
    "\n",
    "def get_nr_response(orig_term):\n",
    "    def create_session():\n",
    "        s = requests.Session()\n",
    "        return s\n",
    " \n",
    "    sess = create_session()\n",
    " \n",
    "    \"\"\"   Runs Name Resolver   \"\"\"\n",
    "    nr_url = 'https://name-resolution-sri.renci.org/lookup'\n",
    "    max_retries = 3 \n",
    "    \n",
    "    input_term = orig_term # in MetaMap, we have to potentially deascii the term and lower case it...for Name Resolver, we don't need to do that. To keep columns consist with MetaMap output, we just keep it and say the original term and the input term are the same. For MetaMap, they might be different\n",
    "    retries = 0\n",
    "    params = {'string':orig_term, 'limit':1} # limit -1 makes this return all available equivalent CURIEs name resolver can give (deprecated)\n",
    "    while retries <= max_retries:\n",
=======
    "    return {\"conditions\": conditions_df, \"interventions\": interventions_df, \"browse_conditions\": browse_conditions_df, \"browse_interventions\": browse_interventions_df}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "044d0f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exact_match_mesh(df_dict):\n",
    "    \n",
    "    # -------    CONDITIONS    ------- #\n",
    "    conditions = df_dict[\"conditions\"]\n",
    "    browse_conditions = df_dict[\"browse_conditions\"] \n",
    "\n",
    "    tomap_conditions = conditions[\"downcase_name\"].values.tolist()\n",
    "    tomap_conditions = list(set(tomap_conditions))\n",
    "    print(\"Number of unique conditions in this Clinical Trials data dump: {}\".format(len(tomap_conditions)))\n",
    "    mesh_exact_mapped = list(set(tomap_conditions).intersection(browse_conditions.downcase_mesh_term.unique()))\n",
    "    print(\"Number of unique conditions that have an exact MeSH term match given in this dump: {}\".format(len(mesh_exact_mapped)))\n",
    "\n",
    "    print(\"since these MeSH terms don't come with identifers, we retrieve them from Name Resolver\")\n",
    "    mesh_exact_mapped_chunked = split_list(mesh_exact_mapped, sublist_length)\n",
    "    mesh_exact_mapped_curied = run_parallel_threads_nr(mesh_exact_mapped_chunked)\n",
    "\n",
    "    mesh_exact_mapped_curied = [element for sublist in mesh_exact_mapped_curied for element in sublist] # flatten the list of lists\n",
    "    mesh_exact_mapped_curied = {key: value for dictionary in mesh_exact_mapped_curied for key, value in dictionary.items()}\n",
    "\n",
    "    mapped_conditions = pd.DataFrame({\"condition_input\": list(mesh_exact_mapped_curied.keys()), # get dataframe of exact MeSH mapped conditions\n",
    "                                      \"condition_CURIE_id\": [value[0] for value in mesh_exact_mapped_curied.values()],\n",
    "                                      \"condition_CURIE_name\": [value[-1] for value in mesh_exact_mapped_curied.values()],\n",
    "                                      \"source\": \"MeSH term exact mapped, Name Resolver CURIE\"})\n",
    "    unmapped_conditions = list(set(tomap_conditions)-set(mapped_conditions.condition_input))\n",
    "    print(\"Number of unique conditions that are unmapped after finding exact MeSH mappings and using Name Resolver to get CURIES: {}\".format(len(unmapped_conditions)))\n",
    "\n",
    "    # -------    INTERVENTIONS    ------- #\n",
    "    interventions = df_dict[\"interventions\"]\n",
    "    browse_interventions = df_dict[\"browse_interventions\"] \n",
    "\n",
    "    tomap_interventions = interventions[\"name\"].values.tolist()\n",
    "    tomap_interventions = reduce(lambda a, b: a+[str(b)], tomap_interventions, [])\n",
    "    tomap_interventions = [string.lower() for string in tomap_interventions] # lowercase the strings\n",
    "    tomap_interventions = list(set(tomap_interventions))\n",
    "    print(\"Number of unique interventions in this Clinical Trials data dump: {}\".format(len(tomap_interventions)))\n",
    "    mesh_exact_mapped = list(set(tomap_interventions).intersection(browse_interventions.downcase_mesh_term.unique()))\n",
    "    print(\"Number of unique interventions that have an exact MeSH term match given in this dump: {}\".format(len(mesh_exact_mapped)))\n",
    "\n",
    "    print(\"since these MeSH terms don't come with identifers, we retrieve them from Name Resolver\")\n",
    "    mesh_exact_mapped_chunked = split_list(mesh_exact_mapped, sublist_length)\n",
    "    mesh_exact_mapped_curied = run_parallel_threads_nr(mesh_exact_mapped_chunked)\n",
    "    mesh_exact_mapped_curied = [element for sublist in mesh_exact_mapped_curied for element in sublist] # flatten the list of lists\n",
    "    mesh_exact_mapped_curied = {key: value for dictionary in mesh_exact_mapped_curied for key, value in dictionary.items()}\n",
    "    mapped_interventions = pd.DataFrame({\"intervention_input\": list(mesh_exact_mapped_curied.keys()),    # get dataframe of exact MeSH mapped interventions\n",
    "                                         \"intervention_CURIE_id\": [value[0] for value in mesh_exact_mapped_curied.values()],\n",
    "                                         \"intervention_CURIE_name\": [value[-1] for value in mesh_exact_mapped_curied.values()],\n",
    "                                         \"source\": \"MeSH term exact mapped, Name Resolver CURIE\"})\n",
    "\n",
    "    unmapped_interventions = list(set(tomap_interventions)-set(mapped_interventions.intervention_input))\n",
    "    print(\"Number of unique interventions that are unmapped after finding exact MeSH mappings and using Name Resolver to get CURIES: {}\".format(len(unmapped_interventions)))\n",
    "\n",
    "    ct_terms = {'mapped_conditions': mapped_conditions,\n",
    "                'unmapped_conditions': unmapped_conditions,\n",
    "                'mapped_interventions': mapped_interventions,\n",
    "                'unmapped_interventions': unmapped_interventions}\n",
    "    return ct_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea63c690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inexact_match_mesh(df_dict, ct_terms):\n",
    "    \n",
    "    # get dataframes bc I'm going to compute fuzzy scores and dump into columns\n",
    "    # find unmapped terms AND THEIR CORRESPONDING NCITS!\n",
    "    # get the conditions that have exact MESH term matches, and conditions that don't have exact MESH term matches. We want to filter for rows that don't have exact MESH term matches bc we already captured those and don't want to run scoring on it\n",
    "\n",
    "    # -------    CONDITIONS    ------- #\n",
    "\n",
    "    print(\"Use fuzzy matching on MeSH terms from Clinical Trials dump to find more potential matches.\")\n",
    "    conditions = df_dict[\"conditions\"] \n",
    "    conditions = conditions[[\"nct_id\", \"downcase_name\"]]\n",
    "\n",
    "    browse_conditions = df_dict[\"browse_conditions\"] \n",
    "    all_mesh_conditions = browse_conditions.downcase_mesh_term.unique()\n",
    "    mask = np.isin(conditions['downcase_name'], all_mesh_conditions)\n",
    "    conditions = conditions.assign(mesh_conditions_exact_mapped = np.where(mask, conditions['downcase_name'], np.nan))# interventions_unmapped = interventions[interventions['mesh_interventions_exact_mapped'].isnull()] # get the rows where mesh_term is empty bc there was no match there, we will run fuzzy scoring on these rows (I'm getting unmapped interventions along with their NCT IDs, not just the condition, bc I need this to find possible MeSH terms by study)\n",
    "    conditions_unmapped = conditions[conditions['mesh_conditions_exact_mapped'].isnull()] # get the rows where mesh_term is empty bc there was no match there, we will run fuzzy scoring on these rows (I'm getting unmapped conditions along with their NCT IDs, not just the condition, bc I need this to find possible MeSH terms by study)\n",
    "    conditions_unmapped = conditions_unmapped.drop('mesh_conditions_exact_mapped', axis=1) # drop the empty column now\n",
    "\n",
    "    mesh_conditions_per_study = pd.DataFrame(browse_conditions[[\"nct_id\", \"downcase_mesh_term\", \"mesh_type\"]].groupby(\"nct_id\")[\"downcase_mesh_term\"].apply(list)) # get all MeSH terms available for each study\n",
    "\n",
    "    conditions_unmapped_all_mesh_terms = pd.merge(conditions_unmapped, \n",
    "                                                  mesh_conditions_per_study,\n",
    "                                                  how='left',\n",
    "                                                  left_on=['nct_id'],\n",
    "                                                  right_on = ['nct_id'])\n",
    "\n",
    "    # some clinical trials are missing from browse_conditions (those nct_ids are not present in the browse_conditions text) They have NaN in the downcase_mesh_term column\n",
    "    conditions_unmapped_all_mesh_terms = conditions_unmapped_all_mesh_terms[~conditions_unmapped_all_mesh_terms['downcase_mesh_term'].isnull()] # subset or delete rows where either column is empty/Nonetype bc fuzzymatching functions will throw error if handling\n",
    "    conditions_unmapped_all_mesh_terms = conditions_unmapped_all_mesh_terms[~conditions_unmapped_all_mesh_terms['downcase_name'].isnull()] # subset or delete rows where either column is empty/Nonetype bc fuzzymatching functions will throw error if handling\n",
    "\n",
    "    conditions_unmapped_all_mesh_terms = conditions_unmapped_all_mesh_terms.explode('downcase_mesh_term')\n",
    "\n",
    "    sort_ratio = np.vectorize(get_token_sort_ratio)\n",
    "    set_ratio = np.vectorize(get_token_set_ratio)\n",
    "    sim_score = np.vectorize(get_similarity_score)\n",
    "\n",
    "    conditions_unmapped_all_mesh_terms[\"sort_ratio\"] = sort_ratio(conditions_unmapped_all_mesh_terms[[\"downcase_mesh_term\"]].values, conditions_unmapped_all_mesh_terms[[\"downcase_name\"]].values) # generate fuzzy scores based between original and MeSH term\n",
    "    conditions_unmapped_all_mesh_terms[\"sim_score\"] = sim_score(conditions_unmapped_all_mesh_terms[[\"downcase_mesh_term\"]].values, conditions_unmapped_all_mesh_terms[[\"downcase_name\"]].values)\n",
    "    conditions_mesh_fuzz_scored = conditions_unmapped_all_mesh_terms[(conditions_unmapped_all_mesh_terms['sim_score'] > 88) | (conditions_unmapped_all_mesh_terms['sort_ratio'] > 88)]\n",
    "    conditions_mesh_fuzz_scored = conditions_mesh_fuzz_scored.sort_values(by = ['nct_id', 'downcase_name'], ascending = [True, True], na_position = 'first')\n",
    "\n",
    "    conditions_mesh_fuzz_scored = conditions_mesh_fuzz_scored.sort_values(by = ['sim_score', 'sort_ratio'], ascending=[False,False], na_position='last').drop_duplicates(['nct_id', 'downcase_name']).sort_index() # there may be many mesh terms that passed the ratio and score filter; this causes duplicates bc I exploded the df...this line of code picks one and throws away other potential matches for one disease-nct_id pair\n",
    "    conditions_mesh_fuzz_scored = conditions_mesh_fuzz_scored.sort_values(['nct_id'], ascending=False)\n",
    "\n",
    "    keys = list(conditions_mesh_fuzz_scored[[\"nct_id\", \"downcase_name\"]].columns.values)\n",
    "    i1 = conditions_unmapped.set_index(keys).index\n",
    "    i2 = conditions_mesh_fuzz_scored.set_index(keys).index\n",
    "\n",
    "    tomap_conditions = conditions_mesh_fuzz_scored[\"downcase_mesh_term\"].values.tolist()\n",
    "    tomap_conditions = list(set(tomap_conditions))\n",
    "    \n",
    "    print(\"Since these MeSH terms don't come with identifers, we retrieve them from Name Resolver\")\n",
    "    mesh_fuzz_mapped_chunked = split_list(tomap_conditions, sublist_length)\n",
    "    mesh_fuzz_mapped_curied = run_parallel_threads_nr(mesh_fuzz_mapped_chunked)\n",
    "    mesh_fuzz_mapped_curied = [element for sublist in mesh_fuzz_mapped_curied for element in sublist] # flatten the list of lists\n",
    "    print(\"Number of unique conditions mapped using fuzzy matching to MeSH terms: {}\".format(len(mesh_fuzz_mapped_curied)))\n",
    "    mesh_fuzz_mapped_curied = {key: value for dictionary in mesh_fuzz_mapped_curied for key, value in dictionary.items()}\n",
    "    fuzz_mapped_conditions = pd.DataFrame({\"condition_input\": list(mesh_fuzz_mapped_curied.keys()),\n",
    "                                           \"condition_CURIE_id\": [value[0] for value in mesh_fuzz_mapped_curied.values()],\n",
    "                                           \"condition_CURIE_name\": [value[-1] for value in mesh_fuzz_mapped_curied.values()],\n",
    "                                           \"source\": \"MeSH term fuzzy mapped, Name Resolver CURIE\"})\n",
    "    previously_mapped = ct_terms[\"mapped_conditions\"]\n",
    "    combined_mapped_conditions = pd.concat([previously_mapped, fuzz_mapped_conditions], ignore_index=True) # get dataframe of combined previously mapped conditions and additional fuzzy MeSH mapped conditions\n",
    "\n",
    "    all_conditions_list = conditions[\"downcase_name\"].values.tolist()\n",
    "    all_conditions_list = list(set(all_conditions_list))\n",
    "    unmapped_conditions = list(set(all_conditions_list)-set(list(combined_mapped_conditions.condition_input.values)))\n",
    "    print(\"Number of unique conditions that are unmapped after finding fuzzy MeSH mappings and using Name Resolver to get CURIES: {}\".format(len(unmapped_conditions)))\n",
    "\n",
    "    # -------    INTERVENTIONS    ------- #\n",
    "\n",
    "    interventions = df_dict[\"interventions\"] \n",
    "    interventions['downcase_name'] = interventions['name'].str.lower()\n",
    "    interventions = interventions[[\"nct_id\", \"downcase_name\"]]\n",
    "    browse_interventions = df_dict[\"browse_interventions\"] \n",
    "    all_mesh_interventions = browse_interventions.downcase_mesh_term.unique()\n",
    "    mask = np.isin(interventions['downcase_name'], all_mesh_interventions)\n",
    "    interventions = interventions.assign(mesh_interventions_exact_mapped = np.where(mask, interventions['downcase_name'], np.nan))# interventions_unmapped = interventions[interventions['mesh_interventions_exact_mapped'].isnull()] # get the rows where mesh_term is empty bc there was no match there, we will run fuzzy scoring on these rows (I'm getting unmapped interventions along with their NCT IDs, not just the condition, bc I need this to find possible MeSH terms by study)\n",
    "    interventions_unmapped = interventions[interventions['mesh_interventions_exact_mapped'].isnull()] # get the rows where mesh_term is empty bc there was no match there, we will run fuzzy scoring on these rows (I'm getting unmapped interventions along with their NCT IDs, not just the condition, bc I need this to find possible MeSH terms by study)\n",
    "    interventions_unmapped = interventions_unmapped.drop('mesh_interventions_exact_mapped', axis=1) # drop the empty column now\n",
    "    mesh_interventions_per_study = pd.DataFrame(browse_interventions[[\"nct_id\", \"downcase_mesh_term\", \"mesh_type\"]].groupby(\"nct_id\")[\"downcase_mesh_term\"].apply(list)) # get all MeSH terms available for each study\n",
    "    interventions_unmapped_all_mesh_terms = pd.merge(interventions_unmapped, \n",
    "                                                     mesh_interventions_per_study,\n",
    "                                                     how='left',\n",
    "                                                     left_on=['nct_id'],\n",
    "                                                     right_on = ['nct_id'])\n",
    "\n",
    "    # # some clinical trials are missing from browse_interventions (those nct_ids are not present in the browse_interventions text) They have NaN in the downcase_mesh_term column\n",
    "    interventions_unmapped_all_mesh_terms = interventions_unmapped_all_mesh_terms[~interventions_unmapped_all_mesh_terms['downcase_mesh_term'].isnull()] # subset or delete rows where either column is empty/Nonetype bc fuzzymatching functions will throw error if handling\n",
    "    interventions_unmapped_all_mesh_terms = interventions_unmapped_all_mesh_terms[~interventions_unmapped_all_mesh_terms['downcase_name'].isnull()] # subset or delete rows where either column is empty/Nonetype bc fuzzymatching functions will throw error if handling\n",
    "\n",
    "    interventions_unmapped_all_mesh_terms = interventions_unmapped_all_mesh_terms.explode('downcase_mesh_term')\n",
    "\n",
    "    sort_ratio = np.vectorize(get_token_sort_ratio)\n",
    "    set_ratio = np.vectorize(get_token_set_ratio)\n",
    "    sim_score = np.vectorize(get_similarity_score)\n",
    "\n",
    "    interventions_unmapped_all_mesh_terms[\"sort_ratio\"] = sort_ratio(interventions_unmapped_all_mesh_terms[[\"downcase_mesh_term\"]].values, interventions_unmapped_all_mesh_terms[[\"downcase_name\"]].values) # generate fuzzy scores based between original and MeSH term\n",
    "    interventions_unmapped_all_mesh_terms[\"sim_score\"] = sim_score(interventions_unmapped_all_mesh_terms[[\"downcase_mesh_term\"]].values, interventions_unmapped_all_mesh_terms[[\"downcase_name\"]].values)\n",
    "    interventions_mesh_fuzz_scored = interventions_unmapped_all_mesh_terms[(interventions_unmapped_all_mesh_terms['sim_score'] > 88) | (interventions_unmapped_all_mesh_terms['sort_ratio'] > 88)]\n",
    "    interventiaons_mesh_fuzz_scored = interventions_mesh_fuzz_scored.sort_values(by = ['nct_id', 'downcase_name'], ascending = [True, True], na_position = 'first')\n",
    "\n",
    "    interventions_mesh_fuzz_scored = interventions_mesh_fuzz_scored.sort_values(by = ['sim_score', 'sort_ratio'], ascending=[False,False], na_position='last').drop_duplicates(['nct_id', 'downcase_name']).sort_index() # there may be many mesh terms that passed the ratio and score filter; this causes duplicates bc I exploded the df...this line of code picks one and throws away other potential matches for one disease-nct_id pair\n",
    "    interventions_mesh_fuzz_scored = interventions_mesh_fuzz_scored.sort_values(['nct_id'], ascending=False)\n",
    "\n",
    "    keys = list(interventions_mesh_fuzz_scored[[\"nct_id\", \"downcase_name\"]].columns.values)\n",
    "    i1 = interventions_unmapped.set_index(keys).index\n",
    "    i2 = interventions_mesh_fuzz_scored.set_index(keys).index\n",
    "\n",
    "    tomap_interventions = interventions_mesh_fuzz_scored[\"downcase_mesh_term\"].values.tolist()\n",
    "    tomap_interventions = list(set(tomap_interventions))\n",
    "\n",
    "    print(\"Since these MeSH terms don't come with identifers, we retrieve them from Name Resolver\")\n",
    "    mesh_fuzz_mapped_chunked = split_list(tomap_interventions, sublist_length)\n",
    "    mesh_fuzz_mapped_curied = run_parallel_threads_nr(mesh_fuzz_mapped_chunked)\n",
    "    mesh_fuzz_mapped_curied = [element for sublist in mesh_fuzz_mapped_curied for element in sublist] # flatten the list of lists\n",
    "    print(\"Number of unique interventions mapped using fuzzy matching to MeSH terms: {}\".format(len(mesh_fuzz_mapped_curied)))\n",
    "    mesh_fuzz_mapped_curied = {key: value for dictionary in mesh_fuzz_mapped_curied for key, value in dictionary.items()}\n",
    "    fuzz_mapped_interventions = pd.DataFrame({\"intervention_input\": list(mesh_fuzz_mapped_curied.keys()),\n",
    "                                              \"intervention_CURIE_id\": [value[0] for value in mesh_fuzz_mapped_curied.values()],\n",
    "                                              \"intervention_CURIE_name\": [value[-1] for value in mesh_fuzz_mapped_curied.values()],\n",
    "                                              \"source\": \"MeSH term fuzzy mapped, Name Resolver CURIE\"})\n",
    "    previously_mapped = ct_terms[\"mapped_interventions\"]\n",
    "    combined_mapped_interventions = pd.concat([previously_mapped, fuzz_mapped_interventions], ignore_index=True) # get dataframe of combined previously mapped interventions and additional fuzzy MeSH mapped interventions\n",
    "\n",
    "    all_interventions_list = interventions[\"downcase_name\"].values.tolist()\n",
    "    all_interventions_list = list(set(all_interventions_list))\n",
    "    unmapped_interventions = list(set(all_interventions_list)-set(list(combined_mapped_interventions.intervention_input.values)))\n",
    "    print(\"Number of unique interventions that are unmapped after finding fuzzy MeSH mappings and using Name Resolver to get CURIES: {}\".format(len(unmapped_interventions)))\n",
    "\n",
    "\n",
    "    ct_terms = {'mapped_conditions': combined_mapped_conditions,\n",
    "                'unmapped_conditions': unmapped_conditions,\n",
    "                'mapped_interventions': combined_mapped_interventions,\n",
    "                'unmapped_interventions': unmapped_interventions,\n",
    "                \"mesh_conditions_per_study\": mesh_conditions_per_study,\n",
    "                \"mesh_interventions_per_study\": mesh_interventions_per_study}\n",
    "    return ct_terms\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7226c96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def term_list_to_nr(df_dict, ct_terms):\n",
    "    \n",
    "    # -------    CONDITIONS    ------- #\n",
    "\n",
    "    unmapped_conditions = ct_terms[\"unmapped_conditions\"]\n",
    "    conditions_unmapped_chunked = split_list(unmapped_conditions, sublist_length)\n",
    "    nr_conditions = run_parallel_threads_nr(conditions_unmapped_chunked)\n",
    "    nr_conditions = [element for sublist in nr_conditions for element in sublist] # flatten the list of lists\n",
    "    print(\"Number of unique conditions mapped using Name Resolver: {}\".format(len(nr_conditions)))\n",
    "    nr_conditions = {key: value for dictionary in nr_conditions for key, value in dictionary.items()}\n",
    "    nr_conditions_df = pd.DataFrame({\"condition_input\": list(nr_conditions.keys()),\n",
    "                                     \"condition_CURIE_id\": [value[0] for value in nr_conditions.values()],\n",
    "                                     \"condition_CURIE_name\": [value[-1] for value in nr_conditions.values()],\n",
    "                                     \"source\": \"Name Resolver, no further curation\"})\n",
    "    \n",
    "    previously_mapped = ct_terms[\"mapped_conditions\"]\n",
    "    combined_mapped_conditions = pd.concat([previously_mapped, nr_conditions_df], ignore_index=True) # get dataframe of combined previously mapped conditions and additional fuzzy MeSH mapped conditions\n",
    "    \n",
    "    conditions = df_dict[\"conditions\"]\n",
    "    all_conditions_list = conditions[\"downcase_name\"].values.tolist()\n",
    "    all_conditions_list = list(set(all_conditions_list))\n",
    "    unmapped_conditions = list(set(all_conditions_list)-set(list(combined_mapped_conditions.condition_input.values)))\n",
    "    print(\"Number of unique conditions that are unmapped after using Name Resolver: {}\".format(len(unmapped_conditions)))\n",
    "    \n",
    "    # -------    INTERVENTIONS    ------- #\n",
    "    \n",
    "    unmapped_interventions = ct_terms[\"unmapped_interventions\"]\n",
    "    interventions_unmapped_chunked = split_list(unmapped_interventions, sublist_length)\n",
    "    nr_interventions = run_parallel_threads_nr(interventions_unmapped_chunked)\n",
    "    nr_interventions = [element for sublist in nr_interventions for element in sublist] # flatten the list of lists\n",
    "    print(\"Number of unique interventions mapped using Name Resolver: {}\".format(len(nr_interventions)))\n",
    "    nr_interventions = {key: value for dictionary in nr_interventions for key, value in dictionary.items()}\n",
    "    nr_interventions_df = pd.DataFrame({\"intervention_input\": list(nr_interventions.keys()),\n",
    "                                     \"intervention_CURIE_id\": [value[0] for value in nr_interventions.values()],\n",
    "                                     \"intervention_CURIE_name\": [value[-1] for value in nr_interventions.values()],\n",
    "                                     \"source\": \"Name Resolver, no further curation\"})\n",
    "    \n",
    "    previously_mapped = ct_terms[\"mapped_interventions\"]\n",
    "    combined_mapped_interventions = pd.concat([previously_mapped, nr_interventions_df], ignore_index=True) # get dataframe of combined previously mapped interventions and additional fuzzy MeSH mapped interventions\n",
    "    \n",
    "    interventions = df_dict[\"interventions\"]\n",
    "    all_interventions_list = interventions[\"downcase_name\"].values.tolist()\n",
    "    all_interventions_list = list(set(all_interventions_list))\n",
    "    unmapped_interventions = list(set(all_interventions_list)-set(list(combined_mapped_interventions.intervention_input.values)))\n",
    "    print(\"Number of unique interventions that are unmapped after using Name Resolver: {}\".format(len(unmapped_interventions)))\n",
    "    \n",
    "    ct_terms = {'mapped_conditions': combined_mapped_conditions, 'unmapped_conditions': unmapped_conditions, 'mapped_interventions': combined_mapped_interventions, 'unmapped_interventions': unmapped_interventions}\n",
    "    return ct_terms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8caca8de",
   "metadata": {},
   "source": [
    "# USE METAMAP API TO MAP REMAINING TERMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f8ccf1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_service_ticket(serverurl, ticket_granting_ticket, serviceurl):\n",
    "    \"\"\" Obtain a Single-Use Proxy Ticket (also known as service ticket).\n",
    "    Request for a Service Ticket:\n",
    "        POST /cas/v1/tickets/{TGT id} HTTP/1.0\n",
    "    data:\n",
    "           service={form encoded parameter for the service url}\n",
    "    Sucessful Response:\n",
    "        200 OK\n",
    "        ST-1-FFDFHDSJKHSDFJKSDHFJKRUEYREWUIFSD2132\n",
    "    @param serverurl authentication server\n",
    "    @param ticketGrantingTicket a Proxy Granting Ticket.\n",
    "    @param serviceurl url of service with protected resources\n",
    "    @return authentication ticket for service. \"\"\"\n",
    "    resp = requests.post(\"{}/{}\".format(serverurl, ticket_granting_ticket),\n",
    "                         {\"service\": serviceurl})\n",
    "    if resp.status_code == 200:\n",
    "        return resp.content\n",
    "    return 'Error: status: {}'.format(resp.content)\n",
    "\n",
    "def get_ticket(cas_serverurl, apikey, serviceurl):\n",
    "    # set ticket granting ticket server url\n",
    "    tgtserverurl = cas_serverurl + \"/api-key\"\n",
    "    # set service ticket server url\n",
    "    stserverurl = cas_serverurl + \"/tickets\"\n",
    "    tgt = get_ticket_granting_ticket(tgtserverurl, apikey)\n",
    "    return get_service_ticket(stserverurl, tgt, serviceurl)\n",
    "\n",
    "def get_ticket_granting_ticket(tgtserverurl, apikey):\n",
    "    # http://serviceurl/cas/v1/tickets/{TGT id}\n",
    "    response = requests.post(tgtserverurl, {'apikey': apikey},\n",
    "                             headers={'Accept': 'test/plain'})\n",
    "    return extract_tgt_ticket(response.content)\n",
    "\n",
    "def extract_tgt_ticket(htmlcontent):\n",
    "#     print(htmlcontent)\n",
    "    \"Extract ticket granting ticket from HTML.\"    \n",
    "    soup = BeautifulSoup(htmlcontent)\n",
    "#     print(soup.find('form').get(\"action\"))\n",
    "    cas_url = soup.find(\"form\").get(\"action\")\n",
    "    \"Extract ticket granting ticket out of 'action' attribute\"\n",
    "    return cas_url.rsplit('/')[-1]\n",
    "\n",
    "def get_redirect_target(resp):\n",
    "        \"\"\"Receives a Response. Returns a redirect URI or ``None``\"\"\"\n",
    "        # Due to the nature of how requests processes redirects this method will\n",
    "        # be called at least once upon the original response and at least twice\n",
    "        # on each subsequent redirect response (if any).\n",
    "        # If a custom mixin is used to handle this logic, it may be advantageous\n",
    "        # to cache the redirect location onto the response object as a private\n",
    "        # attribute.\n",
    "        if resp.is_redirect:\n",
    "            location = resp.headers[\"location\"]\n",
    "            # Currently the underlying http module on py3 decode headers\n",
    "            # in latin1, but empirical evidence suggests that latin1 is very\n",
    "            # rarely used with non-ASCII characters in HTTP headers.\n",
    "            # It is more likely to get UTF8 header rather than latin1.\n",
    "            # This causes incorrect handling of UTF8 encoded location headers.\n",
    "            # To solve this, we re-encode the location in latin1.\n",
    "#             print(location)\n",
    "            location = location.encode(\"latin1\")\n",
    "#             print(location)\n",
    "#             print(to_native_string(location, \"utf8\"))\n",
    "            return to_native_string(location, \"utf8\")\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "19876639",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metamap_mappings(chunk, args):\n",
    "    \n",
    "    form = {}\n",
    "    form['KSOURCE'] = ksource\n",
    "    form['COMMAND_ARGS'] = args\n",
    "    headers = {'Accept': 'application/json'}\n",
    "\n",
    "    mm_terms = {}\n",
    "    cui_pattern = r\"C\\d+(?=:)\"\n",
    "    name_pattern = r\"(?<=:)[^[]+\"\n",
    "    semtype_pattern = r\"\\[(.*?)\\]\"\n",
    "    \n",
    "    form['APIText'] = chunk\n",
    "    service_ticket = get_ticket(CAS_SERVERURL, apikey, serviceurl)\n",
    "    params = {'ticket': service_ticket}\n",
    "\n",
    "    s = requests.Session()\n",
    "    trycnt = 5  # max try count to receive response from MetaMap Interactive API\n",
    "    while trycnt > 0:\n",
>>>>>>> Stashed changes
    "        try:\n",
    "            r = sess.post(nr_url, params=params)\n",
    "            check_limit() # counts how many requests have been sent to NR. If limit of 40 have been sent, sleeps for 1 min\n",
    "            if r.status_code == 200:\n",
    "                mapping_tool_response = r.json()  # process Name Resolver response\n",
    "                return mapping_tool_response\n",
    "            else:\n",
<<<<<<< Updated upstream
    "                return None\n",
    "        except (requests.RequestException, ConnectionResetError, OSError) as ex:\n",
    "            print(f\"\\nName Resolver request failed for term: {term}. Error: {ex}\")\n",
    "            retries += 1\n",
    "            if retries < max_retries:\n",
    "                print(f\"Retrying ({retries}/{max_retries}) after a delay.\")\n",
    "                time.sleep(2 ** retries)  # Increase the delay between retries exponentially\n",
    "            else:\n",
    "                print(f\"Max retries (Name Resolver) reached for term: {term}.\")\n",
    "                return None\n",
    "\n",
    "# I'm only getting 1 concept from Name Resolver. \n",
    "# Both MetaMap and Name Resolver return several, \n",
    "# but I only take 1 from Name Resolver bc they have a preferred concept.\n",
    "# MetaMap's 2nd or 3rd result is often the best one, so I collect all of them and try to score\"\n",
    "\n",
    "def process_metamap_concept(concept):\n",
    "    concept = concept._asdict()\n",
    "    concept_dict  = {\"mapped_name\": concept.get(\"preferred_name\"),\n",
    "                     \"mapped_curie\": concept.get(\"cui\"),\n",
    "                     \"mapped_score\": concept.get(\"score\"),\n",
    "                     \"mapped_semtypes\": concept.get(\"semtypes\")}\n",
    "    if not concept.get(\"preferred_name\"): # if condition triggered if the concept dict looks like following, where AA is for Abbreviation.... {'index': 'USER', 'aa': 'AA', 'short_form': 'copd', 'long_form': 'chronic obstructive pulmonary disease', 'num_tokens_short_form': '1', 'num_chars_short_form': '4', 'num_tokens_long_form': '7', 'num_chars_long_form': '37', 'pos_info': '43:4'}\n",
    "        concept_dict = None\n",
    "    return concept_dict\n",
    "\n",
    "def process_nameresolver_response(nr_response):              \n",
    "    nr_curie = nr_response[0][\"curie\"]\n",
    "    nr_name = nr_response[0][\"label\"]\n",
    "    nr_type = nr_response[0][\"types\"][0]\n",
    "    nr_score = nr_response[0][\"score\"]\n",
    "    concept_dict = {\"mapped_name\": nr_name,\n",
    "                    \"mapped_curie\": nr_curie,\n",
    "                    \"mapped_score\": nr_score,\n",
    "                    \"mapped_semtypes\": nr_type}\n",
    "    return concept_dict\n",
    "\n",
    "\n",
    "def run_mappers(term_pair, params, term_type, csv_writer):\n",
    "    # check_count()\n",
    "\n",
    "    orig_term = term_pair[0]\n",
    "    input_term = term_pair[1]\n",
    "    from_mapper = []\n",
    "    mm = MetaMap.get_instance(metamap_dirs[\"metamap_base_dir\"] + metamap_dirs[\"metamap_bin_dir\"])\n",
    "\n",
    "    # Format of output TSV: header = ['mapping_tool', 'term_type', 'clintrial_term', 'input_term', 'mapping_tool_response', 'score']\n",
    "\n",
    "    if params.get(\"exclude_sts\") is None: # exclude_sts is used for Interventions. restrict_to_sts is used for Conditions. So, the logic is, if we're mapping Conditions, execute \"if\" part of code. If we're mapping Interventions, execute \"else\" part of code\n",
    "        try:\n",
    "            concepts,error = mm.extract_concepts([input_term],\n",
    "                                                 restrict_to_sts = params[\"restrict_to_sts\"],\n",
    "                                                 term_processing = params[\"term_processing\"],\n",
    "                                                 ignore_word_order = params[\"ignore_word_order\"],\n",
    "                                                 strict_model = params[\"strict_model\"],)\n",
    "                                                    \n",
    "            if concepts:   # if MetaMap gives response, process response\n",
    "                mapping_tool = \"metamap\"\n",
    "                for concept in concepts:\n",
    "                    concept_info = []\n",
    "                    new_concept_dict = process_metamap_concept(concept)\n",
    "                    concept_info.extend([mapping_tool, term_type, orig_term, input_term, new_concept_dict]) # score column is empty, Format of output TSV: header = ['mapping_tool', 'term_type', 'clintrial_term', 'input_term', 'mapping_tool_response', 'score']\n",
    "                    from_mapper.append(concept_info)\n",
    "            else:   # if MetaMap fails, try using Name Resolver and process response\n",
    "                nr_response = get_nr_response(orig_term)\n",
    "                if nr_response: # if Name Resolver gives response, process repsonse\n",
    "                    input_term = orig_term # no preprocessing (lowercasing or deascii-ing) necessary to submit terms to Name Resolver (unlike MetaMap)\n",
    "                    mapping_tool = \"nameresolver\"\n",
    "                    concept_info = []\n",
    "                    new_concept_dict = process_nameresolver_response(nr_response)\n",
    "                    concept_info.extend([mapping_tool, term_type, orig_term, input_term, new_concept_dict]) # Add None for score column, empty bc not scored yet\n",
    "                    from_mapper.append(concept_info)\n",
    "                else:\n",
    "                    concept_info = []\n",
    "                    # print(\"Nothing returned from NR or Metamap\")\n",
    "                    concept_info.extend([\"mapping_tools_failed\", term_type, orig_term, input_term, \"mapping_tools_failed\"])\n",
    "                    from_mapper.append(concept_info)\n",
    "        except:\n",
    "            concept_info = []\n",
    "            # print(\"Nothing returned from NR or Metamap\")\n",
    "            concept_info.extend([\"mapping_tools_failed\", term_type, orig_term, input_term, \"mapping_tools_failed\"])\n",
    "            from_mapper.append(concept_info)\n",
    "            \n",
    "    else:   # Else block triggered if mapping Interventions\n",
    "        try:\n",
    "            concepts,error = mm.extract_concepts([input_term],\n",
    "                                                 exclude_sts = params[\"exclude_sts\"],\n",
    "                                                 term_processing = params[\"term_processing\"],\n",
    "                                                 ignore_word_order = params[\"ignore_word_order\"],\n",
    "                                                 strict_model = params[\"strict_model\"],) \n",
    "                                                   \n",
    "            if concepts:   # if MetaMap gives response, process response\n",
    "                mapping_tool = \"metamap\"\n",
    "                for concept in concepts:\n",
    "                    concept_info = []\n",
    "                    new_concept_dict = process_metamap_concept(concept)\n",
    "                    concept_info.extend([mapping_tool, term_type, orig_term, input_term, new_concept_dict]) # score column is empty, Format of output TSV: header = ['mapping_tool', 'term_type', 'clintrial_term', 'input_term', 'mapping_tool_response', 'score']\n",
    "                    from_mapper.append(concept_info)\n",
    "            else:   # if MetaMap fails, try using Name Resolver and process response\n",
    "                nr_response = get_nr_response(orig_term) \n",
    "                if nr_response: # if Name Resolver gives response, process repsonse\n",
    "                    input_term = orig_term # no preprocessing (lowercasing or deascii-ing) necessary to submit terms to Name Resolver (unlike MetaMap)\n",
    "                    mapping_tool = \"nameresolver\"\n",
    "                    concept_info = []\n",
    "                    new_concept_dict = process_nameresolver_response(nr_response)\n",
    "                    concept_info.extend([mapping_tool, term_type, orig_term, input_term, new_concept_dict])\n",
    "                    from_mapper.append(concept_info)\n",
    "                else:\n",
    "                    concept_info = []\n",
    "                    # print(\"Nothing returned from NR or Metamap\")\n",
    "                    concept_info.extend([\"mapping_tools_failed\", term_type, orig_term, input_term, \"mapping_tools_failed\"])\n",
    "                    from_mapper.append(concept_info)\n",
    "        except:\n",
    "            concept_info = []\n",
    "            # print(\"Nothing returned from NR or Metamap\")\n",
    "            concept_info.extend([\"mapping_tools_failed\", term_type, orig_term, input_term, \"mapping_tools_failed\"])\n",
    "            from_mapper.append(concept_info)\n",
    "      \n",
    "    for result in from_mapper:\n",
    "        # print(result)\n",
    "        if result[0] == \"mapping_tools_failed\":\n",
    "            result.append(-1)\n",
    "        else:\n",
    "            result.append(\"unscored\")\n",
    "        # print(result)\n",
    "\n",
    "        csv_writer.writerow(result)\n",
=======
    "                cui_match = re.findall(cui_pattern, line)\n",
    "                if cui_match: \n",
    "                    cui_match_count +=1\n",
    "                    if cui_match_count > 1: # get only 1st CUI/CURIE per Phrase; continue to next loop iteration to skip lines with more available CUIs\n",
    "                        continue\n",
    "                    cui_info = []\n",
    "                    name_match = re.findall(name_pattern, line)\n",
    "                    semtype_match = re.findall(semtype_pattern, line)\n",
    "                    try: cui_info.append(cui_match[0].strip())\n",
    "                    except: cui_info.append(None)\n",
    "                    try: cui_info.append(name_match[0].strip())\n",
    "                    except: cui_info.append(None)\n",
    "                    try: cui_info.append(semtype_match[0].strip())\n",
    "                    except: cui_info.append(None)\n",
    "                    cuis_per_input.append(cui_info)\n",
    "                    mm_terms[mm_input] = cuis_per_input\n",
    "    return mm_terms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4d233f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_parallel_threads_mm(terms_chunked, args):\n",
    "    # multithread implementation for retrieving MetaMap API responses\n",
    "    # Create a ThreadPoolExecutor with the desired number of threads\n",
    "    with concurrent.futures.ThreadPoolExecutor(multiprocessing.cpu_count() - 1) as executor:\n",
    "        # Submit the get_response() function for each item in the list\n",
    "        futures = [executor.submit(get_metamap_mappings, term, args) for term in terms_chunked]\n",
    "\n",
    "        # Retrieve the results as they become available\n",
    "        output = [future.result() for future in concurrent.futures.as_completed(futures)]\n",
    "    mm_dict = reduce(lambda d1, d2: {**d1, **d2}, output) # merge the list of dicts of MetaMap responses in output into 1 dict\n",
    "    return mm_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5ed1f3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_list_by_char_lim(lst):\n",
    "    result = []\n",
    "    current_sublist = []\n",
    "    current_length = 0\n",
    "    for item in lst:\n",
    "        item_length = len(item)\n",
    "        if current_length + item_length > 9000: # max is 10,000 char allowed by MetaMap\n",
    "            result.append(current_sublist)\n",
    "            current_sublist = []\n",
    "            current_length = 0\n",
    "        item = item + \"\\n\"  # add a \"\\n\" for term processing option by MetaMap, the terms in the input file must be separated by blank lines (https://lhncbc.nlm.nih.gov/ii/tools/MetaMap/Docs/TermProcessing.pdf)\n",
    "        current_sublist.append(item)\n",
    "        current_length += item_length\n",
    "    result.append(current_sublist)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2403b2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def term_list_to_mm(df_dict, ct_terms):\n",
>>>>>>> Stashed changes
    "    \n",
    "def parallelize_mappers(term_pair_list, params, term_type, csv_writer):\n",
    "    \n",
<<<<<<< Updated upstream
    "    start_metamap_servers(metamap_dirs) # start the MetaMap servers\n",
    "    terms_left = len(term_pair_list)\n",
    "    future_to_pair = {}\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=20) as executor:\n",
    "        future_to_pair = {executor.submit(run_mappers, term_pair, params, term_type, csv_writer): term_pair for term_pair in term_pair_list}\n",
    "        try:\n",
    "            for future in concurrent.futures.as_completed(future_to_pair, timeout=180): # timeout after 3 min\n",
    "                term_pair = future_to_pair[future]\n",
    "                try:\n",
    "                    result = future.result()\n",
    "                    # Process result if needed\n",
    "                except Exception as exc:\n",
    "                    print(f\"Job {term_pair} generated an exception: {exc}\")\n",
    "                finally:\n",
    "                    terms_left -= 1\n",
    "                    if terms_left % 10 == 0:\n",
    "                        gc.collect()\n",
    "                        time.sleep(2)\n",
    "        except concurrent.futures.TimeoutError:\n",
    "            print(\"Timeout occurred while processing futures\")\n",
    "            for key, future in future_to_pair.items():\n",
    "                if key not in results:\n",
    "                    future.cancel()\n",
=======
    "    # many MetaMap terms are returned as \"term (term)\". For example, \"Nonessential Amino Acid (Nonessential amino acid)\". This repetition messes up the sort ratio and sim score, so we extract the substrings out of the parenthesis to conduct scoring on those\n",
    "    mm_conditions_df[['condition_CURIE_name_1', 'condition_CURIE_name_2']] = mm_conditions_df['condition_CURIE_name'].str.extract(r'^(.*?)\\s*\\((.*?)\\)$').fillna('NA') # \n",
>>>>>>> Stashed changes
    "\n",
    "    stop_metamap_servers(metamap_dirs) # stop the MetaMap servers\n",
    "\n",
<<<<<<< Updated upstream
=======
    "    mm_conditions_scored = mm_conditions_df.copy()\n",
    "    mm_conditions_scored[\"sort_ratio\"] = sort_ratio(mm_conditions_scored[[\"condition_input\"]].values, mm_conditions_scored[[\"condition_CURIE_name\"]].values) # generate fuzzy scores based between original and MeSH term\n",
    "    mm_conditions_scored[\"sim_score\"] = sim_score(mm_conditions_scored[[\"condition_input\"]].values, mm_conditions_scored[[\"condition_CURIE_name\"]].values)\n",
>>>>>>> Stashed changes
    "\n",
    "def term_list_to_mappers(dict_new_terms):   \n",
    "    metamap_version = [int(s) for s in re.findall(r'\\d+', metamap_dirs.get('metamap_bin_dir'))] # get MetaMap version being run \n",
    "    deasciier = np.vectorize(de_ascii_er) # vectorize function\n",
    "    \n",
    "    # open mapping cache to add mapped terms\n",
    "    mapping_filename = \"mapping_cache.tsv\"\n",
    "    if os.path.exists(mapping_filename):\n",
    "        output = open(mapping_filename, 'a', newline='', encoding=\"utf-8\") \n",
    "        csv_writer = csv.writer(output, delimiter='\\t')\n",
    "    else:\n",
    "        output = open(mapping_filename, 'w+', newline='', encoding='utf-8')\n",
    "        col_names = ['mapping_tool', 'term_type', 'clintrial_term', 'input_term', 'mapping_tool_response', 'score']\n",
    "        csv_writer = csv.writer(output, delimiter='\\t')\n",
    "        csv_writer.writerow(col_names)\n",
    "\n",
    "    #  - Conditions\n",
    "    condition_semantic_type_restriction = ['acab,anab,cgab,comd,dsyn,inpo,mobd,neop,patf,clna,fndg']  # see https://lhncbc.nlm.nih.gov/ii/tools/MetaMap/Docs/SemanticTypes_2018AB.txt for semantic types (\"acab,anab,etc.\")\n",
    "    conditions = dict_new_terms.get(\"conditions\")\n",
    "    condition_params = {\"restrict_to_sts\":condition_semantic_type_restriction, \"term_processing\":True, \"ignore_word_order\":True, \"strict_model\":False} # strict_model and relaxed_model are presumably opposites? relaxed_model = True is what I want, but that option appears to be broken in Pymetamap (returns no results when used). Using strict_model = False instead...\n",
    "    # conditon_term_type = \"condition\"\n",
    "\n",
    "    #  - Interventions\n",
    "    condition_semantic_type_restriction = ['acab,anab,cgab,comd,dsyn,inpo,mobd,neop,patf,clna,fndg']  # see https://lhncbc.nlm.nih.gov/ii/tools/MetaMap/Docs/SemanticTypes_2018AB.txt for semantic types (\"acab,anab,etc.\")\n",
    "    interventions = dict_new_terms.get(\"interventions\")\n",
    "    intervention_params = {\"exclude_sts\":condition_semantic_type_restriction, \"term_processing\":True, \"ignore_word_order\":True, \"strict_model\":False} # strict_model and relaxed_model are presumably opposites? relaxed_model = True is what I want, but that option appears to be broken in Pymetamap (returns no results when used). Using strict_model = False instead...\n",
    "    # intervention_term_type = \"intervention\"\n",
    "\n",
    "    #  - Alternate Interventions\n",
    "    condition_semantic_type_restriction = ['acab,anab,cgab,comd,dsyn,inpo,mobd,neop,patf,clna,fndg']  # see https://lhncbc.nlm.nih.gov/ii/tools/MetaMap/Docs/SemanticTypes_2018AB.txt for semantic types (\"acab,anab,etc.\")\n",
    "    interventions_alts = dict_new_terms.get(\"interventions_alts\")\n",
    "    intervention_alts_params = intervention_params # same params as interventions\n",
    "    # intervention_alternate_term_type = \"intervention_alternate\"\n",
    "    \n",
    "    chunksize = 20\n",
    "    \n",
<<<<<<< Updated upstream
    "    if metamap_version[0] >= 20:\n",
=======
    "    mm_conditions_scored_thresholded = mm_conditions_scored_thresholded.drop(['condition_CURIE_name_1',\n",
    "                                                                              'condition_CURIE_name_2',\n",
    "                                                                              'sort_ratio',\n",
    "                                                                              'sim_score',\n",
    "                                                                              'sort_ratio_1',\n",
    "                                                                              'sim_score_1',\n",
    "                                                                              'sort_ratio_2',\n",
    "                                                                              'sim_score_2'], axis=1)\n",
    "    previously_mapped = ct_terms[\"mapped_conditions\"]\n",
    "    combined_mapped_conditions = pd.concat([previously_mapped, mm_conditions_scored_thresholded], ignore_index=True) # get dataframe of combined previously mapped conditions and additional MetaMapped interventions that passed threshold scoring\n",
    "\n",
    "    conditions = df_dict[\"conditions\"]\n",
    "    all_conditions_list = conditions[\"downcase_name\"].values.tolist()\n",
    "    all_conditions_list = list(set(all_conditions_list))\n",
    "    unmapped_conditions = list(set(all_conditions_list)-set(list(combined_mapped_conditions.condition_input.values)))\n",
    "    print(\"Number of unique conditions that are unmapped after using MetaMap and similarity and ratio score thresholds of 88: {}\".format(len(unmapped_conditions)))\n",
    "          \n",
    "    # -------    INTERVENTIONS    ------- #\n",
    "    print(\"Using UMLS MetaMap to get more mappings for interventions. MetaMap returns mappings, CUIs, and semantic type of mapping.\")\n",
    "    unmapped_interventions = ct_terms[\"unmapped_interventions\"]\n",
    "    interventions_unmapped_chunked = split_list_by_char_lim(unmapped_interventions)\n",
    "    # see MetaMap Usage instructions: https://lhncbc.nlm.nih.gov/ii/tools/MetaMap/Docs/MM_2016_Usage.pdf\n",
    "    # removing sosy semantic type (sign or symptom) - often get MetaMap matches to the sign or symptom instead of the full disease...for example, will get back \"exercise-induced\" instead of \"immune dysfunction\" for \"exercise-induced immune dysfunction\" bc it matches the descriptive quality \"exercise-induced\" is matched on \n",
    "    intervention_args = ['--sldi -I -C -k acab,anab,cgab,comd,dsyn,inpo,mobd,neop,patf,sosy -z -i -f']  # see https://lhncbc.nlm.nih.gov/ii/tools/MetaMap/Docs/SemanticTypes_2018AB.txt for semantic types (\"acab,anab,etc.\") (I used inverse of semantic terms picked for conditions here)\n",
    "    mm_interventions = run_parallel_threads_mm(interventions_unmapped_chunked, intervention_args)\n",
    "    flattened_mm_interventions = {key: [item for sublist in value for item in sublist] for key, value in mm_interventions.items()}\n",
    "    mm_interventions_df = pd.DataFrame({\"intervention_input\": list(flattened_mm_interventions.keys()),\n",
    "                                        \"intervention_CURIE_id\": [value[0] for value in flattened_mm_interventions.values()],\n",
    "                                        \"intervention_CURIE_name\": [value[1] for value in flattened_mm_interventions.values()],\n",
    "                                        \"intervention_semantic_type\": [value[-1] for value in flattened_mm_interventions.values()],\n",
    "                                        \"source\": \"MetaMap via UMLS, term and CURIE\"})\n",
    "\n",
    "    mm_interventions_df[['intervention_CURIE_name_1', 'intervention_CURIE_name_2']] = mm_interventions_df['intervention_CURIE_name'].str.extract(r'^(.*?)\\s*\\((.*?)\\)$').fillna('NA') # \n",
    "\n",
    "    sort_ratio = np.vectorize(get_token_sort_ratio)\n",
    "    set_ratio = np.vectorize(get_token_set_ratio)\n",
    "    sim_score = np.vectorize(get_similarity_score)\n",
    "\n",
    "    # many MetaMap terms are returned as \"term (term)\". For example, \"Nonessential Amino Acid (Nonessential amino acid)\". This repetition messes up the sort ratio and sim score, so we extract the substrings out of the parenthesis to conduct scoring on those\n",
    "    mm_interventions_scored = mm_interventions_df.copy()\n",
    "    mm_interventions_scored[\"sort_ratio\"] = sort_ratio(mm_interventions_scored[[\"intervention_input\"]].values, mm_interventions_scored[[\"intervention_CURIE_name\"]].values) # generate fuzzy scores based between original and MeSH term\n",
    "    mm_interventions_scored[\"sim_score\"] = sim_score(mm_interventions_scored[[\"intervention_input\"]].values, mm_interventions_scored[[\"intervention_CURIE_name\"]].values)\n",
    "\n",
    "    mm_interventions_scored[\"sort_ratio_1\"] = sort_ratio(mm_interventions_scored[[\"intervention_input\"]].values, mm_interventions_scored[[\"intervention_CURIE_name_1\"]].values) # generate fuzzy scores based between original and MetaMap term\n",
    "    mm_interventions_scored[\"sim_score_1\"] = sim_score(mm_interventions_scored[[\"intervention_input\"]].values, mm_interventions_scored[[\"intervention_CURIE_name_1\"]].values)\n",
    "\n",
    "    mm_interventions_scored[\"sort_ratio_2\"] = sort_ratio(mm_interventions_scored[[\"intervention_input\"]].values, mm_interventions_scored[[\"intervention_CURIE_name_2\"]].values) # generate fuzzy scores based between original and MetaMap term\n",
    "    mm_interventions_scored[\"sim_score_2\"] = sim_score(mm_interventions_scored[[\"intervention_input\"]].values, mm_interventions_scored[[\"intervention_CURIE_name_2\"]].values)\n",
    "\n",
    "    mm_interventions_scored_thresholded = mm_interventions_scored.copy() \n",
    "    mm_interventions_scored_thresholded = mm_interventions_scored_thresholded[(mm_interventions_scored_thresholded['sim_score'] > 88) |\n",
    "                                                                              (mm_interventions_scored_thresholded['sort_ratio'] > 88) |\n",
    "                                                                              (mm_interventions_scored_thresholded['sim_score_1'] > 88) |\n",
    "                                                                              (mm_interventions_scored_thresholded['sort_ratio_1'] > 88) |\n",
    "                                                                              (mm_interventions_scored_thresholded['sim_score_2'] > 88) |\n",
    "                                                                              (mm_interventions_scored_thresholded['sort_ratio_2'] > 88)]\n",
    "    \n",
    "    print(\"Number of unique interventions that are mapped after using MetaMap and similarity and ratio score thresholds of 88: {}\".format(mm_interventions_scored_thresholded.shape[0]))\n",
    "\n",
    "    mm_interventions_scored_thresholded = mm_interventions_scored_thresholded.drop(['intervention_CURIE_name_1',\n",
    "                                                                                    'intervention_CURIE_name_2',\n",
    "                                                                                    'sort_ratio',\n",
    "                                                                                    'sim_score',\n",
    "                                                                                    'sort_ratio_1',\n",
    "                                                                                    'sim_score_1',\n",
    "                                                                                    'sort_ratio_2',\n",
    "                                                                                    'sim_score_2'], axis=1)\n",
    "    previously_mapped = ct_terms[\"mapped_interventions\"]\n",
    "    combined_mapped_interventions = pd.concat([previously_mapped, mm_interventions_scored_thresholded], ignore_index=True) # get dataframe of combined previously mapped interventions and additional MetaMapped interventions that passed threshold scoring\n",
    "    interventions = df_dict[\"interventions\"]\n",
    "    all_interventions_list = interventions[\"downcase_name\"].values.tolist()\n",
    "    all_interventions_list = list(set(all_interventions_list))\n",
    "    unmapped_interventions = list(set(all_interventions_list)-set(list(combined_mapped_interventions.intervention_input.values)))\n",
    "    print(\"Number of unique interventions that are unmapped after using MetaMap and similarity and ratio score thresholds of 88: {}\".format(len(unmapped_interventions)))\n",
    "    ct_terms = {'mapped_conditions': combined_mapped_conditions,\n",
    "                'unmapped_conditions': unmapped_conditions,\n",
    "                'mapped_interventions': combined_mapped_interventions,\n",
    "                'unmapped_interventions': unmapped_interventions,\n",
    "                'all_metamapped_conditions': mm_conditions_df,\n",
    "                'all_metamapped_interventions': mm_interventions_df}\n",
    "\n",
    "\n",
    "    return ct_terms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "69a40f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output all results to TSVs\n",
    "def compile_and_output(df_dict, ct_terms, remaining_unmapped_possible):\n",
    "    print(\"\\n\")\n",
    "    print(\"#   -------- -------- -------- --------  \")\n",
    "    print(\"Final Tallies:\")\n",
    "    print(\"Total # of conditions mapped: {}\".format(ct_terms[\"mapped_conditions\"].shape[0]))\n",
    "    print(\"Total # of interventions mapped: {}\".format(ct_terms[\"mapped_interventions\"].shape[0]))\n",
    "    print(\"Total # of conditions unmapped or not mapped: {}\".format(len(ct_terms[\"unmapped_conditions\"])))\n",
    "    print(\"Total # of interventions unmapped or not mapped: {}\".format(len(ct_terms[\"unmapped_interventions\"])))    \n",
    "    # How many Clinical Trials are there? Well, it's different depending on the Conditions or Interventions dataframes...\n",
    "    conditions_nctids = len(df_dict[\"conditions\"].nct_id.unique())\n",
    "    interventions_nctids = len(df_dict[\"interventions\"].nct_id.unique())\n",
    "    print(\"Number of Clinical Trials NCITs in Conditions table: {}\".format(conditions_nctids))      \n",
    "    print(\"Number of Clinical Trials NCITs in Interventions table: {}\".format(interventions_nctids))\n",
    "    print(\"#   -------- -------- -------- --------  \")\n",
    "\n",
    "    \"\"\" create tables of unused MeSH and MetaMap CURIEs that could be used for unmapped Conditions and Interventions \"\"\"\n",
    "    # -------    CONDITIONS    ------- #\n",
    "    all_conditions = df_dict[\"conditions\"][[\"nct_id\", \"downcase_name\"]]\n",
    "    conditions_mesh = pd.merge(all_conditions, \n",
    "                               remaining_unmapped_possible[\"mesh_conditions_per_study\"],\n",
    "                               how='left',\n",
    "                               left_on=['nct_id'],\n",
    "                               right_on = ['nct_id'])\n",
    "    \n",
    "    metamap_possibilities = remaining_unmapped_possible[\"all_metamapped_conditions\"][[\"condition_input\", \"condition_CURIE_id\", \"condition_CURIE_name\", \"condition_semantic_type\"]]\n",
    "    conditions_mesh_metamap = pd.merge(conditions_mesh, \n",
    "                                       metamap_possibilities,\n",
    "                                       how='left',\n",
    "                                       left_on=['downcase_name'],\n",
    "                                       right_on = ['condition_input'])\n",
    "    \n",
    "    unmapped_conditions_possible_terms = conditions_mesh_metamap[conditions_mesh_metamap['downcase_name'].isin(ct_terms[\"unmapped_conditions\"])]\n",
    "    unmapped_conditions_possible_terms = unmapped_conditions_possible_terms.drop('condition_input', axis=1) # drop the redundant column now\n",
    "    \n",
    "    # -------    INTERVENTIONS    ------- #\n",
    "    all_interventions = df_dict[\"interventions\"][[\"nct_id\", \"downcase_name\"]]\n",
    "    interventions_mesh = pd.merge(all_interventions, \n",
    "                               remaining_unmapped_possible[\"mesh_interventions_per_study\"],\n",
    "                               how='left',\n",
    "                               left_on=['nct_id'],\n",
    "                               right_on = ['nct_id'])\n",
    "    \n",
    "    metamap_possibilities = remaining_unmapped_possible[\"all_metamapped_interventions\"][[\"intervention_input\", \"intervention_CURIE_id\", \"intervention_CURIE_name\", \"intervention_semantic_type\"]]\n",
    "    interventions_mesh_metamap = pd.merge(interventions_mesh, \n",
    "                                       metamap_possibilities,\n",
    "                                       how='left',\n",
    "                                       left_on=['downcase_name'],\n",
    "                                       right_on = ['intervention_input'])\n",
    "    \n",
    "    unmapped_interventions_possible_terms = interventions_mesh_metamap[interventions_mesh_metamap['downcase_name'].isin(ct_terms[\"unmapped_interventions\"])]\n",
    "    unmapped_interventions_possible_terms = unmapped_interventions_possible_terms.drop('intervention_input', axis=1) # drop the redundant column now\n",
    "          \n",
>>>>>>> Stashed changes
    "        \n",
    "        cons_processed = list(zip(conditions, conditions))  # these are lists of the same term repeated twice, bc MetaMap 2020 does not require deasciing, so the 2nd term remains unchanged and is a repeat of the first term\n",
    "        ints_processed = list(zip(interventions, interventions))\n",
    "        ints_alts_processed = list(zip(interventions_alts, interventions_alts))\n",
    "    \n",
<<<<<<< Updated upstream
    "        conditions_chunked = [cons_processed[i:i + chunksize] for i in range(0, len(cons_processed), chunksize)]  \n",
    "        interventions_chunked = [ints_processed[i:i + chunksize] for i in range(0, len(ints_processed), chunksize)]  \n",
    "        interventions_alts_chunked = [ints_alts_processed[i:i + chunksize] for i in range(0, len(ints_alts_processed), chunksize)] \n",
=======
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ea0650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test_or_prod():\n",
    "#     print(\"The test run of this code performs the construction of the KG on a subset of 200 Conditions and 200 Interventions from Clinical Trials.\\n\")\n",
    "#     test_or_prod = input(\"Is this a test run or the production of a new version of the KG? Write T for test, or P for production: \")\n",
    "#     if test_or_prod == \"T\":\n",
    "#         flag_and_path = get_raw_ct_data() # uncomment for production\n",
    "#         flag_and_path[\"term_program_flag\"] = False\n",
    "#         run_ETL_mapping(flag_and_path)\n",
    "#     elif test_or_prod == \"P\":\n",
    "#         flag_and_path = get_raw_ct_data() \n",
    "#         run_ETL_mapping(flag_and_path)\n",
    "#     else:\n",
    "#         print(\"Bad input\")\n",
    "#         sys.exit(0)\n",
>>>>>>> Stashed changes
    "        \n",
    "        print(\"MetaMap version >= 2020, conduct mapping on original terms\")\n",
    "        \n",
<<<<<<< Updated upstream
    "        LENGTH = len(cons_processed)  # Number of iterations required to fill progress bar (pbar)\n",
    "        # pbar = tqdm(total=LENGTH, desc=\"% conditions mapped\", position=0, leave=True, mininterval = LENGTH/20, bar_format='{l_bar}{bar:20}{r_bar}{bar:-10b}')  # Init progress bar\n",
    "        pbar = tqdm(total=LENGTH, desc=\"% conditions mapped\", position=0, leave=True, mininterval = LENGTH/40, bar_format='{l_bar}{bar:40}{r_bar}{bar:-10b}')  # Init progress bar\n",
    "        for chunk in conditions_chunked:\n",
    "            parallelize_mappers(chunk, condition_params, \"condition\", csv_writer)\n",
    "            pbar.update(n=len(chunk))\n",
    "        \n",
    "        LENGTH = len(ints_processed)  # Number of iterations required to fill progress bar (pbar)\n",
    "        pbar = tqdm(total=LENGTH, desc=\"% interventions mapped\", position=0, leave=True, mininterval = LENGTH/40, bar_format='{l_bar}{bar:40}{r_bar}{bar:-10b}')  # Init progress bar\n",
    "        for chunk in interventions_chunked:\n",
    "            parallelize_mappers(chunk, intervention_params, \"intervention\", csv_writer)\n",
    "            pbar.update(n=len(chunk))\n",
    "        \n",
    "        LENGTH = len(ints_alts_processed)  # Number of iterations required to fill progress bar (pbar)\n",
    "        pbar = tqdm(total=LENGTH, desc=\"% alternate interventions mapped\", position=0, leave=True, mininterval = LENGTH/40, bar_format='{l_bar}{bar:40}{r_bar}{bar:-10b}')  # Init progress bar\n",
    "        for chunk in interventions_alts_chunked:\n",
    "            parallelize_mappers(chunk, intervention_alts_params, \"alternate_intervention\", csv_writer)\n",
    "            pbar.update(n=len(chunk))\n",
    "        \n",
    "    else:\n",
    "        print(\"MetaMap version < 2020, conduct mapping on terms after removing ascii characters\")\n",
    "        \n",
    "        deascii_cons = deasciier(conditions)\n",
    "        deascii_ints = deasciier(interventions)\n",
    "        deascii_int_alts = deasciier(interventions_alts)\n",
    "                \n",
    "        cons_processed = list(zip(conditions, deascii_cons)) # these are lists of the original term, and the deasciied term, bc MetaMap 2018 does not process ascii characters\n",
    "        ints_processed = list(zip(interventions, deascii_ints))\n",
    "        ints_alts_processed = list(zip(interventions_alts, deascii_int_alts))\n",
    "        \n",
    "        conditions_chunked = [cons_processed[i:i + chunksize] for i in range(0, len(cons_processed), chunksize)]  \n",
    "        interventions_chunked = [ints_processed[i:i + chunksize] for i in range(0, len(ints_processed), chunksize)]  \n",
    "        interventions_alts_chunked = [ints_alts_processed[i:i + chunksize] for i in range(0, len(ints_alts_processed), chunksize)] \n",
    "        \n",
    "        LENGTH = len(cons_processed)  # Number of iterations required to fill progress bar (pbar)\n",
    "        pbar = tqdm(total=LENGTH, desc=\"% conditions mapped\", position=0, leave=True, mininterval = LENGTH/40, bar_format='{l_bar}{bar:40}{r_bar}{bar:-10b}')  # Init progress bar\n",
    "        for chunk in conditions_chunked:\n",
    "            parallelize_mappers(chunk, condition_params, \"condition\", csv_writer)\n",
    "            pbar.update(n=len(chunk))\n",
    "        \n",
    "        LENGTH = len(ints_processed)  # Number of iterations required to fill progress bar (pbar)\n",
    "        pbar = tqdm(total=LENGTH, desc=\"% interventions mapped\", position=0, leave=True, mininterval = LENGTH/40, bar_format='{l_bar}{bar:40}{r_bar}{bar:-10b}')  # Init progress bar\n",
    "        for chunk in interventions_chunked:\n",
    "            parallelize_mappers(chunk, intervention_params, \"intervention\", csv_writer)\n",
    "            pbar.update(n=len(chunk))\n",
    "        \n",
    "        LENGTH = len(ints_alts_processed)  # Number of iterations required to fill progress bar (pbar)\n",
    "        pbar = tqdm(total=LENGTH, desc=\"% alternate interventions mapped\", position=0, leave=True, mininterval = LENGTH/40, bar_format='{l_bar}{bar:40}{r_bar}{bar:-10b}')  # Init progress bar\n",
    "        for chunk in interventions_alts_chunked:\n",
    "            parallelize_mappers(chunk, intervention_alts_params, \"alternate_intervention\", csv_writer)\n",
    "            pbar.update(n=len(chunk))\n",
=======
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750aab63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def run_ETL_mapping(flag_and_path):\n",
    "#     df_dict = read_raw_ct_data(flag_and_path)\n",
    "#     ct_terms = exact_match_mesh(df_dict)\n",
    "#     ct_terms = inexact_match_mesh(df_dict, ct_terms)\n",
>>>>>>> Stashed changes
    "\n",
    "    output.close()\n",
    "    \n",
    "    # \"\"\" Remove duplicate rows \"\"\"\n",
    "    mapping_filename = \"mapping_cache.tsv\"\n",
    "    cache = pd.read_csv(mapping_filename, sep='\\t', index_col=False, header=0, encoding_errors='ignore')\n",
    "    cache = cache.drop_duplicates()\n",
    "    cache.to_csv(mapping_filename, sep=\"\\t\", index=False, header=True) # output deduplicated cache terms to TSV\n",
    "    \n",
    "\n",
    "def score_mappings():\n",
    "    print(\"Scoring cache\")\n",
    "\n",
    "    def get_max_score(str1, str2, old_score):\n",
    "        \n",
    "        try:\n",
    "            if old_score == \"unscored\":\n",
    "                sortratio_score = get_token_sort_ratio(str1, str2)\n",
    "                similarity_score = get_similarity_score(str1, str2)\n",
    "                max_score = max(sortratio_score, similarity_score)\n",
    "                score = max_score\n",
    "            else:\n",
    "                score = old_score   \n",
    "        except:\n",
    "            score = old_score\n",
    "        return score\n",
    "\n",
    "    def wrap(x): # use this to convert string objects to dicts \n",
    "        try:\n",
    "            a = ast.literal_eval(x)\n",
    "            return(a)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    with pd.read_csv(\"mapping_cache.tsv\", sep='\\t', index_col=False, header=0, on_bad_lines = 'warn', usecols=lambda c: not c.startswith('Unnamed:'), chunksize=1000) as reader:\n",
    "        write_header = True\n",
    "        for chunk in reader:\n",
    "            chunk[\"mapping_tool_response\"] = chunk[\"mapping_tool_response\"].apply(lambda x: wrap(x))\n",
    "            mapping_info = chunk[\"mapping_tool_response\"].apply(pd.Series, dtype='object')\n",
    "            chunk[\"mapped_name\"] = mapping_info[\"mapped_name\"]\n",
    "            chunk[\"score\"] = chunk.apply(lambda x: get_max_score(x['input_term'], x['mapped_name'], x['score']), axis=1) # get score for score rows that are empty/not scored yet\n",
    "            chunk.drop([\"mapped_name\"], axis = 1, inplace = True)\n",
    "            chunk.to_csv(f'mapping_cache_scored_temp.tsv', sep=\"\\t\", index=False, header=write_header, mode = 'a', encoding=\"utf-8\") # output to TSV\n",
    "            write_header = False\n",
    "\n",
    "    os.rename('mapping_cache.tsv','mapping_cache_backup.tsv')       \n",
    "    os.rename('mapping_cache_scored_temp.tsv','mapping_cache.tsv')\n",
    "\n",
    "\n",
    "def output_terms_files():\n",
    "    print(\"Generating output files\")\n",
    "\n",
    "    \"\"\"   Get high scorers   \"\"\"\n",
    "    cache = pd.read_csv(\"mapping_cache.tsv\", sep='\\t', index_col=False, header=0)\n",
    "    cache['score'] = pd.to_numeric(cache['score'], errors='coerce')\n",
    "    highscorers = cache[cache['score'] >= 80] \n",
    "    # test = highscorers.groupby('clintrial_term')\n",
    "    idx = highscorers.groupby('clintrial_term')['score'].idxmax()  # group by the clinical trial term and get the highest scoring\n",
    "    auto_selected = highscorers.loc[idx]\n",
    "    auto_selected.to_csv(f'autoselected_terms.tsv', sep=\"\\t\", index=False, header=True) # output to TSV\n",
    "\n",
    "    \"\"\"   Get low scorers, aggregate for manual selections  \"\"\"\n",
    "    low_scorers = cache[cache['score'] < 80]\n",
    "    manual_review = low_scorers[~low_scorers.clintrial_term.isin(highscorers['clintrial_term'].unique().tolist())] # there are terms autoselected that have mappings that didn't pass threshold too, but we want to consider that term mapped. So get rid of these rows too\n",
    "    mapping_tool_response = manual_review['mapping_tool_response'].apply(lambda x: wrap(x))\n",
    "    manual_review = manual_review.copy()\n",
    "    mapping_tool_response = mapping_tool_response.apply(pd.Series, dtype='object')\n",
    "    manual_review.loc[:, 'mapping_tool_response_lists'] = mapping_tool_response.values.tolist()\n",
    "    manual_review.drop('mapping_tool_response', axis=1, inplace=True)\n",
    "    manual_review = manual_review[[\"mapping_tool\", \"term_type\", \"clintrial_term\", \"mapping_tool_response_lists\", \"input_term\", \"score\"]]\n",
    "    # manual_review['mapping_tool_response_lists'] = manual_review['mapping_tool_response_lists'].apply(lambda x: ' | '.join(x) if isinstance(x, list) else None)  # Multindexing does not work on lists, so remove the CURIE information out of the list to enable this\n",
    "    manual_review['mapping_tool_response'] = [' | '.join(map(str, l)) for l in manual_review['mapping_tool_response_lists']]\n",
    "    manual_review.drop('mapping_tool_response_lists', axis=1, inplace=True)\n",
    "    manual_review = manual_review.sort_values(by=[\"mapping_tool\", \"term_type\", \"clintrial_term\", \"input_term\"], ascending=False)\n",
    "    manual_review.set_index([\"mapping_tool\", \"term_type\", \"clintrial_term\", \"input_term\"], inplace=True)   # create index\n",
    "    # manual_review.drop([\"temp\"], axis = 1, inplace = True)   # drop the temp column\n",
    "    manual_review['manually_selected_CURIE'] = None # make a column \n",
    "    manual_review.to_excel('manual_review.xlsx', engine='xlsxwriter', index=True)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # flag_and_path = get_raw_ct_data() # download raw data\n",
    "\n",
    "    flag_and_path = {\"term_program_flag\": False, \"data_extracted_path\": \"/15TB_2/gglusman/datasets/clinicaltrials/AACT-20240227\", \"date_string\": \"02_27_2024\"}\n",
    "    # flag_and_path = {\"term_program_flag\": False, \"data_extracted_path\": \"/Users/Kamileh/Work/ISB/NCATS_BiomedicalTranslator/Projects/ClinicalTrials/ETL_Python/data/02_27_2024_extracted\", \"date_string\": \"02_27_2024\"}\n",
    "    global metamap_dirs\n",
    "    metamap_dirs = check_os()\n",
    "    subset_size = None\n",
    "    df_dict = read_raw_ct_data(flag_and_path, subset_size) # read the clinical trial data\n",
    "    dict_new_terms = check_against_cache(df_dict) # use the existing cache of MetaMapped terms so that only new terms are mapped\n",
    "    term_list_to_mappers(dict_new_terms)\n",
    "    score_mappings()\n",
    "    output_terms_files()\n",
    "\n",
    "    "
   ]
<<<<<<< Updated upstream
=======
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41753c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "flag_and_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df27b59c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c1040c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573f6a03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8075d50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d35dafb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b27a08e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2221f50e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf764b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6de8531b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique conditions in this Clinical Trials data dump: 104917\n",
      "Number of unique conditions that have an exact MeSH term match given in this dump: 3670\n",
      "since these MeSH terms don't come with identifers, we retrieve them from Name Resolver\n",
      "Number of unique conditions that are unmapped after finding exact MeSH mappings and using Name Resolver to get CURIES: 101248\n",
      "Number of unique interventions in this Clinical Trials data dump: 373600\n",
      "Number of unique interventions that have an exact MeSH term match given in this dump: 2339\n",
      "since these MeSH terms don't come with identifers, we retrieve them from Name Resolver\n",
      "Number of unique interventions that are unmapped after finding exact MeSH mappings and using Name Resolver to get CURIES: 371263\n",
      "Use fuzzy matching on MeSH terms from Clinical Trials dump to find more potential matches.\n",
      "Since these MeSH terms don't come with identifers, we retrieve them from Name Resolver\n",
      "Number of unique conditions mapped using fuzzy matching to MeSH terms: 2156\n",
      "Number of unique conditions that are unmapped after finding fuzzy MeSH mappings and using Name Resolver to get CURIES: 101248\n",
      "Since these MeSH terms don't come with identifers, we retrieve them from Name Resolver\n",
      "Number of unique interventions mapped using fuzzy matching to MeSH terms: 768\n",
      "Number of unique interventions that are unmapped after finding fuzzy MeSH mappings and using Name Resolver to get CURIES: 371263\n",
      "Name Resolver request failed for element: waldenstr(sqrroot)(delta)m s macroglobulinemia. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=waldenstr%28sqrroot%29%28delta%29m+s+macroglobulinemia&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0f17c14d0>: Failed to establish a new connection: [Errno 60] Operation timed out'))Name Resolver request failed for element: 1- primary (sarcomeric) hypertrophic cardiomyopathy. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=1-+primary+%28sarcomeric%29+hypertrophic+cardiomyopathy&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0eb249510>: Failed to establish a new connection: [Errno 60] Operation timed out'))\n",
      "\n",
      "Retrying (1/5) after a delay.Retrying (1/5) after a delay.\n",
      "\n",
      "Name Resolver request failed for element: anesthesia and procedure related time intervals. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Read timed out. (read timeout=None)\n",
      "Retrying (1/5) after a delay.\n",
      "Name Resolver request failed for element: waldenstr(sqrroot)(delta)m s macroglobulinemia. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=waldenstr%28sqrroot%29%28delta%29m+s+macroglobulinemia&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0eb19ae10>: Failed to establish a new connection: [Errno 60] Operation timed out'))\n",
      "Retrying (2/5) after a delay.\n",
      "Name Resolver request failed for element: 1- primary (sarcomeric) hypertrophic cardiomyopathy. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=1-+primary+%28sarcomeric%29+hypertrophic+cardiomyopathy&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0eb56d590>: Failed to establish a new connection: [Errno 60] Operation timed out'))\n",
      "Retrying (2/5) after a delay.\n",
      "Name Resolver request failed for element: anesthesia and procedure related time intervals. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Read timed out. (read timeout=None)\n",
      "Retrying (2/5) after a delay.\n",
      "Name Resolver request failed for element: anesthesia and procedure related time intervals. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=anesthesia+and+procedure+related+time+intervals&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0ec5f4890>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Retrying (3/5) after a delay.\n",
      "Name Resolver request failed for element: 1- primary (sarcomeric) hypertrophic cardiomyopathy. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=1-+primary+%28sarcomeric%29+hypertrophic+cardiomyopathy&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0fa04dad0>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Retrying (3/5) after a delay.\n",
      "Name Resolver request failed for element: waldenstr(sqrroot)(delta)m s macroglobulinemia. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=waldenstr%28sqrroot%29%28delta%29m+s+macroglobulinemia&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0ec74d610>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Retrying (3/5) after a delay.\n",
      "Name Resolver request failed for element: anesthesia and procedure related time intervals. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=anesthesia+and+procedure+related+time+intervals&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0ed398f10>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Retrying (4/5) after a delay.\n",
      "Name Resolver request failed for element: waldenstr(sqrroot)(delta)m s macroglobulinemia. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=waldenstr%28sqrroot%29%28delta%29m+s+macroglobulinemia&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0eafb1dd0>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Retrying (4/5) after a delay.\n",
      "Name Resolver request failed for element: 1- primary (sarcomeric) hypertrophic cardiomyopathy. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=1-+primary+%28sarcomeric%29+hypertrophic+cardiomyopathy&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0eb972f10>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Retrying (4/5) after a delay.\n",
      "Name Resolver request failed for element: anesthesia and procedure related time intervals. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=anesthesia+and+procedure+related+time+intervals&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0eb989dd0>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Max retries (Name Resolver) reached for term: anesthesia and procedure related time intervals. Moving to the next term.\n",
      "Name Resolver request failed for element: anesthesia and procedure related time intervals. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=anesthesia+and+procedure+related+time+intervals&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0eaf48bd0>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Max retries (Name Resolver) reached for term: anesthesia and procedure related time intervals. Moving to the next term.\n",
      "Name Resolver request failed for element: stage iii mucosal melanoma of the head and neck ajcc v7. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=stage+iii+mucosal+melanoma+of+the+head+and+neck+ajcc+v7&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0ed165550>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Retrying (1/5) after a delay.\n",
      "Name Resolver request failed for element: waldenstr(sqrroot)(delta)m s macroglobulinemia. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=waldenstr%28sqrroot%29%28delta%29m+s+macroglobulinemia&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0fa068250>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Max retries (Name Resolver) reached for term: waldenstr(sqrroot)(delta)m s macroglobulinemia. Moving to the next term.\n",
      "Name Resolver request failed for element: 1- primary (sarcomeric) hypertrophic cardiomyopathy. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=1-+primary+%28sarcomeric%29+hypertrophic+cardiomyopathy&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0ecff0fd0>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Max retries (Name Resolver) reached for term: 1- primary (sarcomeric) hypertrophic cardiomyopathy. Moving to the next term.\n",
      "Name Resolver request failed for element: waldenstr(sqrroot)(delta)m s macroglobulinemia. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=waldenstr%28sqrroot%29%28delta%29m+s+macroglobulinemia&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0ed3987d0>: Failed to establish a new connection: [Errno 61] Connection refused'))Name Resolver request failed for element: 1- primary (sarcomeric) hypertrophic cardiomyopathy. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=1-+primary+%28sarcomeric%29+hypertrophic+cardiomyopathy&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0fa35cc10>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Max retries (Name Resolver) reached for term: waldenstr(sqrroot)(delta)m s macroglobulinemia. Moving to the next term.\n",
      "\n",
      "Max retries (Name Resolver) reached for term: 1- primary (sarcomeric) hypertrophic cardiomyopathy. Moving to the next term.\n",
      "Name Resolver request failed for element: coronary artery stenoses. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=coronary+artery+stenoses&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0fa8d5390>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Retrying (1/5) after a delay.\n",
      "Name Resolver request failed for element: age under 85 years. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=age+under+85+years&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0fa693510>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Name Resolver request failed for element: stage iii mucosal melanoma of the head and neck ajcc v7. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=stage+iii+mucosal+melanoma+of+the+head+and+neck+ajcc+v7&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0eb9caf10>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Retrying (2/5) after a delay.\n",
      "Retrying (1/5) after a delay.\n",
      "Name Resolver request failed for element: coronary artery stenoses. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=coronary+artery+stenoses&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0ec74d5d0>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Retrying (2/5) after a delay.\n",
      "Name Resolver request failed for element: age under 85 years. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=age+under+85+years&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0fa7bdf90>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Retrying (2/5) after a delay.\n",
      "Name Resolver request failed for element: stage iii mucosal melanoma of the head and neck ajcc v7. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=stage+iii+mucosal+melanoma+of+the+head+and+neck+ajcc+v7&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0eafb1650>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Retrying (3/5) after a delay.\n",
      "Name Resolver request failed for element: coronary artery stenoses. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=coronary+artery+stenoses&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0fa69e650>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Retrying (3/5) after a delay.\n",
      "Name Resolver request failed for element: age under 85 years. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=age+under+85+years&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0ec4dd790>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Retrying (3/5) after a delay.\n",
      "Name Resolver request failed for element: stage iii mucosal melanoma of the head and neck ajcc v7. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=stage+iii+mucosal+melanoma+of+the+head+and+neck+ajcc+v7&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0eaf48bd0>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Retrying (4/5) after a delay.\n",
      "Name Resolver request failed for element: age under 85 years. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=age+under+85+years&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0ed7b1310>: Failed to establish a new connection: [Errno 61] Connection refused'))Name Resolver request failed for element: coronary artery stenoses. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=coronary+artery+stenoses&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0ed2de910>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Retrying (4/5) after a delay.\n",
      "\n",
      "Retrying (4/5) after a delay.\n",
      "Name Resolver request failed for element: stage iii mucosal melanoma of the head and neck ajcc v7. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=stage+iii+mucosal+melanoma+of+the+head+and+neck+ajcc+v7&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0ed217ed0>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Max retries (Name Resolver) reached for term: stage iii mucosal melanoma of the head and neck ajcc v7. Moving to the next term.\n",
      "Name Resolver request failed for element: stage iii mucosal melanoma of the head and neck ajcc v7. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=stage+iii+mucosal+melanoma+of+the+head+and+neck+ajcc+v7&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0fd67e750>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Max retries (Name Resolver) reached for term: stage iii mucosal melanoma of the head and neck ajcc v7. Moving to the next term.\n",
      "Name Resolver request failed for element: motor evoked potential monitoring. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=motor+evoked+potential+monitoring&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0fa7e9cd0>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Retrying (1/5) after a delay.\n",
      "Name Resolver request failed for element: age under 85 years. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=age+under+85+years&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0fa934590>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Max retries (Name Resolver) reached for term: age under 85 years. Moving to the next term.\n",
      "Name Resolver request failed for element: coronary artery stenoses. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=coronary+artery+stenoses&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0faa9b050>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Max retries (Name Resolver) reached for term: coronary artery stenoses. Moving to the next term.\n",
      "Name Resolver request failed for element: coronary artery stenoses. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=coronary+artery+stenoses&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0fa934f10>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Max retries (Name Resolver) reached for term: coronary artery stenoses. Moving to the next term.\n",
      "Name Resolver request failed for element: motor evoked potential monitoring. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=motor+evoked+potential+monitoring&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0eb9caad0>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Retrying (2/5) after a delay.\n",
      "Name Resolver request failed for element: age under 85 years. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=age+under+85+years&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0fa9340d0>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Max retries (Name Resolver) reached for term: age under 85 years. Moving to the next term.\n",
      "Name Resolver request failed for element: bilateral stent insertion. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=bilateral+stent+insertion&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0ed2de910>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Retrying (1/5) after a delay.\n",
      "Name Resolver request failed for element: clomiphene allergy. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=clomiphene+allergy&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0eb9ca990>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Retrying (1/5) after a delay.\n",
      "Name Resolver request failed for element: bilateral stent insertion. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=bilateral+stent+insertion&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0fa284d50>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Retrying (2/5) after a delay.\n",
      "Name Resolver request failed for element: clomiphene allergy. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=clomiphene+allergy&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0f25b4f10>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Retrying (2/5) after a delay.\n",
      "Name Resolver request failed for element: motor evoked potential monitoring. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=motor+evoked+potential+monitoring&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0ec5f4090>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Retrying (3/5) after a delay.\n",
      "Name Resolver request failed for element: bilateral stent insertion. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=bilateral+stent+insertion&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0eb593bd0>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Retrying (3/5) after a delay.\n",
      "Name Resolver request failed for element: clomiphene allergy. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=clomiphene+allergy&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0fd67e810>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Retrying (3/5) after a delay.\n",
      "Name Resolver request failed for element: motor evoked potential monitoring. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=motor+evoked+potential+monitoring&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0fa284bd0>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Retrying (4/5) after a delay.\n",
      "Name Resolver request failed for element: bilateral stent insertion. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=bilateral+stent+insertion&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0fa8daed0>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Retrying (4/5) after a delay.\n",
      "Name Resolver request failed for element: clomiphene allergy. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=clomiphene+allergy&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0ec371d90>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Retrying (4/5) after a delay.\n",
      "Name Resolver request failed for element: motor evoked potential monitoring. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=motor+evoked+potential+monitoring&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0fabfbed0>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Max retries (Name Resolver) reached for term: motor evoked potential monitoring. Moving to the next term.\n",
      "Name Resolver request failed for element: motor evoked potential monitoring. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=motor+evoked+potential+monitoring&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0fa56da10>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Max retries (Name Resolver) reached for term: motor evoked potential monitoring. Moving to the next term.\n",
      "Name Resolver request failed for element: ectopic pregnancies. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=ectopic+pregnancies&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0fabfbd90>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Retrying (1/5) after a delay.\n",
      "Name Resolver request failed for element: clomiphene allergy. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=clomiphene+allergy&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0f25f6790>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Max retries (Name Resolver) reached for term: clomiphene allergy. Moving to the next term.\n",
      "Name Resolver request failed for element: bilateral stent insertion. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=bilateral+stent+insertion&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0eaf48bd0>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Max retries (Name Resolver) reached for term: bilateral stent insertion. Moving to the next term.\n",
      "Name Resolver request failed for element: bilateral stent insertion. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=bilateral+stent+insertion&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0fd2ec8d0>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Max retries (Name Resolver) reached for term: bilateral stent insertion. Moving to the next term.\n",
      "Name Resolver request failed for element: clomiphene allergy. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=clomiphene+allergy&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0fa3eed10>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Max retries (Name Resolver) reached for term: clomiphene allergy. Moving to the next term.\n",
      "Name Resolver request failed for element: spinal anaesthetic. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=spinal+anaesthetic&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0eafb1a50>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Retrying (1/5) after a delay.\n",
      "Name Resolver request failed for element: cervical spine degenerative disease. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=cervical+spine+degenerative+disease&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0eb593090>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Retrying (1/5) after a delay.\n",
      "Name Resolver request failed for element: ectopic pregnancies. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=ectopic+pregnancies&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0fd3033d0>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Retrying (2/5) after a delay.\n",
      "Name Resolver request failed for element: cervical spine degenerative disease. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=cervical+spine+degenerative+disease&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0fd54dbd0>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Retrying (2/5) after a delay.\n",
      "Name Resolver request failed for element: spinal anaesthetic. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=spinal+anaesthetic&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0ec740c50>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Retrying (2/5) after a delay.\n",
      "Name Resolver request failed for element: ectopic pregnancies. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=ectopic+pregnancies&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0ed217e90>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Retrying (3/5) after a delay.\n",
      "Name Resolver request failed for element: cervical spine degenerative disease. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=cervical+spine+degenerative+disease&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0fa69e650>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Retrying (3/5) after a delay.\n",
      "Name Resolver request failed for element: spinal anaesthetic. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=spinal+anaesthetic&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0f17c1f90>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Retrying (3/5) after a delay.\n",
      "Name Resolver request failed for element: ectopic pregnancies. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=ectopic+pregnancies&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0fabfb590>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Retrying (4/5) after a delay.\n",
      "Name Resolver request failed for element: spinal anaesthetic. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=spinal+anaesthetic&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0ed398f10>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Retrying (4/5) after a delay.\n",
      "Name Resolver request failed for element: cervical spine degenerative disease. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=cervical+spine+degenerative+disease&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0eb92fc90>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Retrying (4/5) after a delay.\n",
      "Name Resolver request failed for element: ectopic pregnancies. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=ectopic+pregnancies&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0fa693610>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Max retries (Name Resolver) reached for term: ectopic pregnancies. Moving to the next term.\n",
      "Name Resolver request failed for element: ectopic pregnancies. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=ectopic+pregnancies&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0eb9f2610>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Max retries (Name Resolver) reached for term: ectopic pregnancies. Moving to the next term.\n",
      "Name Resolver request failed for element: von willebrand factor, deficiency. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=von+willebrand+factor%2C+deficiency&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0fa693d50>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Retrying (1/5) after a delay.\n",
      "Name Resolver request failed for element: spinal anaesthetic. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=spinal+anaesthetic&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0fd624590>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Max retries (Name Resolver) reached for term: spinal anaesthetic. Moving to the next term.\n",
      "Name Resolver request failed for element: cervical spine degenerative disease. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=cervical+spine+degenerative+disease&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0fd5a0f50>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Max retries (Name Resolver) reached for term: cervical spine degenerative disease. Moving to the next term.\n",
      "Name Resolver request failed for element: spinal anaesthetic. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=spinal+anaesthetic&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0f25b49d0>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Max retries (Name Resolver) reached for term: spinal anaesthetic. Moving to the next term.\n",
      "Name Resolver request failed for element: cervical spine degenerative disease. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=cervical+spine+degenerative+disease&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0fd67ea90>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Max retries (Name Resolver) reached for term: cervical spine degenerative disease. Moving to the next term.\n",
      "Name Resolver request failed for element: glioma, mixed. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=glioma%2C+mixed&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0fd67ed10>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Retrying (1/5) after a delay.\n",
      "Name Resolver request failed for element: von willebrand factor, deficiency. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=von+willebrand+factor%2C+deficiency&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0fd67e750>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Retrying (2/5) after a delay.\n",
      "Name Resolver request failed for element: surgical navigation systems. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=surgical+navigation+systems&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0fc6e9f50>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Retrying (1/5) after a delay.\n",
      "Name Resolver request failed for element: glioma, mixed. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=glioma%2C+mixed&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0fd2eced0>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Retrying (2/5) after a delay.\n",
      "Name Resolver request failed for element: surgical navigation systems. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=surgical+navigation+systems&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0f24beb50>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Retrying (2/5) after a delay.\n",
      "Name Resolver request failed for element: von willebrand factor, deficiency. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=von+willebrand+factor%2C+deficiency&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0eb9cac90>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Retrying (3/5) after a delay.\n",
      "Name Resolver request failed for element: glioma, mixed. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=glioma%2C+mixed&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0eb9f2450>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Retrying (3/5) after a delay.\n",
      "Name Resolver request failed for element: surgical navigation systems. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=surgical+navigation+systems&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0f22aee90>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Retrying (3/5) after a delay.\n",
      "Name Resolver request failed for element: von willebrand factor, deficiency. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=von+willebrand+factor%2C+deficiency&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0ed05ed10>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Retrying (4/5) after a delay.\n",
      "Name Resolver request failed for element: glioma, mixed. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=glioma%2C+mixed&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0fa068ad0>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Retrying (4/5) after a delay.\n",
      "Name Resolver request failed for element: surgical navigation systems. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=surgical+navigation+systems&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0f2430990>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Retrying (4/5) after a delay.\n",
      "Name Resolver request failed for element: von willebrand factor, deficiency. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=von+willebrand+factor%2C+deficiency&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0eb989190>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Max retries (Name Resolver) reached for term: von willebrand factor, deficiency. Moving to the next term.\n",
      "Name Resolver request failed for element: von willebrand factor, deficiency. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=von+willebrand+factor%2C+deficiency&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0fa068250>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Max retries (Name Resolver) reached for term: von willebrand factor, deficiency. Moving to the next term.\n",
      "Name Resolver request failed for element: bone fractures. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=bone+fractures&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0fa5274d0>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Retrying (1/5) after a delay.\n",
      "Name Resolver request failed for element: glioma, mixed. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=glioma%2C+mixed&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0eb56d150>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Max retries (Name Resolver) reached for term: glioma, mixed. Moving to the next term.\n",
      "Name Resolver request failed for element: glioma, mixed. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=glioma%2C+mixed&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0ecf20390>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Max retries (Name Resolver) reached for term: glioma, mixed. Moving to the next term.\n",
      "Name Resolver request failed for element: surgical site infection. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=surgical+site+infection&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0fa4d1350>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Retrying (1/5) after a delay.\n",
      "Name Resolver request failed for element: surgical navigation systems. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=surgical+navigation+systems&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0fa7bdcd0>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Max retries (Name Resolver) reached for term: surgical navigation systems. Moving to the next term.\n",
      "Name Resolver request failed for element: bone fractures. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=bone+fractures&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0ed165290>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Retrying (2/5) after a delay.\n",
      "Name Resolver request failed for element: surgical navigation systems. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=surgical+navigation+systems&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0eb56dd50>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Max retries (Name Resolver) reached for term: surgical navigation systems. Moving to the next term.\n",
      "Name Resolver request failed for element: high risk for eating disorder. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=high+risk+for+eating+disorder&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0fa7e9210>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Retrying (1/5) after a delay.\n",
      "Name Resolver request failed for element: surgical site infection. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=surgical+site+infection&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0fcf83cd0>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Retrying (2/5) after a delay.\n",
      "Name Resolver request failed for element: high risk for eating disorder. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=high+risk+for+eating+disorder&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0ec6acd50>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Retrying (2/5) after a delay.\n",
      "Name Resolver request failed for element: bone fractures. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=bone+fractures&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0eb54b290>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Retrying (3/5) after a delay.\n",
      "Name Resolver request failed for element: surgical site infection. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=surgical+site+infection&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0f2430a90>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Retrying (3/5) after a delay.\n",
      "Name Resolver request failed for element: high risk for eating disorder. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=high+risk+for+eating+disorder&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0fa493bd0>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Retrying (3/5) after a delay.\n",
      "Name Resolver request failed for element: bone fractures. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=bone+fractures&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0fa40f0d0>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Retrying (4/5) after a delay.\n",
      "Name Resolver request failed for element: surgical site infection. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=surgical+site+infection&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0f1544b50>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Retrying (4/5) after a delay.\n",
      "Name Resolver request failed for element: high risk for eating disorder. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=high+risk+for+eating+disorder&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0fa72f290>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Retrying (4/5) after a delay.\n",
      "Name Resolver request failed for element: bone fractures. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=bone+fractures&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0fc7e3f90>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Max retries (Name Resolver) reached for term: bone fractures. Moving to the next term.\n",
      "Name Resolver request failed for element: bone fractures. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=bone+fractures&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0f23aaed0>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Max retries (Name Resolver) reached for term: bone fractures. Moving to the next term.\n",
      "Name Resolver request failed for element: locally advanced cutaneous squamous cell carcinoma. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=locally+advanced+cutaneous+squamous+cell+carcinoma&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0f1544550>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Retrying (1/5) after a delay.\n",
      "Name Resolver request failed for element: surgical site infection. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=surgical+site+infection&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0fab28b90>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Max retries (Name Resolver) reached for term: surgical site infection. Moving to the next term.\n",
      "Name Resolver request failed for element: surgical site infection. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=surgical+site+infection&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0fa9dd590>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Max retries (Name Resolver) reached for term: surgical site infection. Moving to the next term.\n",
      "Name Resolver request failed for element: alcohol use/abuse. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=alcohol+use%2Fabuse&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0f2430a90>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Retrying (1/5) after a delay.\n",
      "Name Resolver request failed for element: locally advanced cutaneous squamous cell carcinoma. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=locally+advanced+cutaneous+squamous+cell+carcinoma&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0fa8d50d0>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Retrying (2/5) after a delay.\n",
      "Name Resolver request failed for element: high risk for eating disorder. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=high+risk+for+eating+disorder&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0ec360f50>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Max retries (Name Resolver) reached for term: high risk for eating disorder. Moving to the next term.\n",
      "Name Resolver request failed for element: high risk for eating disorder. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=high+risk+for+eating+disorder&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0eba03e90>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Max retries (Name Resolver) reached for term: high risk for eating disorder. Moving to the next term.\n",
      "Name Resolver request failed for element: alcohol use/abuse. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=alcohol+use%2Fabuse&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0ed59da10>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Retrying (2/5) after a delay.\n",
      "Name Resolver request failed for element: raynaud secondary to other autoimmune disease. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=raynaud+secondary+to+other+autoimmune+disease&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0ec3525d0>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Retrying (1/5) after a delay.\n",
      "Name Resolver request failed for element: raynaud secondary to other autoimmune disease. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=raynaud+secondary+to+other+autoimmune+disease&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0eb1a7390>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Retrying (2/5) after a delay.\n",
      "Name Resolver request failed for element: locally advanced cutaneous squamous cell carcinoma. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=locally+advanced+cutaneous+squamous+cell+carcinoma&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0fc7e3dd0>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Retrying (3/5) after a delay.\n",
      "Name Resolver request failed for element: alcohol use/abuse. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=alcohol+use%2Fabuse&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0fd2bd910>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Retrying (3/5) after a delay.\n",
      "Name Resolver request failed for element: raynaud secondary to other autoimmune disease. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=raynaud+secondary+to+other+autoimmune+disease&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0ed06f050>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Retrying (3/5) after a delay.\n",
      "Name Resolver request failed for element: locally advanced cutaneous squamous cell carcinoma. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=locally+advanced+cutaneous+squamous+cell+carcinoma&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0ec18f6d0>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Retrying (4/5) after a delay.\n",
      "Name Resolver request failed for element: alcohol use/abuse. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=alcohol+use%2Fabuse&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0f23aacd0>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Retrying (4/5) after a delay.\n",
      "Name Resolver request failed for element: raynaud secondary to other autoimmune disease. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=raynaud+secondary+to+other+autoimmune+disease&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0fc702450>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Retrying (4/5) after a delay.\n",
      "Name Resolver request failed for element: locally advanced cutaneous squamous cell carcinoma. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=locally+advanced+cutaneous+squamous+cell+carcinoma&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0eb56d150>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Max retries (Name Resolver) reached for term: locally advanced cutaneous squamous cell carcinoma. Moving to the next term.\n",
      "Name Resolver request failed for element: locally advanced cutaneous squamous cell carcinoma. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=locally+advanced+cutaneous+squamous+cell+carcinoma&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0eda09bd0>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Max retries (Name Resolver) reached for term: locally advanced cutaneous squamous cell carcinoma. Moving to the next term.\n",
      "Name Resolver request failed for element: stimulant use (diagnosis). Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=stimulant+use+%28diagnosis%29&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0ed2ef090>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Retrying (1/5) after a delay.\n",
      "Name Resolver request failed for element: alcohol use/abuse. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=alcohol+use%2Fabuse&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0ec415610>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Max retries (Name Resolver) reached for term: alcohol use/abuse. Moving to the next term.\n",
      "Name Resolver request failed for element: alcohol use/abuse. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=alcohol+use%2Fabuse&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0fa48bb50>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Max retries (Name Resolver) reached for term: alcohol use/abuse. Moving to the next term.\n",
      "Name Resolver request failed for element: neoplasms with mesothelin expression. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=neoplasms+with+mesothelin+expression&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0fa48bed0>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Retrying (1/5) after a delay.\n",
      "Name Resolver request failed for element: stimulant use (diagnosis). Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=stimulant+use+%28diagnosis%29&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0fa7bdcd0>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Retrying (2/5) after a delay.\n",
      "Name Resolver request failed for element: raynaud secondary to other autoimmune disease. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=raynaud+secondary+to+other+autoimmune+disease&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0fd2bd310>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Max retries (Name Resolver) reached for term: raynaud secondary to other autoimmune disease. Moving to the next term.\n",
      "Name Resolver request failed for element: raynaud secondary to other autoimmune disease. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=raynaud+secondary+to+other+autoimmune+disease&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0fc7e3750>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Max retries (Name Resolver) reached for term: raynaud secondary to other autoimmune disease. Moving to the next term.\n",
      "Name Resolver request failed for element: contractility. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=contractility&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0ed2efd90>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Retrying (1/5) after a delay.\n",
      "Name Resolver request failed for element: neoplasms with mesothelin expression. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=neoplasms+with+mesothelin+expression&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0ec4154d0>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Retrying (2/5) after a delay.\n",
      "Name Resolver request failed for element: contractility. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=contractility&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0fa7c0e50>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Retrying (2/5) after a delay.\n",
      "Name Resolver request failed for element: stimulant use (diagnosis). Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=stimulant+use+%28diagnosis%29&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0fa973890>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Retrying (3/5) after a delay.\n",
      "Name Resolver request failed for element: neoplasms with mesothelin expression. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=neoplasms+with+mesothelin+expression&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0ec360990>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Retrying (3/5) after a delay.\n",
      "Name Resolver request failed for element: contractility. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=contractility&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0ec18fed0>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Retrying (3/5) after a delay.\n",
      "Name Resolver request failed for element: stimulant use (diagnosis). Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=stimulant+use+%28diagnosis%29&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0ec415710>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Retrying (4/5) after a delay.\n",
      "Name Resolver request failed for element: neoplasms with mesothelin expression. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=neoplasms+with+mesothelin+expression&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0ed513950>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Retrying (4/5) after a delay.\n",
      "Name Resolver request failed for element: contractility. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=contractility&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0fd60d350>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Retrying (4/5) after a delay.\n",
      "Name Resolver request failed for element: stimulant use (diagnosis). Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=stimulant+use+%28diagnosis%29&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0fa48ba10>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Max retries (Name Resolver) reached for term: stimulant use (diagnosis). Moving to the next term.\n",
      "Name Resolver request failed for element: stimulant use (diagnosis). Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=stimulant+use+%28diagnosis%29&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0eb090c90>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Max retries (Name Resolver) reached for term: stimulant use (diagnosis). Moving to the next term.\n",
      "Name Resolver request failed for element: large brain mets. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=large+brain+mets&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0fa48b550>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Retrying (1/5) after a delay.\n",
      "Name Resolver request failed for element: neoplasms with mesothelin expression. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=neoplasms+with+mesothelin+expression&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0faae3210>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Max retries (Name Resolver) reached for term: neoplasms with mesothelin expression. Moving to the next term.\n",
      "Name Resolver request failed for element: neoplasms with mesothelin expression. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=neoplasms+with+mesothelin+expression&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0faae3e50>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Max retries (Name Resolver) reached for term: neoplasms with mesothelin expression. Moving to the next term.\n",
      "Name Resolver request failed for element: knee joint osteoarthritis. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=knee+joint+osteoarthritis&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0fa7e9210>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Retrying (1/5) after a delay.\n",
      "Name Resolver request failed for element: large brain mets. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=large+brain+mets&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0eb1a7b50>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Retrying (2/5) after a delay.\n",
      "Name Resolver request failed for element: contractility. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=contractility&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0fa936e90>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Max retries (Name Resolver) reached for term: contractility. Moving to the next term.\n",
      "Name Resolver request failed for element: contractility. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=contractility&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0fa9dd490>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Max retries (Name Resolver) reached for term: contractility. Moving to the next term.\n",
      "Name Resolver request failed for element: stage i oropharyngeal carcinoma ajcc v6 and v7. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=stage+i+oropharyngeal+carcinoma+ajcc+v6+and+v7&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0eba03150>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Retrying (1/5) after a delay.\n",
      "Name Resolver request failed for element: knee joint osteoarthritis. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=knee+joint+osteoarthritis&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0eb9f29d0>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Retrying (2/5) after a delay.\n",
      "Name Resolver request failed for element: stage i oropharyngeal carcinoma ajcc v6 and v7. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=stage+i+oropharyngeal+carcinoma+ajcc+v6+and+v7&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0eba03850>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Retrying (2/5) after a delay.\n",
      "Name Resolver request failed for element: large brain mets. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=large+brain+mets&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0fd5323d0>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Retrying (3/5) after a delay.\n",
      "Name Resolver request failed for element: knee joint osteoarthritis. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=knee+joint+osteoarthritis&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0ec18fe90>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Retrying (3/5) after a delay.\n",
      "Name Resolver request failed for element: stage i oropharyngeal carcinoma ajcc v6 and v7. Error: HTTPSConnectionPool(host='name-resolution-sri.renci.org', port=443): Max retries exceeded with url: /lookup?string=stage+i+oropharyngeal+carcinoma+ajcc+v6+and+v7&limit=1 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fe0ed205a90>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "Retrying (3/5) after a delay.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-b8311e1eed45>\u001b[0m in \u001b[0;36mrun_parallel_threads_nr\u001b[0;34m(unmapped_chunked)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;31m# Retrieve the results as they become available\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfuture\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconcurrent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfutures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_completed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfutures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-b8311e1eed45>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;31m# Retrieve the results as they become available\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfuture\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconcurrent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfutures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_completed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfutures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mas_completed\u001b[0;34m(fs, timeout)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m             \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    551\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-ba4f397d0d22>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mmesh_interventions_per_study\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mct_terms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"mesh_interventions_per_study\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mct_terms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mterm_list_to_nr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mct_terms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mct_terms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mterm_list_to_mm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mct_terms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-8ecd5168033d>\u001b[0m in \u001b[0;36mterm_list_to_nr\u001b[0;34m(df_dict, ct_terms)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0munmapped_conditions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mct_terms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"unmapped_conditions\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mconditions_unmapped_chunked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munmapped_conditions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msublist_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mnr_conditions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_parallel_threads_nr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconditions_unmapped_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mnr_conditions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0melement\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msublist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnr_conditions\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0melement\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msublist\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# flatten the list of lists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Number of unique conditions mapped using Name Resolver: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnr_conditions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-b8311e1eed45>\u001b[0m in \u001b[0;36mrun_parallel_threads_nr\u001b[0;34m(unmapped_chunked)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mfutures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mexecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubmit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_nr_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0munmapped_chunked\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;31m# Retrieve the results as they become available\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfuture\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconcurrent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfutures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_completed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfutures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_tb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 623\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    624\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/concurrent/futures/thread.py\u001b[0m in \u001b[0;36mshutdown\u001b[0;34m(self, wait)\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_threads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m                 \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0mshutdown\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_base\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1042\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1044\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1045\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1058\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# already determined that the C code is done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1059\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_stopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1060\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1061\u001b[0m             \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# flag_and_path = get_raw_ct_data() # uncomment for production\n",
    "flag_and_path = {'term_program_flag': False, 'data_extracted_path': '/Users/Kamileh/Work/ISB/NCATS_BiomedicalTranslator/Projects/ClinicalTrials/ETL_Python/data/07_25_2023_extracted'} # comment for production\n",
    "df_dict = read_raw_ct_data(flag_and_path)\n",
    "ct_terms = exact_match_mesh(df_dict)\n",
    "ct_terms = inexact_match_mesh(df_dict, ct_terms)\n",
    "\n",
    "# # pull the available MeSH terms per study out of the returned ct_terms dict \n",
    "mesh_conditions_per_study = ct_terms[\"mesh_conditions_per_study\"]\n",
    "mesh_interventions_per_study = ct_terms[\"mesh_interventions_per_study\"]\n",
    "\n",
    "ct_terms = term_list_to_nr(df_dict, ct_terms)\n",
    "# ct_terms = term_list_to_mm(df_dict, ct_terms)\n",
    "\n",
    "# # pull the available UMLS terms per study out of the returned ct_terms dict \n",
    "# all_metamapped_conditions = ct_terms[\"all_metamapped_conditions\"]\n",
    "# all_metamapped_interventions = ct_terms[\"all_metamapped_interventions\"]\n",
    "\n",
    "# remaining_unmapped_possible = {\"mesh_conditions_per_study\": mesh_conditions_per_study,\n",
    "#                                \"mesh_interventions_per_study\": mesh_interventions_per_study,\n",
    "#                                \"all_metamapped_conditions\": all_metamapped_conditions,\n",
    "#                                \"all_metamapped_interventions\": all_metamapped_interventions}\n",
    "# compile_and_output(df_dict, ct_terms, remaining_unmapped_possible)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1da790",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_seconds_to_hms(seconds):\n",
    "\n",
    "    \"\"\" converts the elapsed or run_time to hours, min, sec \"\"\"\n",
    "    hours = seconds // 3600\n",
    "    seconds %= 3600\n",
    "    minutes = seconds // 60\n",
    "    seconds %= 60\n",
    "    return hours, minutes, seconds\n",
    "\n",
    "current = dt.datetime.now()\n",
    "ts = dt.datetime.timestamp(current)\n",
    "d = dt.datetime.fromtimestamp(ts)\n",
    "str_date_time = d.strftime(\"%d-%m-%Y, %H:%M:%S\")\n",
    "print(\"Timestamp of script start: {}\".format(str_date_time))\n",
    "\n",
    "start_time = time.time()\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "hours, minutes, seconds = convert_seconds_to_hms(elapsed_time)\n",
    "print(f\"Runtime: {hours} hours, {minutes} minutes, {seconds} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad33708d",
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_unmapped_possible"
   ]
>>>>>>> Stashed changes
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
