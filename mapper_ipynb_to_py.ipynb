{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da527b7e-2d30-4660-b6ee-1a63bc9df48f",
   "metadata": {},
   "source": [
    "### THIS SCRIPT USES MetaMap to try and map the bulk of terms, and Name Resolver to pick up what's left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "730291b8-7f12-4e72-bfa6-4c7ae33ae738",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>.output_result { max-width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# display cells to maximum width \n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "display(HTML(\"<style>.output_result { max-width:100% !important; }</style>\"))\n",
    "\n",
    "# lets you preint multiple outputs per cell, not just last\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70b0f433-c7bb-4b98-aa7b-1d1e6ad1e3b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import collections\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pickle\n",
    "from functools import reduce\n",
    "import time\n",
    "from time import sleep\n",
    "import concurrent.futures\n",
    "import multiprocessing\n",
    "import datetime as dt\n",
    "from datetime import date\n",
    "import pathlib\n",
    "import configparser\n",
    "import urllib\n",
    "import zipfile\n",
    "import csv\n",
    "import gc\n",
    "# import sys\n",
    "# sys.path.insert(0, '/Volumes/TOSHIBA_EXT/ISB/clinical_trials/pymetamap-master')\n",
    "# from pymetamap import MetaMap  # https://github.com/AnthonyMRios/pymetamap/blob/master/pymetamap/SubprocessBackend.py\n",
    "from pandas import ExcelWriter\n",
    "import ast\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import subprocess\n",
    "import shlex\n",
    "from collections import Counter\n",
    "from ratelimit import limits, sleep_and_retry\n",
    "\n",
    "# %pip install thefuzz\n",
    "# %pip install levenshtein\n",
    "# %pip install xlsxwriter\n",
    "# %pip install ratelimit\n",
    "# %pip install timeout_decorator\n",
    "\n",
    "from thefuzz import fuzz # fuzzy matching explained: https://www.datacamp.com/tutorial/fuzzy-string-python\n",
    "\n",
    "# 40 calls per minute\n",
    "CALLS = 40\n",
    "RATE_LIMIT = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23d4d831-06e5-4439-920e-b6495d81adc3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_token_sort_ratio(str1, str2):\n",
    "    \"\"\" fuzzy matching explained: https://www.datacamp.com/tutorial/fuzzy-string-python \"\"\"\n",
    "    try:\n",
    "        return fuzz.token_sort_ratio(str1, str2)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def get_similarity_score(str1, str2):\n",
    "    \"\"\" fuzzy matching explained: https://www.datacamp.com/tutorial/fuzzy-string-python \"\"\"\n",
    "    try:\n",
    "        return fuzz.ratio(str1, str2)\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "def convert_seconds_to_hms(seconds):\n",
    "    \"\"\" converts the elapsed time or runtime to hours, min, sec \"\"\"\n",
    "    hours = seconds // 3600\n",
    "    seconds %= 3600\n",
    "    minutes = seconds // 60\n",
    "    seconds %= 60\n",
    "    return hours, minutes, seconds\n",
    "\n",
    "def de_ascii_er(text):\n",
    "    non_ascii = \"[^\\x00-\\x7F]\"\n",
    "    pattern = re.compile(r\"[^\\x00-\\x7F]\")\n",
    "    non_ascii_text = re.sub(pattern, ' ', text)\n",
    "    return non_ascii_text\n",
    "\n",
    "def start_metamap_servers(metamap_dirs):\n",
    "    global metamap_pos_server_dir\n",
    "    global metamap_wsd_server_dir\n",
    "    metamap_pos_server_dir = 'bin/skrmedpostctl' # Part of speech tagger\n",
    "    metamap_wsd_server_dir = 'bin/wsdserverctl' # Word sense disambiguation \n",
    "    \n",
    "    metamap_executable_path_pos = os.path.join(metamap_dirs['metamap_base_dir'], metamap_pos_server_dir)\n",
    "    metamap_executable_path_wsd = os.path.join(metamap_dirs['metamap_base_dir'], metamap_wsd_server_dir)\n",
    "    command_pos = [metamap_executable_path_pos, 'start']\n",
    "    command_wsd = [metamap_executable_path_wsd, 'start']\n",
    "\n",
    "    # Start servers, with open portion redirects output of metamap server printing output to NULL\n",
    "    with open(os.devnull, \"w\") as fnull:\n",
    "        result_post = subprocess.call(command_pos, stdout = fnull, stderr = fnull)\n",
    "        result_wsd = subprocess.call(command_wsd, stdout = fnull, stderr = fnull)\n",
    "    sleep(5)\n",
    "\n",
    "def stop_metamap_servers(metamap_dirs):\n",
    "    metamap_executable_path_pos = os.path.join(metamap_dirs['metamap_base_dir'], metamap_pos_server_dir)\n",
    "    metamap_executable_path_wsd = os.path.join(metamap_dirs['metamap_base_dir'], metamap_wsd_server_dir)\n",
    "    command_pos = [metamap_executable_path_pos, 'stop']\n",
    "    command_wsd = [metamap_executable_path_wsd, 'stop']\n",
    "    \n",
    "    # Stop servers, with open portion redirects output of metamap server printing output to NULL\n",
    "    with open(os.devnull, \"w\") as fnull:\n",
    "        result_post = subprocess.call(command_pos, stdout = fnull, stderr = fnull)\n",
    "        result_wsd = subprocess.call(command_wsd, stdout = fnull, stderr = fnull)\n",
    "    sleep(2)  \n",
    "    \n",
    "def check_os():\n",
    "    if \"linux\" in sys.platform:\n",
    "        print(\"Linux platform detected\")\n",
    "        metamap_base_dir = \"{}/metamap/\".format(pathlib.Path.cwd().parents[0])\n",
    "        metamap_bin_dir = 'bin/metamap20'\n",
    "    else:\n",
    "        metamap_base_dir = '/Volumes/TOSHIBA_EXT/ISB/clinical_trials/public_mm/' # for running on local\n",
    "        metamap_bin_dir = 'bin/metamap18'\n",
    "        \n",
    "    return {\"metamap_base_dir\":metamap_base_dir, \"metamap_bin_dir\":metamap_bin_dir} \n",
    "\n",
    "@sleep_and_retry\n",
    "@limits(calls=CALLS, period=RATE_LIMIT)\n",
    "def check_limit():\n",
    "    ''' Empty function just to check for calls to Name Resolver API '''\n",
    "    return\n",
    "\n",
    "def wrap(x): # use this to convert string objects to dicts \n",
    "    try:\n",
    "        a = ast.literal_eval(x)\n",
    "        return(a)\n",
    "    except:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6746b051-2524-4d93-ad30-69e7fcd225e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_raw_ct_data():\n",
    "    term_program_flag = True\n",
    "    global data_dir\n",
    "    global data_extracted\n",
    "    \n",
    "    try:\n",
    "        # get all the links and associated dates of upload into a dict called date_link\n",
    "        url_all = \"https://aact.ctti-clinicaltrials.org/download\"\n",
    "        response = requests.get(url_all)\n",
    "        soup = BeautifulSoup(response.text, features=\"lxml\")\n",
    "        body = soup.find_all('option') #Find all\n",
    "        date_link = {}\n",
    "        for el in body:\n",
    "            tags = el.find('a')\n",
    "            try:\n",
    "                zip_name = tags.contents[0].split()[0]\n",
    "                date = zip_name.split(\"_\")[0]\n",
    "                date = dt.datetime.strptime(date, '%Y%m%d').date()\n",
    "                date_link[date] = tags.get('href')\n",
    "            except:\n",
    "                pass\n",
    "        latest_file_date = max(date_link.keys())   # get the date of the latest upload\n",
    "        url = date_link[latest_file_date]   # get the corresponding download link of the latest upload so we can download the raw data\n",
    "        date_string = latest_file_date.strftime(\"%m_%d_%Y\")\n",
    "        data_dir = \"{}/data\".format(pathlib.Path.cwd())\n",
    "        data_extracted = data_dir + \"/{}_extracted\".format(date_string)\n",
    "        data_path = \"{}/{}_pipe-delimited-export.zip\".format(data_dir, date_string)\n",
    "    except:\n",
    "        print(\"continue\")\n",
    "\n",
    "    if not os.path.exists(data_path):   # if folder containing most recent data doesn't exist, download and extract it into data folder\n",
    "\n",
    "        term_program_flag = False   # flag below for terminating program if latest download exists (KG is assumed up to date)\n",
    "        print(\"Attempting download of Clinical Trial data as of {}\\n\".format(date_string))\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            if response.status_code == 200:\n",
    "                with open(data_path, 'wb') as file:\n",
    "                    file.write(response.content)\n",
    "                print(\"Finished download of zip\")\n",
    "                with zipfile.ZipFile(data_path, 'r') as download:\n",
    "                    print(\"Unzipping data\")\n",
    "                    download.extractall(data_extracted)\n",
    "        except:\n",
    "            print(\"Failed to scrape AACT for download. Please navigate to https://aact.ctti-clinicaltrials.org/download and manually download zip file.\")\n",
    "            print(\"Please store the downloaded zip in the /data directory. This should be the only item besides the cache file, condition manual review file, and intervention manual review file, in the directory at this time.\")\n",
    "            done = input(\"Type Done when done: \")\n",
    "            if done == \"Done\":\n",
    "                data_dir = \"{}/data\".format(pathlib.Path.cwd())\n",
    "                # list_of_files = glob.glob(data_dir + \"/*\") # get all files in directory\n",
    "                try:\n",
    "                    # latest_file = max(list_of_files, key=os.path.getctime) # get the most recent file in the directory\n",
    "                    pattern = os.path.join(data_dir, \"*.zip\")\n",
    "                    zip_file = glob.glob(pattern) # look for file in directory that ends in \".zip\"\n",
    "                    zip_file = zip_file[0]\n",
    "                    print(\"File found at: \")\n",
    "                    print(zip_file)\n",
    "                    # print(latest_file)\n",
    "                    print(\"Please make sure this the correct zip file from AACT\")\n",
    "                    if not os.path.exists(data_extracted):   # if folder of unzipped data does not exist, unzip\n",
    "                        try:\n",
    "                            with zipfile.ZipFile(zip_file, 'r') as download:\n",
    "                                print(\"Unzipping data into\")\n",
    "                                cttime = os.path.getctime(zip_file)\n",
    "                                date_string = dt.datetime.fromtimestamp(cttime).strftime('%m_%d_%Y')\n",
    "                                data_extracted = data_dir + \"/{}_extracted\".format(date_string)\n",
    "                                print(data_extracted)\n",
    "                                download.extractall(data_extracted)\n",
    "                        except:\n",
    "                            pattern = os.path.join(data_dir, \"*_extracted\")\n",
    "                            extracted_file = glob.glob(pattern) # look for file in directory that ends in \"_extracted\"\n",
    "                            data_extracted = extracted_file[0]\n",
    "                            extracted_name = os.path.basename(os.path.normpath(extracted_file[0]))\n",
    "                            date_string = extracted_name.replace('_extracted', '')\n",
    "                            print(\"Assuming data is already unzipped\")\n",
    "                        \n",
    "                except:\n",
    "                    print(\"Unable to download and extract Clincal Trial data.\")\n",
    "                    print(\"Cannot find pipe-delimited zip in /data folder.\")\n",
    "    else:\n",
    "        print(\"KG is already up to date.\")\n",
    "\n",
    "    return {\"term_program_flag\": term_program_flag, \"data_extracted_path\": data_extracted, \"date_string\": date_string}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4850c49-b00e-422a-805b-137fccb7cd94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_raw_ct_data(flag_and_path, subset_size):\n",
    "    if flag_and_path[\"term_program_flag\"]:\n",
    "        print(\"Exiting program. Assuming KG has already been constructed from most recent data dump from AACT.\")\n",
    "        exit()\n",
    "    else:\n",
    "        data_extracted = flag_and_path[\"data_extracted_path\"]\n",
    "        # read in pipe-delimited files \n",
    "        conditions_df = pd.read_csv(data_extracted + '/conditions.txt', sep='|', index_col=False, header=0, on_bad_lines = 'warn')\n",
    "        interventions_df = pd.read_csv(data_extracted + '/interventions.txt', sep='|', index_col=False, header=0, on_bad_lines = 'warn')\n",
    "        interventions_alts_df = pd.read_csv(data_extracted + '/intervention_other_names.txt', sep='|', index_col=False, header=0, on_bad_lines = 'warn')\n",
    "\n",
    "        if subset_size:   # if a subset size is given, we are running this script on a small subset of the dataset\n",
    "            conditions_df = conditions_df.sample(n=subset_size)\n",
    "            interventions_df = interventions_df.sample(n=subset_size)\n",
    "            interventions_alts_df = interventions_alts_df.sample(n=subset_size)\n",
    "    \n",
    "    df_dict = {\"conditions\": conditions_df, \"interventions\": interventions_df, \"interventions_alts\": interventions_alts_df}\n",
    "    return df_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd612efd-e483-4754-8edd-3dc45fe95e88",
   "metadata": {},
   "source": [
    "# Check against cache, retrieve terms not already mapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eddc0c83-10b4-4065-b2bb-1bad9cb0e63a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def check_against_cache(df_dict):\n",
    "    \n",
    "    conditions_list = df_dict['conditions'].name.unique().tolist()\n",
    "    conditions_list = [str(i) for i in conditions_list]\n",
    "    conditions_list = list(set([i.lower() for i in conditions_list]))\n",
    "    \n",
    "    interventions_list = df_dict['interventions'].name.unique().tolist()\n",
    "    interventions_list = [str(i) for i in interventions_list]\n",
    "    interventions_list = list(set([i.lower() for i in interventions_list]))\n",
    "    \n",
    "    interventions_alts_list = df_dict['interventions_alts'].name.unique().tolist()\n",
    "    interventions_alts_list = [str(i) for i in interventions_alts_list]\n",
    "    interventions_alts_list = list(set([i.lower() for i in interventions_alts_list]))\n",
    "    \n",
    "    try:\n",
    "        cache_manually_selected_terms()\n",
    "    except:\n",
    "        print(\"No manually selected terms file found\")\n",
    "    \n",
    "    try:        \n",
    "        cache_df = pd.read_csv(\"mapping_cache.tsv\", sep =\"\\t\", index_col=False, header=0, on_bad_lines = 'warn')\n",
    "        \n",
    "        conditions_cache = cache_df[cache_df[\"term_type\"] == \"condition\"]\n",
    "        conditions_cache = conditions_cache['clintrial_term'].unique().tolist()\n",
    "        conditions_cache = list(set([i.lower() for i in conditions_cache]))\n",
    "        \n",
    "        conditions_new = [x for x in conditions_list if x not in conditions_cache] # find conditions not in the cache (i.g. new conditions to map)\n",
    "        conditions_new = list(filter(None, conditions_new))\n",
    "        conditions_new = [str(i) for i in conditions_new]\n",
    "        \n",
    "        interventions_cache = cache_df[cache_df[\"term_type\"] == \"intervention\"]\n",
    "        interventions_cache = interventions_cache['clintrial_term'].unique().tolist()\n",
    "        interventions_cache = list(set([i.lower() for i in interventions_cache]))\n",
    "        \n",
    "        interventions_new = [x for x in interventions_list if x not in interventions_cache] # find interventions not in the cache (i.g. new interventions to map)\n",
    "        interventions_new = list(filter(None, interventions_new))\n",
    "        interventions_new = [str(i) for i in interventions_new]\n",
    "        \n",
    "        interventions_alts_cache = cache_df[cache_df[\"term_type\"] == \"intervention_alternate\"]\n",
    "        interventions_alts_cache = interventions_alts_cache['clintrial_term'].unique().tolist()\n",
    "        interventions_alts_cache = list(set([i.lower() for i in interventions_alts_cache]))\n",
    "        \n",
    "        interventions_alts_new = [x for x in interventions_alts_list if x not in interventions_alts_cache] # find interventions_alts not in the cache (i.g. new interventions_alts to map)\n",
    "        interventions_alts_new = list(filter(None, interventions_alts_new))\n",
    "        interventions_alts_new = [str(i) for i in interventions_alts_new]\n",
    "        \n",
    "    except:\n",
    "        print(\"No cache of terms found. Proceeding to map entire KG from scratch\")\n",
    "        conditions_new = conditions_list\n",
    "        interventions_new = interventions_list\n",
    "        interventions_alts_new = interventions_alts_list\n",
    "        \n",
    "    dict_new_terms = {\"conditions\": conditions_new, \"interventions\": interventions_new, \"interventions_alts\": interventions_alts_new}\n",
    "\n",
    "    return dict_new_terms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c2745f5-583f-41be-9073-1b29760604b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cache_manually_selected_terms():    \n",
    "\n",
    "    def return_curie_dict(curie_info_delimited):\n",
    "        keys = [\"mapped_name\", \"mapped_curie\", \"mapped_score\", \"mapped_semtypes\"]\n",
    "        curie_list = curie_info_delimited.split(\" | \")\n",
    "        curie_dict = dict(zip(keys, curie_list))\n",
    "        return curie_dict\n",
    "\n",
    "    files = glob.glob(\"*.xlsx\")\n",
    "    manually_selected_file = [i for i in files if \"manual_review\" in i if not i.startswith(\"~\")][0] # find the file of manual selections\n",
    "    manually_selected = pd.read_excel(manually_selected_file)\n",
    "    cols_to_fill = [\"mapping_tool\", \"term_type\", \"clintrial_term\", \"input_term\"]\n",
    "    manually_selected.loc[:,cols_to_fill] = manually_selected.loc[:,cols_to_fill].ffill()\n",
    "\n",
    "    manually_selected = manually_selected[~manually_selected['manually_selected_CURIE'].isnull()] # get rows where terms were manually chosen\n",
    "    manually_selected.drop([\"mapping_tool_response\"], axis = 1, inplace = True)\n",
    "\n",
    "    manually_selected[\"manually_selected_CURIE\"] = manually_selected[\"manually_selected_CURIE\"].apply(lambda x: return_curie_dict(x)) # convert | delimited strings to CURIE dict\n",
    "    manually_selected[\"score\"] = 1000  # human curated score = 1000\n",
    "    manually_selected.rename(columns = {'manually_selected_CURIE':'mapping_tool_response'}, inplace = True)\n",
    "    manually_selected = manually_selected[[\"mapping_tool\", \"term_type\", \"clintrial_term\", \"input_term\", \"mapping_tool_response\", \"score\"]] # reorder columns to be same as the cache files we're appending to \n",
    "    manually_selected.to_csv(\"mapping_cache.tsv\", mode='a', header=False, sep =\"\\t\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9a77e9-7ead-4ee6-a4df-d2aa636cc893",
   "metadata": {},
   "source": [
    "# Map new terms using Mapper function (MetaMap + Name Resolver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee1c310c-1087-4d72-bef2-53f535f92e6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_nr_response(orig_term):\n",
    "    def create_session():\n",
    "        s = requests.Session()\n",
    "        return s\n",
    " \n",
    "    sess = create_session()\n",
    " \n",
    "    \"\"\"   Runs Name Resolver   \"\"\"\n",
    "    nr_url = 'https://name-resolution-sri.renci.org/lookup'\n",
    "    max_retries = 3 \n",
    "    \n",
    "    input_term = orig_term # in MetaMap, we have to potentially deascii the term and lower case it...for Name Resolver, we don't need to do that. To keep columns consist with MetaMap output, we just keep it and say the original term and the input term are the same. For MetaMap, they might be different\n",
    "    retries = 0\n",
    "    params = {'string':orig_term, 'limit':1} # limit -1 makes this return all available equivalent CURIEs name resolver can give (deprecated)\n",
    "    while retries <= max_retries:\n",
    "        try:\n",
    "            r = sess.post(nr_url, params=params)\n",
    "            check_limit() # counts how many requests have been sent to NR. If limit of 40 have been sent, sleeps for 1 min\n",
    "            if r.status_code == 200:\n",
    "                mapping_tool_response = r.json()  # process Name Resolver response\n",
    "                return mapping_tool_response\n",
    "            else:\n",
    "                return None\n",
    "        except (requests.RequestException, ConnectionResetError, OSError) as ex:\n",
    "            print(f\"\\nName Resolver request failed for term: {term}. Error: {ex}\")\n",
    "            retries += 1\n",
    "            if retries < max_retries:\n",
    "                print(f\"Retrying ({retries}/{max_retries}) after a delay.\")\n",
    "                time.sleep(2 ** retries)  # Increase the delay between retries exponentially\n",
    "            else:\n",
    "                print(f\"Max retries (Name Resolver) reached for term: {term}.\")\n",
    "                return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b60d026-6fdb-44bc-8cf2-6ae28d2edcf6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# I'm only getting 1 concept from Name Resolver. \n",
    "# Both MetaMap and Name Resolver return several, \n",
    "# but I only take 1 from Name Resolver bc they have a preferred concept.\n",
    "# MetaMap's 2nd or 3rd result is often the best one, so I collect all of them and try to score\"\n",
    "\n",
    "def process_metamap_concept(concept):\n",
    "    concept = concept._asdict()\n",
    "    concept_dict  = {\"mapped_name\": concept.get(\"preferred_name\"),\n",
    "                     \"mapped_curie\": concept.get(\"cui\"),\n",
    "                     \"mapped_score\": concept.get(\"score\"),\n",
    "                     \"mapped_semtypes\": concept.get(\"semtypes\")}\n",
    "    if not concept.get(\"preferred_name\"): # if condition triggered if the concept dict looks like following, where AA is for Abbreviation.... {'index': 'USER', 'aa': 'AA', 'short_form': 'copd', 'long_form': 'chronic obstructive pulmonary disease', 'num_tokens_short_form': '1', 'num_chars_short_form': '4', 'num_tokens_long_form': '7', 'num_chars_long_form': '37', 'pos_info': '43:4'}\n",
    "        concept_dict = None\n",
    "    return concept_dict\n",
    "\n",
    "def process_nameresolver_response(nr_response):              \n",
    "    nr_curie = nr_response[0][\"curie\"]\n",
    "    nr_name = nr_response[0][\"label\"]\n",
    "    nr_type = nr_response[0][\"types\"][0]\n",
    "    nr_score = nr_response[0][\"score\"]\n",
    "    concept_dict = {\"mapped_name\": nr_name,\n",
    "                    \"mapped_curie\": nr_curie,\n",
    "                    \"mapped_score\": nr_score,\n",
    "                    \"mapped_semtypes\": nr_type}\n",
    "    return concept_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1292c371-6c17-4102-8f17-0f45846c23b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_mappers(term_pair, params, term_type, csv_writer):\n",
    "    \n",
    "    orig_term = term_pair[0]\n",
    "    input_term = term_pair[1]\n",
    "    from_mapper = []\n",
    "    mm = MetaMap.get_instance(metamap_dirs[\"metamap_base_dir\"] + metamap_dirs[\"metamap_bin_dir\"])\n",
    "\n",
    "    # Format of output TSV: header = ['mapping_tool', 'term_type', 'clintrial_term', 'input_term', 'mapping_tool_response', 'score']\n",
    "\n",
    "    if params.get(\"exclude_sts\") is None: # exclude_sts is used for Interventions. restrict_to_sts is used for Conditions. So, the logic is, if we're mapping Conditions, execute \"if\" part of code. If we're mapping Interventions, execute \"else\" part of code\n",
    "        try:\n",
    "            concepts,error = mm.extract_concepts([input_term],\n",
    "                                                 restrict_to_sts = params[\"restrict_to_sts\"],\n",
    "                                                 term_processing = params[\"term_processing\"],\n",
    "                                                 ignore_word_order = params[\"ignore_word_order\"],\n",
    "                                                 strict_model = params[\"strict_model\"],)\n",
    "                                                    \n",
    "            if concepts:   # if MetaMap gives response, process response\n",
    "                mapping_tool = \"metamap\"\n",
    "                for concept in concepts:\n",
    "                    concept_info = []\n",
    "                    new_concept_dict = process_metamap_concept(concept)\n",
    "                    concept_info.extend([mapping_tool, term_type, orig_term, input_term, new_concept_dict]) # score column is empty, Format of output TSV: header = ['mapping_tool', 'term_type', 'clintrial_term', 'input_term', 'mapping_tool_response', 'score']\n",
    "                    from_mapper.append(concept_info)\n",
    "            else:   # if MetaMap fails, try using Name Resolver and process response\n",
    "                nr_response = get_nr_response(orig_term)\n",
    "                if nr_response: # if Name Resolver gives response, process repsonse\n",
    "                    input_term = orig_term # no preprocessing (lowercasing or deascii-ing) necessary to submit terms to Name Resolver (unlike MetaMap)\n",
    "                    mapping_tool = \"nameresolver\"\n",
    "                    concept_info = []\n",
    "                    new_concept_dict = process_nameresolver_response(nr_response)\n",
    "                    concept_info.extend([mapping_tool, term_type, orig_term, input_term, new_concept_dict]) # Add None for score column, empty bc not scored yet\n",
    "                    from_mapper.append(concept_info)\n",
    "                else:\n",
    "                    concept_info = []\n",
    "                    # print(\"Nothing returned from NR or Metamap\")\n",
    "                    concept_info.extend([\"mapping_tools_failed\", term_type, orig_term, input_term, \"mapping_tools_failed\"])\n",
    "                    from_mapper.append(concept_info)\n",
    "        except:\n",
    "            concept_info = []\n",
    "            # print(\"Nothing returned from NR or Metamap\")\n",
    "            concept_info.extend([\"mapping_tools_failed\", term_type, orig_term, input_term, \"mapping_tools_failed\"])\n",
    "            from_mapper.append(concept_info)\n",
    "            \n",
    "    else:   # Else block triggered if mapping Interventions\n",
    "        try:\n",
    "            concepts,error = mm.extract_concepts([input_term],\n",
    "                                                 exclude_sts = params[\"exclude_sts\"],\n",
    "                                                 term_processing = params[\"term_processing\"],\n",
    "                                                 ignore_word_order = params[\"ignore_word_order\"],\n",
    "                                                 strict_model = params[\"strict_model\"],) \n",
    "                                                   \n",
    "            if concepts:   # if MetaMap gives response, process response\n",
    "                mapping_tool = \"metamap\"\n",
    "                for concept in concepts:\n",
    "                    concept_info = []\n",
    "                    new_concept_dict = process_metamap_concept(concept)\n",
    "                    concept_info.extend([mapping_tool, term_type, orig_term, input_term, new_concept_dict]) # score column is empty, Format of output TSV: header = ['mapping_tool', 'term_type', 'clintrial_term', 'input_term', 'mapping_tool_response', 'score']\n",
    "                    from_mapper.append(concept_info)\n",
    "            else:   # if MetaMap fails, try using Name Resolver and process response\n",
    "                nr_response = get_nr_response(orig_term) \n",
    "                if nr_response: # if Name Resolver gives response, process repsonse\n",
    "                    input_term = orig_term # no preprocessing (lowercasing or deascii-ing) necessary to submit terms to Name Resolver (unlike MetaMap)\n",
    "                    mapping_tool = \"nameresolver\"\n",
    "                    concept_info = []\n",
    "                    new_concept_dict = process_nameresolver_response(nr_response)\n",
    "                    concept_info.extend([mapping_tool, term_type, orig_term, input_term, new_concept_dict])\n",
    "                    from_mapper.append(concept_info)\n",
    "                else:\n",
    "                    concept_info = []\n",
    "                    # print(\"Nothing returned from NR or Metamap\")\n",
    "                    concept_info.extend([\"mapping_tools_failed\", term_type, orig_term, input_term, \"mapping_tools_failed\"])\n",
    "                    from_mapper.append(concept_info)\n",
    "        except:\n",
    "            concept_info = []\n",
    "            # print(\"Nothing returned from NR or Metamap\")\n",
    "            concept_info.extend([\"mapping_tools_failed\", term_type, orig_term, input_term, \"mapping_tools_failed\"])\n",
    "            from_mapper.append(concept_info)\n",
    "      \n",
    "    for result in from_mapper:\n",
    "        # print(result)\n",
    "        if result[0] == \"mapping_tools_failed\":\n",
    "            result.append(-1)\n",
    "        else:\n",
    "            result.append(\"unscored\")\n",
    "        # print(result)\n",
    "        csv_writer.writerow(result)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485fd6bb-2e4a-40f5-bbc9-e92af2c16a87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def parallelize_mappers(term_pair_list, params, term_type, csv_writer):\n",
    "    \n",
    "#     start_metamap_servers(metamap_dirs) # start the MetaMap servers\n",
    "#     terms_left = len(term_pair_list)\n",
    "#     future_to_pair = {}\n",
    "#     with concurrent.futures.ThreadPoolExecutor(max_workers=6) as executor:\n",
    "#         future_to_pair = {executor.submit(run_mappers, term_pair, params, term_type, csv_writer): term_pair for term_pair in term_pair_list}\n",
    "#         for future in concurrent.futures.as_completed(future_to_pair):\n",
    "#             term_pair = future_to_pair[future]\n",
    "#             try:\n",
    "#                 result = future.result()\n",
    "#                 # Process result if needed\n",
    "#             except Exception as exc:\n",
    "#                 print(f\"Job {term_pair} generated an exception: {exc}\")\n",
    "#             finablly:\n",
    "#                 terms_left -= 1\n",
    "#                 # pbar.update(n=1)\n",
    "#                 if terms_left % 10 == 0:\n",
    "#                     gc.collect()\n",
    "#                     time.sleep(3)\n",
    "#     stop_metamap_servers(metamap_dirs) # stop the MetaMap servers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2775f948-3fec-4cdc-99c8-fa744c0e0876",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import multiprocessing\n",
    "# from mapper_wrapper import run_mappers_wrapper\n",
    "\n",
    "\n",
    "# # def run_mappers_wrapper(args):\n",
    "# #     term_pair, params, mm, term_type, output = args\n",
    "# #     with open(output, 'w', newline='') as csvfile:\n",
    "# #         csv_writer = csv.writer(output, delimiter='\\t')\n",
    "# #         try:\n",
    "# #             return run_mappers(term_pair, params, mm, term_type, csv_writer)\n",
    "# #         except Exception as exc:\n",
    "# #             return f\"Error processing {term_pair}: {exc}\"\n",
    "    \n",
    "\n",
    "# def parallelize_mappers(term_pair_list, params, term_type):\n",
    "    \n",
    "#     # open mapping cache to add mapped terms\n",
    "#     mapping_filename = \"mapping_cache.tsv\"\n",
    "#     if os.path.exists(mapping_filename):\n",
    "#         output = open(mapping_filename, 'a', newline='', encoding=\"utf-8\") \n",
    "#         csv_writer = csv.writer(output, delimiter='\\t')\n",
    "#         output.close()\n",
    "#     else:\n",
    "#         output = open(mapping_filename, 'w+', newline='', encoding='utf-8')\n",
    "#         col_names = ['mapping_tool', 'term_type', 'clintrial_term', 'input_term', 'mapping_tool_response', 'score']\n",
    "#         csv_writer = csv.writer(output, delimiter='\\t')\n",
    "#         csv_writer.writerow(col_names)\n",
    "#         output.close()\n",
    "    \n",
    "#     terms_left = len(term_pair_list)\n",
    "#     # Prepare arguments for run_mappers\n",
    "#     args_list = [(term_pair, params, term_type, output) for term_pair in term_pair_list]\n",
    "\n",
    "#     with multiprocessing.Pool(processes=6) as pool:\n",
    "#         results = pool.map(run_mappers_wrapper, args_list)\n",
    "\n",
    "#     # Process results\n",
    "#     for result in results:\n",
    "#         if isinstance(result, str):\n",
    "#             # Handle error\n",
    "#             print(result)\n",
    "#         else:\n",
    "#             # Process result if needed\n",
    "#             pass\n",
    "#         terms_left -= 1\n",
    "#         if terms_left % 20 == 0:\n",
    "#             gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "091f5141-f625-4259-8fb0-1a6a1232467f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import csv\n",
    "from mapper_wrapper import run_mappers_wrapper\n",
    "\n",
    "\n",
    "# def run_mappers_wrapper(args):\n",
    "#     term_pair, params, mm, term_type, output = args\n",
    "#     with open(output, 'w', newline='') as csvfile:\n",
    "#         csv_writer = csv.writer(csvfile, delimiter='\\t')  # Use csvfile instead of output\n",
    "#         try:\n",
    "#             return run_mappers(term_pair, params, mm, term_type, csv_writer)\n",
    "#         except Exception as exc:\n",
    "#             return f\"Error processing {term_pair}: {exc}\"\n",
    "\n",
    "def parallelize_mappers(term_pair_list, params, term_type):\n",
    "    \n",
    "    start_metamap_servers(metamap_dirs) # start the MetaMap servers\n",
    "    terms_left = len(term_pair_list)\n",
    "    # Prepare arguments for run_mappers\n",
    "    args_list = [(term_pair, params, term_type, \"mapping_cache.tsv\") for term_pair in term_pair_list]\n",
    "\n",
    "    with multiprocessing.Pool(processes=6) as pool:\n",
    "        results = pool.map(run_mappers_wrapper, args_list)\n",
    "\n",
    "    # Process results\n",
    "    for result in results:\n",
    "        if isinstance(result, str):\n",
    "            # Handle error\n",
    "            print(result)\n",
    "        else:\n",
    "            # Process result if needed\n",
    "            pass\n",
    "        terms_left -= 1\n",
    "        if terms_left % 20 == 0:\n",
    "            gc.collect()\n",
    "            \n",
    "    stop_metamap_servers(metamap_dirs) # stop the MetaMap servers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f79ca09-cac5-4f30-a262-ff86690265e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def term_list_to_mappers(dict_new_terms):   \n",
    "    metamap_version = [int(s) for s in re.findall(r'\\d+', metamap_dirs.get('metamap_bin_dir'))] # get MetaMap version being run \n",
    "    deasciier = np.vectorize(de_ascii_er) # vectorize function\n",
    "\n",
    "    #  - Conditions\n",
    "    condition_semantic_type_restriction = ['acab,anab,cgab,comd,dsyn,inpo,mobd,neop,patf,clna,fndg']  # see https://lhncbc.nlm.nih.gov/ii/tools/MetaMap/Docs/SemanticTypes_2018AB.txt for semantic types (\"acab,anab,etc.\")\n",
    "    conditions = dict_new_terms.get(\"conditions\")\n",
    "    condition_params = {\"restrict_to_sts\":condition_semantic_type_restriction, \"term_processing\":True, \"ignore_word_order\":True, \"strict_model\":False} # strict_model and relaxed_model are presumably opposites? relaxed_model = True is what I want, but that option appears to be broken in Pymetamap (returns no results when used). Using strict_model = False instead...\n",
    "    # conditon_term_type = \"condition\"\n",
    "\n",
    "    #  - Interventions\n",
    "    condition_semantic_type_restriction = ['acab,anab,cgab,comd,dsyn,inpo,mobd,neop,patf,clna,fndg']  # see https://lhncbc.nlm.nih.gov/ii/tools/MetaMap/Docs/SemanticTypes_2018AB.txt for semantic types (\"acab,anab,etc.\")\n",
    "    interventions = dict_new_terms.get(\"interventions\")\n",
    "    intervention_params = {\"exclude_sts\":condition_semantic_type_restriction, \"term_processing\":True, \"ignore_word_order\":True, \"strict_model\":False} # strict_model and relaxed_model are presumably opposites? relaxed_model = True is what I want, but that option appears to be broken in Pymetamap (returns no results when used). Using strict_model = False instead...\n",
    "    # intervention_term_type = \"intervention\"\n",
    "\n",
    "    #  - Alternate Interventions\n",
    "    condition_semantic_type_restriction = ['acab,anab,cgab,comd,dsyn,inpo,mobd,neop,patf,clna,fndg']  # see https://lhncbc.nlm.nih.gov/ii/tools/MetaMap/Docs/SemanticTypes_2018AB.txt for semantic types (\"acab,anab,etc.\")\n",
    "    interventions_alts = dict_new_terms.get(\"interventions_alts\")\n",
    "    intervention_alts_params = intervention_params # same params as interventions\n",
    "    # intervention_alternate_term_type = \"intervention_alternate\"\n",
    "    \n",
    "    chunksize = 20\n",
    "    \n",
    "    if metamap_version[0] >= 20:\n",
    "        \n",
    "        cons_processed = list(zip(conditions, conditions))  # these are lists of the same term repeated twice, bc MetaMap 2020 does not require deasciing, so the 2nd term remains unchanged and is a repeat of the first term\n",
    "        ints_processed = list(zip(interventions, interventions))\n",
    "        ints_alts_processed = list(zip(interventions_alts, interventions_alts))\n",
    "    \n",
    "        conditions_chunked = [cons_processed[i:i + chunksize] for i in range(0, len(cons_processed), chunksize)]  \n",
    "        interventions_chunked = [ints_processed[i:i + chunksize] for i in range(0, len(ints_processed), chunksize)]  \n",
    "        interventions_alts_chunked = [ints_alts_processed[i:i + chunksize] for i in range(0, len(ints_alts_processed), chunksize)] \n",
    "        \n",
    "        print(\"MetaMap version >= 2020, conduct mapping on original terms\")\n",
    "        \n",
    "        LENGTH = len(cons_processed)  # Number of iterations required to fill progress bar (pbar)\n",
    "        pbar = tqdm(total=LENGTH, desc=\"% conditions mapped\", position=0, leave=True, mininterval = LENGTH/20, bar_format='{l_bar}{bar:20}{r_bar}{bar:-10b}')  # Init progress bar\n",
    "        for chunk in conditions_chunked:\n",
    "            # parallelize_mappers(chunk, condition_params, \"condition\", csv_writer)\n",
    "            parallelize_mappers(chunk, condition_params, \"condition\")\n",
    "            pbar.update(n=len(chunk))\n",
    "        \n",
    "        LENGTH = len(ints_processed)  # Number of iterations required to fill progress bar (pbar)\n",
    "        pbar = tqdm(total=LENGTH, desc=\"% interventions mapped\", position=0, leave=True, mininterval = LENGTH/20, bar_format='{l_bar}{bar:20}{r_bar}{bar:-10b}')  # Init progress bar\n",
    "        for chunk in interventions_chunked:\n",
    "            # parallelize_mappers(chunk, intervention_params, \"intervention\", csv_writer)\n",
    "            parallelize_mappers(chunk, intervention_params, \"intervention\")\n",
    "            pbar.update(n=len(chunk))\n",
    "        \n",
    "        LENGTH = len(ints_alts_processed)  # Number of iterations required to fill progress bar (pbar)\n",
    "        pbar = tqdm(total=LENGTH, desc=\"% alternate interventions mapped\", position=0, leave=True, mininterval = LENGTH/20, bar_format='{l_bar}{bar:20}{r_bar}{bar:-10b}')  # Init progress bar\n",
    "        for chunk in interventions_alts_chunked:\n",
    "            # parallelize_mappers(chunk, intervention_alts_params, \"alternate_intervention\", csv_writer)\n",
    "            parallelize_mappers(chunk, intervention_alts_params, \"alternate_intervention\")\n",
    "            pbar.update(n=len(chunk))\n",
    "        \n",
    "    else:\n",
    "        print(\"MetaMap version < 2020, conduct mapping on terms after removing ascii characters\")\n",
    "        \n",
    "        deascii_cons = deasciier(conditions)\n",
    "        deascii_ints = deasciier(interventions)\n",
    "        deascii_int_alts = deasciier(interventions_alts)\n",
    "                \n",
    "        cons_processed = list(zip(conditions, deascii_cons)) # these are lists of the original term, and the deasciied term, bc MetaMap 2018 does not process ascii characters\n",
    "        ints_processed = list(zip(interventions, deascii_ints))\n",
    "        ints_alts_processed = list(zip(interventions_alts, deascii_int_alts))\n",
    "        \n",
    "        conditions_chunked = [cons_processed[i:i + chunksize] for i in range(0, len(cons_processed), chunksize)]  \n",
    "        interventions_chunked = [ints_processed[i:i + chunksize] for i in range(0, len(ints_processed), chunksize)]  \n",
    "        interventions_alts_chunked = [ints_alts_processed[i:i + chunksize] for i in range(0, len(ints_alts_processed), chunksize)] \n",
    "        \n",
    "        LENGTH = len(cons_processed)  # Number of iterations required to fill progress bar (pbar)\n",
    "        pbar = tqdm(total=LENGTH, desc=\"% conditions mapped\", position=0, leave=True, mininterval = LENGTH/20, bar_format='{l_bar}{bar:20}{r_bar}{bar:-10b}')  # Init progress bar\n",
    "        for chunk in conditions_chunked:\n",
    "            # parallelize_mappers(chunk, condition_params, \"condition\", csv_writer)\n",
    "            parallelize_mappers(chunk, condition_params, \"condition\")\n",
    "            pbar.update(n=len(chunk))\n",
    "        \n",
    "        LENGTH = len(ints_processed)  # Number of iterations required to fill progress bar (pbar)\n",
    "        pbar = tqdm(total=LENGTH, desc=\"% interventions mapped\", position=0, leave=True, mininterval = LENGTH/20, bar_format='{l_bar}{bar:20}{r_bar}{bar:-10b}')  # Init progress bar\n",
    "        for chunk in interventions_chunked:\n",
    "            # parallelize_mappers(chunk, intervention_params, \"intervention\", csv_writer)\n",
    "            parallelize_mappers(chunk, intervention_params, \"intervention\")\n",
    "            pbar.update(n=len(chunk))\n",
    "        \n",
    "        LENGTH = len(ints_alts_processed)  # Number of iterations required to fill progress bar (pbar)\n",
    "        pbar = tqdm(total=LENGTH, desc=\"% alternate interventions mapped\", position=0, leave=True, mininterval = LENGTH/20, bar_format='{l_bar}{bar:20}{r_bar}{bar:-10b}')  # Init progress bar\n",
    "        for chunk in interventions_alts_chunked:\n",
    "            # parallelize_mappers(chunk, intervention_alts_params, \"alternate_intervention\", csv_writer)\n",
    "            parallelize_mappers(chunk, intervention_alts_params, \"alternate_intervention\")\n",
    "            pbar.update(n=len(chunk))\n",
    "\n",
    "    \n",
    "    # \"\"\" Remove duplicate rows \"\"\"\n",
    "    mapping_filename = \"mapping_cache.tsv\"\n",
    "    cache = pd.read_csv(mapping_filename, sep='\\t', index_col=False, header=0, encoding_errors='ignore')\n",
    "    cache = cache.drop_duplicates()\n",
    "    cache.to_csv(mapping_filename, sep=\"\\t\", index=False, header=True) # output deduplicated cache terms to TSV\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cee2c13b-775b-44a6-92be-7b75f70254af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_mappings():   \n",
    "    print(\"Scoring cache\")\n",
    "\n",
    "    def get_max_score(str1, str2, old_score):\n",
    "        \n",
    "        try:\n",
    "            if old_score == \"unscored\":\n",
    "                sortratio_score = get_token_sort_ratio(str1, str2)\n",
    "                similarity_score = get_similarity_score(str1, str2)\n",
    "                max_score = max(sortratio_score, similarity_score)\n",
    "                score = max_score\n",
    "            else:\n",
    "                score = old_score   \n",
    "        except:\n",
    "            score = old_score\n",
    "        return score\n",
    "\n",
    "    def wrap(x): # use this to convert string objects to dicts \n",
    "        try:\n",
    "            a = ast.literal_eval(x)\n",
    "            return(a)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    with pd.read_csv(\"mapping_cache.tsv\", sep='\\t', index_col=False, header=0, on_bad_lines = 'warn', usecols=lambda c: not c.startswith('Unnamed:'), chunksize=1000) as reader:\n",
    "        write_header = True\n",
    "        for chunk in reader:\n",
    "            chunk[\"mapping_tool_response\"] = chunk[\"mapping_tool_response\"].apply(lambda x: wrap(x))\n",
    "            mapping_info = chunk[\"mapping_tool_response\"].apply(pd.Series, dtype='object')\n",
    "            chunk[\"mapped_name\"] = mapping_info[\"mapped_name\"]\n",
    "            chunk[\"score\"] = chunk.apply(lambda x: get_max_score(x['input_term'], x['mapped_name'], x['score']), axis=1) # get score for score rows that are empty/not scored yet\n",
    "            chunk.drop([\"mapped_name\"], axis = 1, inplace = True)\n",
    "            chunk.to_csv(f'mapping_cache_scored_temp.tsv', sep=\"\\t\", index=False, header=write_header, mode = 'a', encoding=\"utf-8\") # output to TSV\n",
    "            write_header = False\n",
    "\n",
    "    os.rename('mapping_cache.tsv','mapping_cache_backup.tsv')       \n",
    "    os.rename('mapping_cache_scored_temp.tsv','mapping_cache.tsv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10f1c3e5-5f4c-4a01-9cf7-0f403503f108",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_terms_files():\n",
    "    print(\"Generating output files\")\n",
    "\n",
    "    \"\"\"   Get high scorers   \"\"\"\n",
    "    cache = pd.read_csv(\"mapping_cache.tsv\", sep='\\t', index_col=False, header=0)\n",
    "    cache['score'] = pd.to_numeric(cache['score'], errors='coerce')\n",
    "    highscorers = cache[cache['score'] >= 80] \n",
    "    idx = highscorers.groupby('clintrial_term')['score'].idxmax()  # group by the clinical trial term and get the highest scoring\n",
    "    auto_selected = highscorers.loc[idx]\n",
    "    auto_selected.to_csv(f'autoselected_terms.tsv', sep=\"\\t\", index=False, header=True) # output to TSV\n",
    "\n",
    "    \"\"\"   Get low scorers, aggregate for manual selections  \"\"\"\n",
    "    low_scorers = cache[cache['score'] < 80]\n",
    "    manual_review = low_scorers[~low_scorers.clintrial_term.isin(highscorers['clintrial_term'].unique().tolist())] # there are terms autoselected that have mappings that didn't pass threshold too, but we want to consider that term mapped. So get rid of these rows too\n",
    "    mapping_tool_response = manual_review['mapping_tool_response'].apply(lambda x: wrap(x))\n",
    "    manual_review = manual_review.copy()\n",
    "    mapping_tool_response = mapping_tool_response.apply(pd.Series)\n",
    "    manual_review.loc[:, 'mapping_tool_response_lists'] = mapping_tool_response.values.tolist()\n",
    "    manual_review.drop('mapping_tool_response', axis=1, inplace=True)\n",
    "    manual_review = manual_review[[\"mapping_tool\", \"term_type\", \"clintrial_term\", \"mapping_tool_response_lists\", \"input_term\", \"score\"]]\n",
    "    manual_review['mapping_tool_response'] = [' | '.join(map(str, l)) for l in manual_review['mapping_tool_response_lists']]\n",
    "    manual_review.drop('mapping_tool_response_lists', axis=1, inplace=True)\n",
    "    manual_review = manual_review.sort_values(by=[\"mapping_tool\", \"term_type\", \"clintrial_term\", \"input_term\"], ascending=False)\n",
    "    manual_review.set_index([\"mapping_tool\", \"term_type\", \"clintrial_term\", \"input_term\"], inplace=True)   # create index\n",
    "    manual_review['manually_selected_CURIE'] = None # make a column \n",
    "    manual_review.to_excel('manual_review.xlsx', engine='xlsxwriter', index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a8fe1d-e5b2-4752-81df-8dcac1502e44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315e7514-ec69-462f-9fb7-77cf0afc47be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e73d53-d952-4f1c-924a-7df5bb494afc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046274d4-f7fe-4daf-8dd6-84e1ea90fd4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd659f7-df88-47fd-9965-567b640b61ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96458f45-d889-4c08-b9d0-b2213d2c343a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d719ee3-148e-4948-8dfe-72a6bd0d730c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087ed414-58ba-4746-ad22-bd8872c8886f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfddebe-14c3-4289-9df7-f68d2657b2af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d00fe8-c542-4d2a-b379-27e3719b00fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "98ceddf5-0ebe-41d9-9b89-85fc647f4136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No cache of terms found. Proceeding to map entire KG from scratch\n",
      "MetaMap version < 2020, conduct mapping on terms after removing ascii characters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "% conditions mapped:   0%|                    | 0/40 [00:00<?, ?it/s] Process SpawnPoolWorker-27:\n",
      "Process SpawnPoolWorker-25:\n",
      "Process SpawnPoolWorker-30:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/Kamileh/anaconda3/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "Process SpawnPoolWorker-28:\n",
      "Process SpawnPoolWorker-26:\n",
      "    self.run()\n",
      "  File \"/Users/Kamileh/anaconda3/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/Kamileh/anaconda3/lib/python3.11/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/Users/Kamileh/anaconda3/lib/python3.11/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/Kamileh/anaconda3/lib/python3.11/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Users/Kamileh/anaconda3/lib/python3.11/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/Kamileh/anaconda3/lib/python3.11/multiprocessing/spawn.py\", line 135, in _main\n",
      "  File \"/Users/Kamileh/anaconda3/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/Kamileh/anaconda3/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/Kamileh/anaconda3/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "    return self._bootstrap(parent_sentinel)\n",
      "  File \"/Users/Kamileh/anaconda3/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/Kamileh/anaconda3/lib/python3.11/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/Users/Kamileh/anaconda3/lib/python3.11/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/Kamileh/anaconda3/lib/python3.11/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/Kamileh/anaconda3/lib/python3.11/multiprocessing/process.py\", line 330, in _bootstrap\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "    traceback.print_exc()\n",
      "  File \"/Users/Kamileh/anaconda3/lib/python3.11/multiprocessing/process.py\", line 317, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/Users/Kamileh/anaconda3/lib/python3.11/multiprocessing/util.py\", line 320, in _exit_function\n",
      "    def _exit_function(info=info, debug=debug, _run_finalizers=_run_finalizers,\n",
      "    \n",
      "KeyboardInterrupt\n",
      "  File \"/Users/Kamileh/anaconda3/lib/python3.11/traceback.py\", line 183, in print_exc\n",
      "  File \"/Users/Kamileh/anaconda3/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/Kamileh/anaconda3/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/Kamileh/anaconda3/lib/python3.11/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/Users/Kamileh/anaconda3/lib/python3.11/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/Kamileh/anaconda3/lib/python3.11/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "    print_exception(*sys.exc_info(), limit=limit, file=file, chain=chain)\n",
      "  File \"/Users/Kamileh/anaconda3/lib/python3.11/traceback.py\", line 124, in print_exception\n",
      "    te = TracebackException(type(value), value, tb, limit=limit, compact=True)\n",
      "  File \"/Users/Kamileh/anaconda3/lib/python3.11/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/Users/Kamileh/anaconda3/lib/python3.11/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/Kamileh/anaconda3/lib/python3.11/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/Kamileh/anaconda3/lib/python3.11/traceback.py\", line 677, in __init__\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/Kamileh/anaconda3/lib/python3.11/multiprocessing/process.py\", line 317, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/Users/Kamileh/anaconda3/lib/python3.11/multiprocessing/util.py\", line 320, in _exit_function\n",
      "    def _exit_function(info=info, debug=debug, _run_finalizers=_run_finalizers,\n",
      "    \n",
      "KeyboardInterrupt\n",
      "  File \"/Users/Kamileh/anaconda3/lib/python3.11/multiprocessing/queues.py\", line 365, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/Kamileh/anaconda3/lib/python3.11/multiprocessing/connection.py\", line 215, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/Kamileh/anaconda3/lib/python3.11/multiprocessing/connection.py\", line 413, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "          ^^^^^^^^^^^^^\n",
      "  File \"/Users/Kamileh/anaconda3/lib/python3.11/multiprocessing/connection.py\", line 378, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "    def __init__(self, exc_type, exc_value, exc_traceback, *, limit=None,\n",
      "\n",
      "KeyboardInterrupt\n",
      "  File \"/Users/Kamileh/anaconda3/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/Kamileh/anaconda3/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/Kamileh/anaconda3/lib/python3.11/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/Users/Kamileh/anaconda3/lib/python3.11/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/Kamileh/anaconda3/lib/python3.11/multiprocessing/synchronize.py\", line 97, in __exit__\n",
      "    def __exit__(self, *args):\n",
      "    \n",
      "KeyboardInterrupt\n",
      "  File \"/Users/Kamileh/anaconda3/lib/python3.11/multiprocessing/process.py\", line 317, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/Users/Kamileh/anaconda3/lib/python3.11/multiprocessing/util.py\", line 350, in _exit_function\n",
      "    for p in active_children():\n",
      "             ^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/Kamileh/anaconda3/lib/python3.11/multiprocessing/process.py\", line 47, in active_children\n",
      "    _cleanup()\n",
      "  File \"/Users/Kamileh/anaconda3/lib/python3.11/multiprocessing/process.py\", line 63, in _cleanup\n",
      "    for p in list(_children):\n",
      "             ^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "Process SpawnPoolWorker-29:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/Kamileh/anaconda3/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/Kamileh/anaconda3/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/Kamileh/anaconda3/lib/python3.11/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/Users/Kamileh/anaconda3/lib/python3.11/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/Kamileh/anaconda3/lib/python3.11/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "% interventions mapped:   0%|                    | 0/39 [08:58<?, ?it/\n",
      "% conditions mapped:   0%|                    | 0/39 [04:33<?, ?it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 23\u001b[0m, in \u001b[0;36mparallelize_mappers\u001b[0;34m(term_pair_list, params, term_type)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m multiprocessing\u001b[38;5;241m.\u001b[39mPool(processes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m pool:\n\u001b[0;32m---> 23\u001b[0m     results \u001b[38;5;241m=\u001b[39m pool\u001b[38;5;241m.\u001b[39mmap(run_mappers_wrapper, args_list)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Process results\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/multiprocessing/pool.py:367\u001b[0m, in \u001b[0;36mPool.map\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;124;03mApply `func` to each element in `iterable`, collecting the results\u001b[39;00m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;124;03min a list that is returned.\u001b[39;00m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m--> 367\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_map_async(func, iterable, mapstar, chunksize)\u001b[38;5;241m.\u001b[39mget()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/multiprocessing/pool.py:768\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    769\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mready():\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/multiprocessing/pool.py:765\u001b[0m, in \u001b[0;36mApplyResult.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwait\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 765\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event\u001b[38;5;241m.\u001b[39mwait(timeout)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/threading.py:622\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    621\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 622\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cond\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    623\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m     waiter\u001b[38;5;241m.\u001b[39macquire()\n\u001b[1;32m    321\u001b[0m     gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m df_dict \u001b[38;5;241m=\u001b[39m read_raw_ct_data(flag_and_path, subset_size) \u001b[38;5;66;03m# read the clinical trial data\u001b[39;00m\n\u001b[1;32m      9\u001b[0m dict_new_terms \u001b[38;5;241m=\u001b[39m check_against_cache(df_dict) \u001b[38;5;66;03m# use the existing cache of MetaMapped terms so that only new terms are mapped\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m term_list_to_mappers(dict_new_terms)\n\u001b[1;32m     11\u001b[0m score_mappings()\n\u001b[1;32m     12\u001b[0m output_terms_files()\n",
      "Cell \u001b[0;32mIn[14], line 77\u001b[0m, in \u001b[0;36mterm_list_to_mappers\u001b[0;34m(dict_new_terms)\u001b[0m\n\u001b[1;32m     74\u001b[0m pbar \u001b[38;5;241m=\u001b[39m tqdm(total\u001b[38;5;241m=\u001b[39mLENGTH, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m% c\u001b[39;00m\u001b[38;5;124monditions mapped\u001b[39m\u001b[38;5;124m\"\u001b[39m, position\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, mininterval \u001b[38;5;241m=\u001b[39m LENGTH\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m20\u001b[39m, bar_format\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{l_bar}\u001b[39;00m\u001b[38;5;132;01m{bar:20}\u001b[39;00m\u001b[38;5;132;01m{r_bar}\u001b[39;00m\u001b[38;5;132;01m{bar:-10b}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Init progress bar\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m conditions_chunked:\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;66;03m# parallelize_mappers(chunk, condition_params, \"condition\", csv_writer)\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m     parallelize_mappers(chunk, condition_params, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcondition\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     78\u001b[0m     pbar\u001b[38;5;241m.\u001b[39mupdate(n\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(chunk))\n\u001b[1;32m     80\u001b[0m LENGTH \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(ints_processed)  \u001b[38;5;66;03m# Number of iterations required to fill progress bar (pbar)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[20], line 22\u001b[0m, in \u001b[0;36mparallelize_mappers\u001b[0;34m(term_pair_list, params, term_type)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Prepare arguments for run_mappers\u001b[39;00m\n\u001b[1;32m     20\u001b[0m args_list \u001b[38;5;241m=\u001b[39m [(term_pair, params, term_type, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmapping_cache.tsv\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m term_pair \u001b[38;5;129;01min\u001b[39;00m term_pair_list]\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m multiprocessing\u001b[38;5;241m.\u001b[39mPool(processes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m pool:\n\u001b[1;32m     23\u001b[0m     results \u001b[38;5;241m=\u001b[39m pool\u001b[38;5;241m.\u001b[39mmap(run_mappers_wrapper, args_list)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Process results\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/multiprocessing/pool.py:739\u001b[0m, in \u001b[0;36mPool.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    738\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, exc_type, exc_val, exc_tb):\n\u001b[0;32m--> 739\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mterminate()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/multiprocessing/pool.py:657\u001b[0m, in \u001b[0;36mPool.terminate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    655\u001b[0m util\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mterminating pool\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m=\u001b[39m TERMINATE\n\u001b[0;32m--> 657\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_terminate()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/multiprocessing/util.py:224\u001b[0m, in \u001b[0;36mFinalize.__call__\u001b[0;34m(self, wr, _finalizer_registry, sub_debug, getpid)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    222\u001b[0m     sub_debug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinalizer calling \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m with args \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m and kwargs \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    223\u001b[0m               \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_kwargs)\n\u001b[0;32m--> 224\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_kwargs)\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_weakref \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_args \u001b[38;5;241m=\u001b[39m \\\n\u001b[1;32m    226\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/multiprocessing/pool.py:695\u001b[0m, in \u001b[0;36mPool._terminate_pool\u001b[0;34m(cls, taskqueue, inqueue, outqueue, pool, change_notifier, worker_handler, task_handler, result_handler, cache)\u001b[0m\n\u001b[1;32m    692\u001b[0m task_handler\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m=\u001b[39m TERMINATE\n\u001b[1;32m    694\u001b[0m util\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhelping task handler/workers to finish\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 695\u001b[0m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_help_stuff_finish(inqueue, task_handler, \u001b[38;5;28mlen\u001b[39m(pool))\n\u001b[1;32m    697\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m result_handler\u001b[38;5;241m.\u001b[39mis_alive()) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mlen\u001b[39m(cache) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m    698\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    699\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot have cache with result_hander not alive\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/multiprocessing/pool.py:675\u001b[0m, in \u001b[0;36mPool._help_stuff_finish\u001b[0;34m(inqueue, task_handler, size)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_help_stuff_finish\u001b[39m(inqueue, task_handler, size):\n\u001b[1;32m    673\u001b[0m     \u001b[38;5;66;03m# task_handler may be blocked trying to put items on inqueue\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     util\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mremoving tasks from inqueue until task handler finished\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 675\u001b[0m     inqueue\u001b[38;5;241m.\u001b[39m_rlock\u001b[38;5;241m.\u001b[39macquire()\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m task_handler\u001b[38;5;241m.\u001b[39mis_alive() \u001b[38;5;129;01mand\u001b[39;00m inqueue\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39mpoll():\n\u001b[1;32m    677\u001b[0m         inqueue\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39mrecv()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# flag_and_path = get_raw_ct_data() # download raw data\n",
    "flag_and_path = {\"term_program_flag\": False,\n",
    "                 \"data_extracted_path\": \"/Users/Kamileh/Work/ISB/NCATS_BiomedicalTranslator/Projects/ClinicalTrials/ETL_Python/data/02_27_2024_extracted\",\n",
    "                 \"date_string\": \"02_27_2024\"}\n",
    "global metamap_dirs\n",
    "metamap_dirs = check_os()\n",
    "subset_size = 40\n",
    "df_dict = read_raw_ct_data(flag_and_path, subset_size) # read the clinical trial data\n",
    "dict_new_terms = check_against_cache(df_dict) # use the existing cache of MetaMapped terms so that only new terms are mapped\n",
    "term_list_to_mappers(dict_new_terms)\n",
    "score_mappings()\n",
    "output_terms_files()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9775bd6-feaf-4536-a7d9-0b981f00df6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011a2122-0904-4b7c-938c-90ccf6027170",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a7955a-24a6-4bc1-82b2-5d4c8d81b59e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8ddb07-a201-4573-8841-cb55a6ca0399",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dc7cfe-5e65-47a3-b531-96a121472fcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388a3697-bd81-4227-a180-66edd5d84a4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fee34d5-3444-405c-ba7f-337834dcbb31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa85aed8-8fbb-4bfa-ac9c-15eba9954dde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a92caa7-a790-401b-8f1e-6ec31fb673d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ea68da-03bb-4b3c-99a2-6741f3bf829f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5027af0-eddf-4e0f-8538-a54d5bbd3d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import collections\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pickle\n",
    "from functools import reduce\n",
    "import time\n",
    "from time import sleep\n",
    "import concurrent.futures\n",
    "import multiprocessing\n",
    "import datetime as dt\n",
    "from datetime import date\n",
    "import pathlib\n",
    "import configparser\n",
    "import sys\n",
    "import urllib\n",
    "import zipfile\n",
    "import csv\n",
    "import gc\n",
    "# sys.path.insert(0, '/Volumes/TOSHIBA_EXT/ISB/clinical_trials/pymetamap-master') # for local\n",
    "sys.path.insert(0, '/users/knarsinh/projects/clinical_trials/metamap/pymetamap') # for hypatia\n",
    "from pymetamap import MetaMap  # https://github.com/AnthonyMRios/pymetamap/blob/master/pymetamap/SubprocessBackend.py\n",
    "from pandas import ExcelWriter\n",
    "import ast\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import subprocess\n",
    "import shlex\n",
    "from collections import Counter\n",
    "from ratelimit import limits, sleep_and_retry\n",
    "\n",
    "# %pip install thefuzz\n",
    "# %pip install levenshtein\n",
    "# %pip install xlsxwriter\n",
    "# %pip install ratelimit\n",
    "# %pip install timeout_decorator\n",
    "\n",
    "from thefuzz import fuzz # fuzzy matching explained: https://www.datacamp.com/tutorial/fuzzy-string-python\n",
    "\n",
    "# 40 calls per minute\n",
    "CALLS = 40\n",
    "RATE_LIMIT = 60\n",
    "\n",
    "\n",
    "def get_token_sort_ratio(str1, str2):\n",
    "    \"\"\" fuzzy matching explained: https://www.datacamp.com/tutorial/fuzzy-string-python \"\"\"\n",
    "    try:\n",
    "        return fuzz.token_sort_ratio(str1, str2)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def get_similarity_score(str1, str2):\n",
    "    \"\"\" fuzzy matching explained: https://www.datacamp.com/tutorial/fuzzy-string-python \"\"\"\n",
    "    try:\n",
    "        return fuzz.ratio(str1, str2)\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "def convert_seconds_to_hms(seconds):\n",
    "    \"\"\" converts the elapsed time or runtime to hours, min, sec \"\"\"\n",
    "    hours = seconds // 3600\n",
    "    seconds %= 3600\n",
    "    minutes = seconds // 60\n",
    "    seconds %= 60\n",
    "    return hours, minutes, seconds\n",
    "\n",
    "def de_ascii_er(text):\n",
    "    non_ascii = \"[^\\x00-\\x7F]\"\n",
    "    pattern = re.compile(r\"[^\\x00-\\x7F]\")\n",
    "    non_ascii_text = re.sub(pattern, ' ', text)\n",
    "    return non_ascii_text\n",
    "\n",
    "def start_metamap_servers(metamap_dirs):\n",
    "    global metamap_pos_server_dir\n",
    "    global metamap_wsd_server_dir\n",
    "    metamap_pos_server_dir = 'bin/skrmedpostctl' # Part of speech tagger\n",
    "    metamap_wsd_server_dir = 'bin/wsdserverctl' # Word sense disambiguation \n",
    "    \n",
    "    metamap_executable_path_pos = os.path.join(metamap_dirs['metamap_base_dir'], metamap_pos_server_dir)\n",
    "    metamap_executable_path_wsd = os.path.join(metamap_dirs['metamap_base_dir'], metamap_wsd_server_dir)\n",
    "    command_pos = [metamap_executable_path_pos, 'start']\n",
    "    command_wsd = [metamap_executable_path_wsd, 'start']\n",
    "\n",
    "    # Start servers, with open portion redirects output of metamap server printing output to NULL\n",
    "    with open(os.devnull, \"w\") as fnull:\n",
    "        result_post = subprocess.call(command_pos, stdout = fnull, stderr = fnull)\n",
    "        result_wsd = subprocess.call(command_wsd, stdout = fnull, stderr = fnull)\n",
    "    sleep(5)\n",
    "\n",
    "def stop_metamap_servers(metamap_dirs):\n",
    "    metamap_executable_path_pos = os.path.join(metamap_dirs['metamap_base_dir'], metamap_pos_server_dir)\n",
    "    metamap_executable_path_wsd = os.path.join(metamap_dirs['metamap_base_dir'], metamap_wsd_server_dir)\n",
    "    command_pos = [metamap_executable_path_pos, 'stop']\n",
    "    command_wsd = [metamap_executable_path_wsd, 'stop']\n",
    "    \n",
    "    # Stop servers, with open portion redirects output of metamap server printing output to NULL\n",
    "    with open(os.devnull, \"w\") as fnull:\n",
    "        result_post = subprocess.call(command_pos, stdout = fnull, stderr = fnull)\n",
    "        result_wsd = subprocess.call(command_wsd, stdout = fnull, stderr = fnull)\n",
    "    sleep(2)  \n",
    "    \n",
    "def check_os():\n",
    "    if \"linux\" in sys.platform:\n",
    "        print(\"Linux platform detected\")\n",
    "        # metamap_base_dir = \"{}/metamap/\".format(pathlib.Path.cwd().parents[0])\n",
    "        metamap_base_dir = \"/users/knarsinh/projects/clinical_trials/metamap/public_mm/\"\n",
    "        metamap_bin_dir = 'bin/metamap20'\n",
    "    else:\n",
    "        metamap_base_dir = '/Volumes/TOSHIBA_EXT/ISB/clinical_trials/public_mm/' # for running on local\n",
    "        metamap_bin_dir = 'bin/metamap18'\n",
    "        \n",
    "    return {\"metamap_base_dir\":metamap_base_dir, \"metamap_bin_dir\":metamap_bin_dir} \n",
    "\n",
    "@sleep_and_retry\n",
    "@limits(calls=CALLS, period=RATE_LIMIT)\n",
    "def check_limit():\n",
    "    ''' Empty function just to check for calls to Name Resolver API '''\n",
    "    return\n",
    "\n",
    "def wrap(x): # use this to convert string objects to dicts \n",
    "    try:\n",
    "        a = ast.literal_eval(x)\n",
    "        return(a)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "def get_raw_ct_data():\n",
    "    term_program_flag = True\n",
    "    global data_dir\n",
    "    global data_extracted\n",
    "    \n",
    "    try:\n",
    "        # get all the links and associated dates of upload into a dict called date_link\n",
    "        url_all = \"https://aact.ctti-clinicaltrials.org/download\"\n",
    "        response = requests.get(url_all)\n",
    "        soup = BeautifulSoup(response.text, features=\"lxml\")\n",
    "        body = soup.find_all('option') #Find all\n",
    "        date_link = {}\n",
    "        for el in body:\n",
    "            tags = el.find('a')\n",
    "            try:\n",
    "                zip_name = tags.contents[0].split()[0]\n",
    "                date = zip_name.split(\"_\")[0]\n",
    "                date = dt.datetime.strptime(date, '%Y%m%d').date()\n",
    "                date_link[date] = tags.get('href')\n",
    "            except:\n",
    "                pass\n",
    "        latest_file_date = max(date_link.keys())   # get the date of the latest upload\n",
    "        url = date_link[latest_file_date]   # get the corresponding download link of the latest upload so we can download the raw data\n",
    "        date_string = latest_file_date.strftime(\"%m_%d_%Y\")\n",
    "        data_dir = \"{}/data\".format(pathlib.Path.cwd())\n",
    "        data_extracted = data_dir + \"/{}_extracted\".format(date_string)\n",
    "        data_path = \"{}/{}_pipe-delimited-export.zip\".format(data_dir, date_string)\n",
    "    except:\n",
    "        print(\"continue\")\n",
    "\n",
    "    if not os.path.exists(data_path):   # if folder containing most recent data doesn't exist, download and extract it into data folder\n",
    "\n",
    "        term_program_flag = False   # flag below for terminating program if latest download exists (KG is assumed up to date)\n",
    "        print(\"Attempting download of Clinical Trial data as of {}\\n\".format(date_string))\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            if response.status_code == 200:\n",
    "                with open(data_path, 'wb') as file:\n",
    "                    file.write(response.content)\n",
    "                print(\"Finished download of zip\")\n",
    "                with zipfile.ZipFile(data_path, 'r') as download:\n",
    "                    print(\"Unzipping data\")\n",
    "                    download.extractall(data_extracted)\n",
    "        except:\n",
    "            print(\"Failed to scrape AACT for download. Please navigate to https://aact.ctti-clinicaltrials.org/download and manually download zip file.\")\n",
    "            print(\"Please store the downloaded zip in the /data directory. This should be the only item besides the cache file, condition manual review file, and intervention manual review file, in the directory at this time.\")\n",
    "            done = input(\"Type Done when done: \")\n",
    "            if done == \"Done\":\n",
    "                data_dir = \"{}/data\".format(pathlib.Path.cwd())\n",
    "                # list_of_files = glob.glob(data_dir + \"/*\") # get all files in directory\n",
    "                try:\n",
    "                    # latest_file = max(list_of_files, key=os.path.getctime) # get the most recent file in the directory\n",
    "                    pattern = os.path.join(data_dir, \"*.zip\")\n",
    "                    zip_file = glob.glob(pattern) # look for file in directory that ends in \".zip\"\n",
    "                    zip_file = zip_file[0]\n",
    "                    print(\"File found at: \")\n",
    "                    print(zip_file)\n",
    "                    # print(latest_file)\n",
    "                    print(\"Please make sure this the correct zip file from AACT\")\n",
    "                    if not os.path.exists(data_extracted):   # if folder of unzipped data does not exist, unzip\n",
    "                        try:\n",
    "                            with zipfile.ZipFile(zip_file, 'r') as download:\n",
    "                                print(\"Unzipping data into\")\n",
    "                                cttime = os.path.getctime(zip_file)\n",
    "                                date_string = dt.datetime.fromtimestamp(cttime).strftime('%m_%d_%Y')\n",
    "                                data_extracted = data_dir + \"/{}_extracted\".format(date_string)\n",
    "                                print(data_extracted)\n",
    "                                download.extractall(data_extracted)\n",
    "                        except:\n",
    "                            pattern = os.path.join(data_dir, \"*_extracted\")\n",
    "                            extracted_file = glob.glob(pattern) # look for file in directory that ends in \"_extracted\"\n",
    "                            data_extracted = extracted_file[0]\n",
    "                            extracted_name = os.path.basename(os.path.normpath(extracted_file[0]))\n",
    "                            date_string = extracted_name.replace('_extracted', '')\n",
    "                            print(\"Assuming data is already unzipped\")\n",
    "                        \n",
    "                except:\n",
    "                    print(\"Unable to download and extract Clincal Trial data.\")\n",
    "                    print(\"Cannot find pipe-delimited zip in /data folder.\")\n",
    "    else:\n",
    "        print(\"KG is already up to date.\")\n",
    "\n",
    "    return {\"term_program_flag\": term_program_flag, \"data_extracted_path\": data_extracted, \"date_string\": date_string}\n",
    "\n",
    "def read_raw_ct_data(flag_and_path, subset_size):\n",
    "    if flag_and_path[\"term_program_flag\"]:\n",
    "        print(\"Exiting program. Assuming KG has already been constructed from most recent data dump from AACT.\")\n",
    "        exit()\n",
    "    else:\n",
    "        data_extracted = flag_and_path[\"data_extracted_path\"]\n",
    "        # read in pipe-delimited files \n",
    "        conditions_df = pd.read_csv(data_extracted + '/conditions.txt.gz', sep='|', index_col=False, header=0, on_bad_lines = 'warn')\n",
    "        interventions_df = pd.read_csv(data_extracted + '/interventions.txt.gz', sep='|', index_col=False, header=0, on_bad_lines = 'warn')\n",
    "        interventions_alts_df = pd.read_csv(data_extracted + '/intervention_other_names.txt.gz', sep='|', index_col=False, header=0, on_bad_lines = 'warn')\n",
    "\n",
    "        if subset_size:   # if a subset size is given, we are running this script on a small subset of the dataset\n",
    "            conditions_df = conditions_df.sample(n=subset_size)\n",
    "            interventions_df = interventions_df.sample(n=subset_size)\n",
    "            interventions_alts_df = interventions_alts_df.sample(n=subset_size)\n",
    "    \n",
    "    df_dict = {\"conditions\": conditions_df, \"interventions\": interventions_df, \"interventions_alts\": interventions_alts_df}\n",
    "    return df_dict\n",
    "\n",
    "\n",
    "\n",
    "def cache_manually_selected_terms():    \n",
    "\n",
    "    def return_curie_dict(curie_info_delimited):\n",
    "        keys = [\"mapped_name\", \"mapped_curie\", \"mapped_score\", \"mapped_semtypes\"]\n",
    "        curie_list = curie_info_delimited.split(\" | \")\n",
    "        curie_dict = dict(zip(keys, curie_list))\n",
    "        return curie_dict\n",
    "\n",
    "    files = glob.glob(\"*.xlsx\")\n",
    "    manually_selected_file = [i for i in files if \"manual_review\" in i if not i.startswith(\"~\")][0] # find the file of manual selections\n",
    "    manually_selected = pd.read_excel(manually_selected_file)\n",
    "    cols_to_fill = [\"mapping_tool\", \"term_type\", \"clintrial_term\", \"input_term\"]\n",
    "    manually_selected.loc[:,cols_to_fill] = manually_selected.loc[:,cols_to_fill].ffill()\n",
    "\n",
    "    manually_selected = manually_selected[~manually_selected['manually_selected_CURIE'].isnull()] # get rows where terms were manually chosen\n",
    "    manually_selected.drop([\"mapping_tool_response\"], axis = 1, inplace = True)\n",
    "\n",
    "    manually_selected[\"manually_selected_CURIE\"] = manually_selected[\"manually_selected_CURIE\"].apply(lambda x: return_curie_dict(x)) # convert | delimited strings to CURIE dict\n",
    "    manually_selected[\"score\"] = 1000  # human curated score = 1000\n",
    "    manually_selected.rename(columns = {'manually_selected_CURIE':'mapping_tool_response'}, inplace = True)\n",
    "    manually_selected = manually_selected[[\"mapping_tool\", \"term_type\", \"clintrial_term\", \"input_term\", \"mapping_tool_response\", \"score\"]] # reorder columns to be same as the cache files we're appending to \n",
    "    manually_selected.to_csv(\"mapping_cache.tsv\", mode='a', header=False, sep =\"\\t\", index=False)\n",
    "\n",
    "\n",
    "def check_against_cache(df_dict):\n",
    "    \n",
    "    conditions_list = df_dict['conditions'].name.unique().tolist()\n",
    "    conditions_list = [str(i) for i in conditions_list]\n",
    "    conditions_list = list(set([i.lower() for i in conditions_list]))\n",
    "    \n",
    "    interventions_list = df_dict['interventions'].name.unique().tolist()\n",
    "    interventions_list = [str(i) for i in interventions_list]\n",
    "    interventions_list = list(set([i.lower() for i in interventions_list]))\n",
    "    \n",
    "    interventions_alts_list = df_dict['interventions_alts'].name.unique().tolist()\n",
    "    interventions_alts_list = [str(i) for i in interventions_alts_list]\n",
    "    interventions_alts_list = list(set([i.lower() for i in interventions_alts_list]))\n",
    "    \n",
    "    try:\n",
    "        cache_manually_selected_terms()\n",
    "    except:\n",
    "        print(\"No manually selected terms file found\")\n",
    "    \n",
    "    try:        \n",
    "        cache_df = pd.read_csv(\"mapping_cache.tsv\", sep =\"\\t\", index_col=False, header=0, on_bad_lines = 'warn')\n",
    "        \n",
    "        conditions_cache = cache_df[cache_df[\"term_type\"] == \"condition\"]\n",
    "        conditions_cache = conditions_cache['clintrial_term'].unique().tolist()\n",
    "        conditions_cache = list(set([i.lower() for i in conditions_cache]))\n",
    "        \n",
    "        conditions_new = [x for x in conditions_list if x not in conditions_cache] # find conditions not in the cache (i.g. new conditions to map)\n",
    "        conditions_new = list(filter(None, conditions_new))\n",
    "        conditions_new = [str(i) for i in conditions_new]\n",
    "        \n",
    "        interventions_cache = cache_df[cache_df[\"term_type\"] == \"intervention\"]\n",
    "        interventions_cache = interventions_cache['clintrial_term'].unique().tolist()\n",
    "        interventions_cache = list(set([i.lower() for i in interventions_cache]))\n",
    "        \n",
    "        interventions_new = [x for x in interventions_list if x not in interventions_cache] # find interventions not in the cache (i.g. new interventions to map)\n",
    "        interventions_new = list(filter(None, interventions_new))\n",
    "        interventions_new = [str(i) for i in interventions_new]\n",
    "        \n",
    "        interventions_alts_cache = cache_df[cache_df[\"term_type\"] == \"intervention_alternate\"]\n",
    "        interventions_alts_cache = interventions_alts_cache['clintrial_term'].unique().tolist()\n",
    "        interventions_alts_cache = list(set([i.lower() for i in interventions_alts_cache]))\n",
    "        \n",
    "        interventions_alts_new = [x for x in interventions_alts_list if x not in interventions_alts_cache] # find interventions_alts not in the cache (i.g. new interventions_alts to map)\n",
    "        interventions_alts_new = list(filter(None, interventions_alts_new))\n",
    "        interventions_alts_new = [str(i) for i in interventions_alts_new]\n",
    "        \n",
    "    except:\n",
    "        print(\"No cache of terms found. Proceeding to map entire KG from scratch\")\n",
    "        conditions_new = conditions_list\n",
    "        interventions_new = interventions_list\n",
    "        interventions_alts_new = interventions_alts_list\n",
    "        \n",
    "    dict_new_terms = {\"conditions\": conditions_new, \"interventions\": interventions_new, \"interventions_alts\": interventions_alts_new}\n",
    "\n",
    "    return dict_new_terms\n",
    "\n",
    "\n",
    "def get_nr_response(orig_term):\n",
    "    def create_session():\n",
    "        s = requests.Session()\n",
    "        return s\n",
    " \n",
    "    sess = create_session()\n",
    " \n",
    "    \"\"\"   Runs Name Resolver   \"\"\"\n",
    "    nr_url = 'https://name-resolution-sri.renci.org/lookup'\n",
    "    max_retries = 3 \n",
    "    \n",
    "    input_term = orig_term # in MetaMap, we have to potentially deascii the term and lower case it...for Name Resolver, we don't need to do that. To keep columns consist with MetaMap output, we just keep it and say the original term and the input term are the same. For MetaMap, they might be different\n",
    "    retries = 0\n",
    "    params = {'string':orig_term, 'limit':1} # limit -1 makes this return all available equivalent CURIEs name resolver can give (deprecated)\n",
    "    while retries <= max_retries:\n",
    "        try:\n",
    "            r = sess.post(nr_url, params=params)\n",
    "            check_limit() # counts how many requests have been sent to NR. If limit of 40 have been sent, sleeps for 1 min\n",
    "            if r.status_code == 200:\n",
    "                mapping_tool_response = r.json()  # process Name Resolver response\n",
    "                return mapping_tool_response\n",
    "            else:\n",
    "                return None\n",
    "        except (requests.RequestException, ConnectionResetError, OSError) as ex:\n",
    "            print(f\"\\nName Resolver request failed for term: {term}. Error: {ex}\")\n",
    "            retries += 1\n",
    "            if retries < max_retries:\n",
    "                print(f\"Retrying ({retries}/{max_retries}) after a delay.\")\n",
    "                time.sleep(2 ** retries)  # Increase the delay between retries exponentially\n",
    "            else:\n",
    "                print(f\"Max retries (Name Resolver) reached for term: {term}.\")\n",
    "                return None\n",
    "\n",
    "# I'm only getting 1 concept from Name Resolver. \n",
    "# Both MetaMap and Name Resolver return several, \n",
    "# but I only take 1 from Name Resolver bc they have a preferred concept.\n",
    "# MetaMap's 2nd or 3rd result is often the best one, so I collect all of them and try to score\"\n",
    "\n",
    "def process_metamap_concept(concept):\n",
    "    concept = concept._asdict()\n",
    "    concept_dict  = {\"mapped_name\": concept.get(\"preferred_name\"),\n",
    "                     \"mapped_curie\": concept.get(\"cui\"),\n",
    "                     \"mapped_score\": concept.get(\"score\"),\n",
    "                     \"mapped_semtypes\": concept.get(\"semtypes\")}\n",
    "    if not concept.get(\"preferred_name\"): # if condition triggered if the concept dict looks like following, where AA is for Abbreviation.... {'index': 'USER', 'aa': 'AA', 'short_form': 'copd', 'long_form': 'chronic obstructive pulmonary disease', 'num_tokens_short_form': '1', 'num_chars_short_form': '4', 'num_tokens_long_form': '7', 'num_chars_long_form': '37', 'pos_info': '43:4'}\n",
    "        concept_dict = None\n",
    "    return concept_dict\n",
    "\n",
    "def process_nameresolver_response(nr_response):              \n",
    "    nr_curie = nr_response[0][\"curie\"]\n",
    "    nr_name = nr_response[0][\"label\"]\n",
    "    nr_type = nr_response[0][\"types\"][0]\n",
    "    nr_score = nr_response[0][\"score\"]\n",
    "    concept_dict = {\"mapped_name\": nr_name,\n",
    "                    \"mapped_curie\": nr_curie,\n",
    "                    \"mapped_score\": nr_score,\n",
    "                    \"mapped_semtypes\": nr_type}\n",
    "    return concept_dict\n",
    "\n",
    "\n",
    "def run_mappers(term_pair, params, term_type, csv_writer):\n",
    "    # check_count()\n",
    "\n",
    "    orig_term = term_pair[0]\n",
    "    input_term = term_pair[1]\n",
    "    from_mapper = []\n",
    "    mm = MetaMap.get_instance(metamap_dirs[\"metamap_base_dir\"] + metamap_dirs[\"metamap_bin_dir\"])\n",
    "\n",
    "    # Format of output TSV: header = ['mapping_tool', 'term_type', 'clintrial_term', 'input_term', 'mapping_tool_response', 'score']\n",
    "\n",
    "    if params.get(\"exclude_sts\") is None: # exclude_sts is used for Interventions. restrict_to_sts is used for Conditions. So, the logic is, if we're mapping Conditions, execute \"if\" part of code. If we're mapping Interventions, execute \"else\" part of code\n",
    "        try:\n",
    "            concepts,error = mm.extract_concepts([input_term],\n",
    "                                                 restrict_to_sts = params[\"restrict_to_sts\"],\n",
    "                                                 term_processing = params[\"term_processing\"],\n",
    "                                                 ignore_word_order = params[\"ignore_word_order\"],\n",
    "                                                 strict_model = params[\"strict_model\"],)\n",
    "                                                    \n",
    "            if concepts:   # if MetaMap gives response, process response\n",
    "                mapping_tool = \"metamap\"\n",
    "                for concept in concepts:\n",
    "                    concept_info = []\n",
    "                    new_concept_dict = process_metamap_concept(concept)\n",
    "                    concept_info.extend([mapping_tool, term_type, orig_term, input_term, new_concept_dict]) # score column is empty, Format of output TSV: header = ['mapping_tool', 'term_type', 'clintrial_term', 'input_term', 'mapping_tool_response', 'score']\n",
    "                    from_mapper.append(concept_info)\n",
    "            else:   # if MetaMap fails, try using Name Resolver and process response\n",
    "                nr_response = get_nr_response(orig_term)\n",
    "                if nr_response: # if Name Resolver gives response, process repsonse\n",
    "                    input_term = orig_term # no preprocessing (lowercasing or deascii-ing) necessary to submit terms to Name Resolver (unlike MetaMap)\n",
    "                    mapping_tool = \"nameresolver\"\n",
    "                    concept_info = []\n",
    "                    new_concept_dict = process_nameresolver_response(nr_response)\n",
    "                    concept_info.extend([mapping_tool, term_type, orig_term, input_term, new_concept_dict]) # Add None for score column, empty bc not scored yet\n",
    "                    from_mapper.append(concept_info)\n",
    "                else:\n",
    "                    concept_info = []\n",
    "                    # print(\"Nothing returned from NR or Metamap\")\n",
    "                    concept_info.extend([\"mapping_tools_failed\", term_type, orig_term, input_term, \"mapping_tools_failed\"])\n",
    "                    from_mapper.append(concept_info)\n",
    "        except:\n",
    "            concept_info = []\n",
    "            # print(\"Nothing returned from NR or Metamap\")\n",
    "            concept_info.extend([\"mapping_tools_failed\", term_type, orig_term, input_term, \"mapping_tools_failed\"])\n",
    "            from_mapper.append(concept_info)\n",
    "            \n",
    "    else:   # Else block triggered if mapping Interventions\n",
    "        try:\n",
    "            concepts,error = mm.extract_concepts([input_term],\n",
    "                                                 exclude_sts = params[\"exclude_sts\"],\n",
    "                                                 term_processing = params[\"term_processing\"],\n",
    "                                                 ignore_word_order = params[\"ignore_word_order\"],\n",
    "                                                 strict_model = params[\"strict_model\"],) \n",
    "                                                   \n",
    "            if concepts:   # if MetaMap gives response, process response\n",
    "                mapping_tool = \"metamap\"\n",
    "                for concept in concepts:\n",
    "                    concept_info = []\n",
    "                    new_concept_dict = process_metamap_concept(concept)\n",
    "                    concept_info.extend([mapping_tool, term_type, orig_term, input_term, new_concept_dict]) # score column is empty, Format of output TSV: header = ['mapping_tool', 'term_type', 'clintrial_term', 'input_term', 'mapping_tool_response', 'score']\n",
    "                    from_mapper.append(concept_info)\n",
    "            else:   # if MetaMap fails, try using Name Resolver and process response\n",
    "                nr_response = get_nr_response(orig_term) \n",
    "                if nr_response: # if Name Resolver gives response, process repsonse\n",
    "                    input_term = orig_term # no preprocessing (lowercasing or deascii-ing) necessary to submit terms to Name Resolver (unlike MetaMap)\n",
    "                    mapping_tool = \"nameresolver\"\n",
    "                    concept_info = []\n",
    "                    new_concept_dict = process_nameresolver_response(nr_response)\n",
    "                    concept_info.extend([mapping_tool, term_type, orig_term, input_term, new_concept_dict])\n",
    "                    from_mapper.append(concept_info)\n",
    "                else:\n",
    "                    concept_info = []\n",
    "                    # print(\"Nothing returned from NR or Metamap\")\n",
    "                    concept_info.extend([\"mapping_tools_failed\", term_type, orig_term, input_term, \"mapping_tools_failed\"])\n",
    "                    from_mapper.append(concept_info)\n",
    "        except:\n",
    "            concept_info = []\n",
    "            # print(\"Nothing returned from NR or Metamap\")\n",
    "            concept_info.extend([\"mapping_tools_failed\", term_type, orig_term, input_term, \"mapping_tools_failed\"])\n",
    "            from_mapper.append(concept_info)\n",
    "      \n",
    "    for result in from_mapper:\n",
    "        # print(result)\n",
    "        if result[0] == \"mapping_tools_failed\":\n",
    "            result.append(-1)\n",
    "        else:\n",
    "            result.append(\"unscored\")\n",
    "        # print(result)\n",
    "\n",
    "        csv_writer.writerow(result)\n",
    "    \n",
    "def parallelize_mappers(term_pair_list, params, term_type, csv_writer):\n",
    "    \n",
    "    start_metamap_servers(metamap_dirs) # start the MetaMap servers\n",
    "    terms_left = len(term_pair_list)\n",
    "    future_to_pair = {}\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=20) as executor:\n",
    "        future_to_pair = {executor.submit(run_mappers, term_pair, params, term_type, csv_writer): term_pair for term_pair in term_pair_list}\n",
    "        try:\n",
    "            for future in concurrent.futures.as_completed(future_to_pair, timeout=180): # timeout after 3 min\n",
    "                term_pair = future_to_pair[future]\n",
    "                try:\n",
    "                    result = future.result()\n",
    "                    # Process result if needed\n",
    "                except Exception as exc:\n",
    "                    print(f\"Job {term_pair} generated an exception: {exc}\")\n",
    "                finally:\n",
    "                    terms_left -= 1\n",
    "                    if terms_left % 10 == 0:\n",
    "                        gc.collect()\n",
    "                        time.sleep(2)\n",
    "        except concurrent.futures.TimeoutError:\n",
    "            print(\"Timeout occurred while processing futures\")\n",
    "            for key, future in future_to_pair.items():\n",
    "                if key not in results:\n",
    "                    future.cancel()\n",
    "\n",
    "    stop_metamap_servers(metamap_dirs) # stop the MetaMap servers\n",
    "\n",
    "\n",
    "def term_list_to_mappers(dict_new_terms):   \n",
    "    metamap_version = [int(s) for s in re.findall(r'\\d+', metamap_dirs.get('metamap_bin_dir'))] # get MetaMap version being run \n",
    "    deasciier = np.vectorize(de_ascii_er) # vectorize function\n",
    "    \n",
    "    # open mapping cache to add mapped terms\n",
    "    mapping_filename = \"mapping_cache.tsv\"\n",
    "    if os.path.exists(mapping_filename):\n",
    "        output = open(mapping_filename, 'a', newline='', encoding=\"utf-8\") \n",
    "        csv_writer = csv.writer(output, delimiter='\\t')\n",
    "    else:\n",
    "        output = open(mapping_filename, 'w+', newline='', encoding='utf-8')\n",
    "        col_names = ['mapping_tool', 'term_type', 'clintrial_term', 'input_term', 'mapping_tool_response', 'score']\n",
    "        csv_writer = csv.writer(output, delimiter='\\t')\n",
    "        csv_writer.writerow(col_names)\n",
    "\n",
    "    #  - Conditions\n",
    "    condition_semantic_type_restriction = ['acab,anab,cgab,comd,dsyn,inpo,mobd,neop,patf,clna,fndg']  # see https://lhncbc.nlm.nih.gov/ii/tools/MetaMap/Docs/SemanticTypes_2018AB.txt for semantic types (\"acab,anab,etc.\")\n",
    "    conditions = dict_new_terms.get(\"conditions\")\n",
    "    condition_params = {\"restrict_to_sts\":condition_semantic_type_restriction, \"term_processing\":True, \"ignore_word_order\":True, \"strict_model\":False} # strict_model and relaxed_model are presumably opposites? relaxed_model = True is what I want, but that option appears to be broken in Pymetamap (returns no results when used). Using strict_model = False instead...\n",
    "    # conditon_term_type = \"condition\"\n",
    "\n",
    "    #  - Interventions\n",
    "    condition_semantic_type_restriction = ['acab,anab,cgab,comd,dsyn,inpo,mobd,neop,patf,clna,fndg']  # see https://lhncbc.nlm.nih.gov/ii/tools/MetaMap/Docs/SemanticTypes_2018AB.txt for semantic types (\"acab,anab,etc.\")\n",
    "    interventions = dict_new_terms.get(\"interventions\")\n",
    "    intervention_params = {\"exclude_sts\":condition_semantic_type_restriction, \"term_processing\":True, \"ignore_word_order\":True, \"strict_model\":False} # strict_model and relaxed_model are presumably opposites? relaxed_model = True is what I want, but that option appears to be broken in Pymetamap (returns no results when used). Using strict_model = False instead...\n",
    "    # intervention_term_type = \"intervention\"\n",
    "\n",
    "    #  - Alternate Interventions\n",
    "    condition_semantic_type_restriction = ['acab,anab,cgab,comd,dsyn,inpo,mobd,neop,patf,clna,fndg']  # see https://lhncbc.nlm.nih.gov/ii/tools/MetaMap/Docs/SemanticTypes_2018AB.txt for semantic types (\"acab,anab,etc.\")\n",
    "    interventions_alts = dict_new_terms.get(\"interventions_alts\")\n",
    "    intervention_alts_params = intervention_params # same params as interventions\n",
    "    # intervention_alternate_term_type = \"intervention_alternate\"\n",
    "    \n",
    "    chunksize = 20\n",
    "    \n",
    "    if metamap_version[0] >= 20:\n",
    "        \n",
    "        cons_processed = list(zip(conditions, conditions))  # these are lists of the same term repeated twice, bc MetaMap 2020 does not require deasciing, so the 2nd term remains unchanged and is a repeat of the first term\n",
    "        ints_processed = list(zip(interventions, interventions))\n",
    "        ints_alts_processed = list(zip(interventions_alts, interventions_alts))\n",
    "    \n",
    "        conditions_chunked = [cons_processed[i:i + chunksize] for i in range(0, len(cons_processed), chunksize)]  \n",
    "        interventions_chunked = [ints_processed[i:i + chunksize] for i in range(0, len(ints_processed), chunksize)]  \n",
    "        interventions_alts_chunked = [ints_alts_processed[i:i + chunksize] for i in range(0, len(ints_alts_processed), chunksize)] \n",
    "        \n",
    "        print(\"MetaMap version >= 2020, conduct mapping on original terms\")\n",
    "        \n",
    "        LENGTH = len(cons_processed)  # Number of iterations required to fill progress bar (pbar)\n",
    "        # pbar = tqdm(total=LENGTH, desc=\"% conditions mapped\", position=0, leave=True, mininterval = LENGTH/20, bar_format='{l_bar}{bar:20}{r_bar}{bar:-10b}')  # Init progress bar\n",
    "        pbar = tqdm(total=LENGTH, desc=\"% conditions mapped\", position=0, leave=True, mininterval = LENGTH/40, bar_format='{l_bar}{bar:40}{r_bar}{bar:-10b}')  # Init progress bar\n",
    "        for chunk in conditions_chunked:\n",
    "            parallelize_mappers(chunk, condition_params, \"condition\", csv_writer)\n",
    "            pbar.update(n=len(chunk))\n",
    "        \n",
    "        LENGTH = len(ints_processed)  # Number of iterations required to fill progress bar (pbar)\n",
    "        pbar = tqdm(total=LENGTH, desc=\"% interventions mapped\", position=0, leave=True, mininterval = LENGTH/40, bar_format='{l_bar}{bar:40}{r_bar}{bar:-10b}')  # Init progress bar\n",
    "        for chunk in interventions_chunked:\n",
    "            parallelize_mappers(chunk, intervention_params, \"intervention\", csv_writer)\n",
    "            pbar.update(n=len(chunk))\n",
    "        \n",
    "        LENGTH = len(ints_alts_processed)  # Number of iterations required to fill progress bar (pbar)\n",
    "        pbar = tqdm(total=LENGTH, desc=\"% alternate interventions mapped\", position=0, leave=True, mininterval = LENGTH/40, bar_format='{l_bar}{bar:40}{r_bar}{bar:-10b}')  # Init progress bar\n",
    "        for chunk in interventions_alts_chunked:\n",
    "            parallelize_mappers(chunk, intervention_alts_params, \"alternate_intervention\", csv_writer)\n",
    "            pbar.update(n=len(chunk))\n",
    "        \n",
    "    else:\n",
    "        print(\"MetaMap version < 2020, conduct mapping on terms after removing ascii characters\")\n",
    "        \n",
    "        deascii_cons = deasciier(conditions)\n",
    "        deascii_ints = deasciier(interventions)\n",
    "        deascii_int_alts = deasciier(interventions_alts)\n",
    "                \n",
    "        cons_processed = list(zip(conditions, deascii_cons)) # these are lists of the original term, and the deasciied term, bc MetaMap 2018 does not process ascii characters\n",
    "        ints_processed = list(zip(interventions, deascii_ints))\n",
    "        ints_alts_processed = list(zip(interventions_alts, deascii_int_alts))\n",
    "        \n",
    "        conditions_chunked = [cons_processed[i:i + chunksize] for i in range(0, len(cons_processed), chunksize)]  \n",
    "        interventions_chunked = [ints_processed[i:i + chunksize] for i in range(0, len(ints_processed), chunksize)]  \n",
    "        interventions_alts_chunked = [ints_alts_processed[i:i + chunksize] for i in range(0, len(ints_alts_processed), chunksize)] \n",
    "        \n",
    "        LENGTH = len(cons_processed)  # Number of iterations required to fill progress bar (pbar)\n",
    "        pbar = tqdm(total=LENGTH, desc=\"% conditions mapped\", position=0, leave=True, mininterval = LENGTH/40, bar_format='{l_bar}{bar:40}{r_bar}{bar:-10b}')  # Init progress bar\n",
    "        for chunk in conditions_chunked:\n",
    "            parallelize_mappers(chunk, condition_params, \"condition\", csv_writer)\n",
    "            pbar.update(n=len(chunk))\n",
    "        \n",
    "        LENGTH = len(ints_processed)  # Number of iterations required to fill progress bar (pbar)\n",
    "        pbar = tqdm(total=LENGTH, desc=\"% interventions mapped\", position=0, leave=True, mininterval = LENGTH/40, bar_format='{l_bar}{bar:40}{r_bar}{bar:-10b}')  # Init progress bar\n",
    "        for chunk in interventions_chunked:\n",
    "            parallelize_mappers(chunk, intervention_params, \"intervention\", csv_writer)\n",
    "            pbar.update(n=len(chunk))\n",
    "        \n",
    "        LENGTH = len(ints_alts_processed)  # Number of iterations required to fill progress bar (pbar)\n",
    "        pbar = tqdm(total=LENGTH, desc=\"% alternate interventions mapped\", position=0, leave=True, mininterval = LENGTH/40, bar_format='{l_bar}{bar:40}{r_bar}{bar:-10b}')  # Init progress bar\n",
    "        for chunk in interventions_alts_chunked:\n",
    "            parallelize_mappers(chunk, intervention_alts_params, \"alternate_intervention\", csv_writer)\n",
    "            pbar.update(n=len(chunk))\n",
    "\n",
    "    output.close()\n",
    "    \n",
    "    # \"\"\" Remove duplicate rows \"\"\"\n",
    "    mapping_filename = \"mapping_cache.tsv\"\n",
    "    cache = pd.read_csv(mapping_filename, sep='\\t', index_col=False, header=0, encoding_errors='ignore')\n",
    "    cache = cache.drop_duplicates()\n",
    "    cache.to_csv(mapping_filename, sep=\"\\t\", index=False, header=True) # output deduplicated cache terms to TSV\n",
    "    \n",
    "\n",
    "def score_mappings():\n",
    "    print(\"Scoring cache\")\n",
    "\n",
    "    def get_max_score(str1, str2, old_score):\n",
    "        \n",
    "        try:\n",
    "            if old_score == \"unscored\":\n",
    "                sortratio_score = get_token_sort_ratio(str1, str2)\n",
    "                similarity_score = get_similarity_score(str1, str2)\n",
    "                max_score = max(sortratio_score, similarity_score)\n",
    "                score = max_score\n",
    "            else:\n",
    "                score = old_score   \n",
    "        except:\n",
    "            score = old_score\n",
    "        return score\n",
    "\n",
    "    def wrap(x): # use this to convert string objects to dicts \n",
    "        try:\n",
    "            a = ast.literal_eval(x)\n",
    "            return(a)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    with pd.read_csv(\"mapping_cache.tsv\", sep='\\t', index_col=False, header=0, on_bad_lines = 'warn', usecols=lambda c: not c.startswith('Unnamed:'), chunksize=1000) as reader:\n",
    "        write_header = True\n",
    "        for chunk in reader:\n",
    "            chunk[\"mapping_tool_response\"] = chunk[\"mapping_tool_response\"].apply(lambda x: wrap(x))\n",
    "            mapping_info = chunk[\"mapping_tool_response\"].apply(pd.Series, dtype='object')\n",
    "            chunk[\"mapped_name\"] = mapping_info[\"mapped_name\"]\n",
    "            chunk[\"score\"] = chunk.apply(lambda x: get_max_score(x['input_term'], x['mapped_name'], x['score']), axis=1) # get score for score rows that are empty/not scored yet\n",
    "            chunk.drop([\"mapped_name\"], axis = 1, inplace = True)\n",
    "            chunk.to_csv(f'mapping_cache_scored_temp.tsv', sep=\"\\t\", index=False, header=write_header, mode = 'a', encoding=\"utf-8\") # output to TSV\n",
    "            write_header = False\n",
    "\n",
    "    os.rename('mapping_cache.tsv','mapping_cache_backup.tsv')       \n",
    "    os.rename('mapping_cache_scored_temp.tsv','mapping_cache.tsv')\n",
    "\n",
    "\n",
    "def output_terms_files():\n",
    "    print(\"Generating output files\")\n",
    "\n",
    "    \"\"\"   Get high scorers   \"\"\"\n",
    "    cache = pd.read_csv(\"mapping_cache.tsv\", sep='\\t', index_col=False, header=0)\n",
    "    cache['score'] = pd.to_numeric(cache['score'], errors='coerce')\n",
    "    highscorers = cache[cache['score'] >= 80] \n",
    "    # test = highscorers.groupby('clintrial_term')\n",
    "    idx = highscorers.groupby('clintrial_term')['score'].idxmax()  # group by the clinical trial term and get the highest scoring\n",
    "    auto_selected = highscorers.loc[idx]\n",
    "    auto_selected.to_csv(f'autoselected_terms.tsv', sep=\"\\t\", index=False, header=True) # output to TSV\n",
    "\n",
    "    \"\"\"   Get low scorers, aggregate for manual selections  \"\"\"\n",
    "    low_scorers = cache[cache['score'] < 80]\n",
    "    manual_review = low_scorers[~low_scorers.clintrial_term.isin(highscorers['clintrial_term'].unique().tolist())] # there are terms autoselected that have mappings that didn't pass threshold too, but we want to consider that term mapped. So get rid of these rows too\n",
    "    mapping_tool_response = manual_review['mapping_tool_response'].apply(lambda x: wrap(x))\n",
    "    manual_review = manual_review.copy()\n",
    "    mapping_tool_response = mapping_tool_response.apply(pd.Series, dtype='object')\n",
    "    manual_review.loc[:, 'mapping_tool_response_lists'] = mapping_tool_response.values.tolist()\n",
    "    manual_review.drop('mapping_tool_response', axis=1, inplace=True)\n",
    "    manual_review = manual_review[[\"mapping_tool\", \"term_type\", \"clintrial_term\", \"mapping_tool_response_lists\", \"input_term\", \"score\"]]\n",
    "    # manual_review['mapping_tool_response_lists'] = manual_review['mapping_tool_response_lists'].apply(lambda x: ' | '.join(x) if isinstance(x, list) else None)  # Multindexing does not work on lists, so remove the CURIE information out of the list to enable this\n",
    "    manual_review['mapping_tool_response'] = [' | '.join(map(str, l)) for l in manual_review['mapping_tool_response_lists']]\n",
    "    manual_review.drop('mapping_tool_response_lists', axis=1, inplace=True)\n",
    "    manual_review = manual_review.sort_values(by=[\"mapping_tool\", \"term_type\", \"clintrial_term\", \"input_term\"], ascending=False)\n",
    "    manual_review.set_index([\"mapping_tool\", \"term_type\", \"clintrial_term\", \"input_term\"], inplace=True)   # create index\n",
    "    # manual_review.drop([\"temp\"], axis = 1, inplace = True)   # drop the temp column\n",
    "    manual_review['manually_selected_CURIE'] = None # make a column \n",
    "    manual_review.to_excel('manual_review.xlsx', engine='xlsxwriter', index=True)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # flag_and_path = get_raw_ct_data() # download raw data\n",
    "\n",
    "    flag_and_path = {\"term_program_flag\": False, \"data_extracted_path\": \"/15TB_2/gglusman/datasets/clinicaltrials/AACT-20240227\", \"date_string\": \"02_27_2024\"}\n",
    "    # flag_and_path = {\"term_program_flag\": False, \"data_extracted_path\": \"/Users/Kamileh/Work/ISB/NCATS_BiomedicalTranslator/Projects/ClinicalTrials/ETL_Python/data/02_27_2024_extracted\", \"date_string\": \"02_27_2024\"}\n",
    "    global metamap_dirs\n",
    "    metamap_dirs = check_os()\n",
    "    subset_size = None\n",
    "    df_dict = read_raw_ct_data(flag_and_path, subset_size) # read the clinical trial data\n",
    "    dict_new_terms = check_against_cache(df_dict) # use the existing cache of MetaMapped terms so that only new terms are mapped\n",
    "    term_list_to_mappers(dict_new_terms)\n",
    "    score_mappings()\n",
    "    output_terms_files()\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
